{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from haversine import haversine, Unit\n",
    "from obspy.taup import TauPyModel\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np"
   ],
   "id": "14c8ff373a927ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load stations infos",
   "id": "705356b8447a1bb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# FILE = 'C:/Users/Romain/PycharmProjects/toolbox/data/recensement_stations_OHASISBIO_RS.csv'\n",
    "FILE = '/media/rsafran/CORSAIR/MAHY/MAHY.csv'\n",
    "stations = pd.read_csv(FILE )\n",
    "stations = stations.loc[stations.dataset==\"MAHY4\"]\n",
    "stations\n",
    "station_dic = {}\n",
    "for name in stations.station_name :\n",
    "    st_lat, st_lon  = stations.loc[stations.station_name==name].lat.values[0],\\\n",
    "    stations.loc[stations.station_name==name].lon.values[0]\n",
    "    station_dic[name] = [st_lat, st_lon]\n",
    "    print(name , st_lat, st_lon)"
   ],
   "id": "f86f87c4b94d1186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "station_dic['MAHY41']",
   "id": "bac92e0f508b1585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate travel times",
   "id": "a8a9c1f6f5877641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define station coordinates and initialize model\n",
    "\n",
    "name = 'MAHY41'\n",
    "model = TauPyModel(model=\"ak135\")\n",
    "\n",
    "[st_lat, st_lon]  =  station_dic[name]\n",
    "def calculate_phases(row):\n",
    "    \"\"\"\n",
    "    Compute travel times and metadata for all phases in `phase_list`.\n",
    "    Returns a list of dictionaries (one per phase).\n",
    "    \"\"\"\n",
    "\n",
    "    event_lat = row['latitude']\n",
    "    event_lon = row['longitude']\n",
    "    event_depth = row['depth']  # Ensure depth is in kilometers\n",
    "    origin_time = row['time']\n",
    "\n",
    "    # Calculate angular distance between event and station\n",
    "    dist_deg = haversine(\n",
    "        (event_lat, event_lon), (st_lat, st_lon), unit=Unit.DEGREES\n",
    "    )\n",
    "\n",
    "    # Get travel times for all phases in the list\n",
    "    try:\n",
    "        arrivals = model.get_travel_times(\n",
    "            source_depth_in_km=event_depth,\n",
    "            distance_in_degree=dist_deg,\n",
    "            phase_list=[\"P\"] #, \"PKP\", \"PKiKP\", \"PKIKP\"]\n",
    "        )\n",
    "    except ValueError:\n",
    "        return []  # Return empty list if no arrivals\n",
    "\n",
    "    phases = []\n",
    "    for arrival in arrivals:\n",
    "        phase_info = {\n",
    "            'phase': arrival.name,\n",
    "            'travel_time': arrival.time,\n",
    "            'arrival_time': origin_time + pd.to_timedelta(arrival.time, unit='s'),\n",
    "            'distance_deg': dist_deg\n",
    "        }\n",
    "        phases.append(phase_info)\n",
    "\n",
    "    return phases\n",
    "\n",
    "# Load catalogue and compute phases\n",
    "# PATH = 'C:/Users/Romain/PycharmProjects/NEIC_ISC_join/data/ISC_cat'\n",
    "PATH = '../../../../data/ISC_cat'\n",
    "NAME_NEIC = '/NEIC_2020_2025_M55.csv'\n",
    "catalogue = pd.read_csv(PATH + NAME_NEIC, parse_dates=['time'], date_format='ISO8601')\n",
    "#\n",
    "# # Expand DataFrame to include all phases\n",
    "catalogue['phases'] = catalogue.apply(calculate_phases, axis=1)\n",
    "catalogue = catalogue.explode('phases').reset_index(drop=True)\n",
    "#\n",
    "# # Extract phase details into columns\n",
    "phase_data = pd.json_normalize(catalogue['phases'])\n",
    "catalogue = pd.concat([catalogue.drop('phases', axis=1), phase_data], axis=1)\n",
    "#\n",
    "# # Drop rows where phases were not found (optional)\n",
    "catalogue = catalogue.dropna(subset=['phase'])\n",
    "\n",
    "# Save the extended catalogue\n",
    "# catalogue.to_csv(PATH + '/NEIC_2018_M6_with_tt.csv', index=False)"
   ],
   "id": "9949045ca032fe47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue.sort_values(\"time\", inplace=True, ignore_index=True)\n",
   "id": "38a8861b4cf73d49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue.columns",
   "id": "a789948ba7018e3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dat = ['2018-02-25 17:44:44.140000','2018-03-25 20:14:47.690000','2018-03-26 09:51:00.430000','2018-04-02 13:40:34.840000','2018-07-28 22:47:38.740000','2018-08-05 11:46:38.630000','2018-08-19 00:19:40.670000','2018-08-19 14:56:27.490000','2018-09-06 15:49:18.710000','2018-09-10 04:19:02.630000','2018-09-28 10:02:45.250000','2018-10-29 06:54:21.250000','2018-12-05 04:18:08.420000','2018-12-11 02:26:29.420000','2018-12-29 03:39:09.740000']\n",
    "Err =  [2.66,2.21,2.39,2.01,2.32 ,2.14,1.61,1.78,2.16,1.93,2.03,1.60,1.5,1.88,2.94]\n",
    "# for i, l in enumerate(dat) :\n",
    "#     print(catalogue.loc[catalogue['time']==l]['rms'].unique()*2.0)\n",
    "\n",
    "catalogue.loc[catalogue['time']==dat[6]]"
   ],
   "id": "43c5c0b9db56e960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finding candidates",
   "id": "4f53747101d5df73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter tools",
   "id": "551bdaebae292bfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to downsample audio data\n",
    "def downsample_audio(data, original_fs, target_fs):\n",
    "    \"\"\"Downsample audio data to the target frequency\"\"\"\n",
    "    # print(f\"Downsampling from {original_fs}Hz to {target_fs}Hz\")\n",
    "    # Calculate downsampling factor\n",
    "    factor = int(original_fs / target_fs)\n",
    "    # Apply anti-aliasing filter before downsampling\n",
    "    b, a = signal.butter(5, target_fs/2, fs=original_fs, btype='low')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    # Downsample by taking every 'factor' sample\n",
    "    downsampled_data = filtered_data[::factor]\n",
    "    return downsampled_data, target_fs\n",
    "\n",
    "# Function to apply dehazing (using spectral subtraction)\n",
    "def dehaze_audio(data, fs, frame_size=1024, overlap=0.8):\n",
    "    \"\"\"Apply spectral subtraction for dehazing\"\"\"\n",
    "    # print(\"Applying dehazing using spectral subtraction\")\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    # Estimate noise profile from first few frames\n",
    "    num_noise_frames = 5\n",
    "    noise_estimate = np.zeros(frame_size // 2 + 1)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(0, len(data) - frame_size, hop_size):\n",
    "        frame = data[i:i+frame_size]\n",
    "        if len(frame) < frame_size:\n",
    "            frame = np.pad(frame, (0, frame_size - len(frame)))\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Estimate noise from first few frames\n",
    "    for i in range(min(num_noise_frames, len(frames))):\n",
    "        noise_frame = frames[i]\n",
    "        noise_spectrum = np.abs(np.fft.rfft(noise_frame * np.hanning(frame_size)))\n",
    "        noise_estimate += noise_spectrum / num_noise_frames\n",
    "\n",
    "    # Apply spectral subtraction\n",
    "    result = np.zeros(len(data))\n",
    "    window = np.hanning(frame_size)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        windowed_frame = frame * window\n",
    "        spectrum = np.fft.rfft(windowed_frame)\n",
    "        magnitude = np.abs(spectrum)\n",
    "        phase = np.angle(spectrum)\n",
    "\n",
    "        # Subtract noise and ensure no negative values\n",
    "        magnitude = np.maximum(magnitude - noise_estimate * 1.5, 0.01 * magnitude)\n",
    "\n",
    "        # Reconstruct frame\n",
    "        enhanced_spectrum = magnitude * np.exp(1j * phase)\n",
    "        enhanced_frame = np.fft.irfft(enhanced_spectrum)\n",
    "\n",
    "        # Overlap-add\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        result[start:end] += enhanced_frame\n",
    "\n",
    "    # Normalize\n",
    "    result = result / np.max(np.abs(result))\n",
    "    return result\n",
    "\n",
    "# Function to apply Butterworth bandpass filter\n",
    "def apply_butter_bandpass(data, fs, lowcut, highcut, order=5):\n",
    "    \"\"\"Apply Butterworth bandpass filter\"\"\"\n",
    "    # print(f\"Applying bandpass filter: {lowcut}-{highcut}Hz, order {order}\")\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to create spectrogram\n",
    "def create_spectrogram(data, fs, nperseg=256, noverlap=128, cmap='viridis'):\n",
    "    \"\"\"Create and return spectrogram of the data\"\"\"\n",
    "    # print(\"Creating spectrogram\")\n",
    "    f, t, Sxx = signal.spectrogram(data, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    return f, t, Sxx\n",
    "\n",
    "# Function to detect potential seismic events using energy\n",
    "def detect_seismic_events(data, fs, window_size=5.0, threshold_factor=5.0):\n",
    "    \"\"\"Detect potential seismic events based on energy threshold\"\"\"\n",
    "    print(\"Detecting potential seismic events\")\n",
    "    window_samples = int(window_size * fs)\n",
    "    energy = []\n",
    "\n",
    "    # Calculate energy in sliding windows\n",
    "    for i in range(0, len(data) - window_samples, window_samples // 2):\n",
    "        window = data[i:i+window_samples]\n",
    "        window_energy = np.sum(window**2) / len(window)\n",
    "        energy.append(window_energy)\n",
    "\n",
    "    # Set threshold as a factor of the median energy\n",
    "    energy = np.array(energy)\n",
    "    threshold = np.median(energy) * threshold_factor\n",
    "\n",
    "    # Find events that exceed threshold\n",
    "    events = []\n",
    "    in_event = False\n",
    "    event_start = 0\n",
    "\n",
    "    for i, e in enumerate(energy):\n",
    "        if e > threshold and not in_event:\n",
    "            in_event = True\n",
    "            event_start = i * (window_samples // 2) / fs\n",
    "        elif e <= threshold and in_event:\n",
    "            in_event = False\n",
    "            event_end = i * (window_samples // 2) / fs\n",
    "            events.append((event_start, event_end))\n",
    "\n",
    "    # Handle if we're still in an event at the end\n",
    "    if in_event:\n",
    "        event_end = len(data) / fs\n",
    "        events.append((event_start, event_end))\n",
    "\n",
    "    return events, energy, threshold\n",
    "\n",
    "# Main processing function\n",
    "def process_underwater_recording(data,df,date_time,original_fs=240., target_fs=50, low_pass=1.5, high_pass=0.6):\n",
    "    \"\"\"Process underwater recording to visualize seismic events\"\"\"\n",
    "    # Load data\n",
    "\n",
    "    print(f\"Original sampling rate: {original_fs}Hz\")\n",
    "    print(f\"Original data length: {len(data)} samples ({len(data)/original_fs:.2f} seconds)\")\n",
    "\n",
    "    # Downsample to 50Hz\n",
    "    downsampled_data, new_fs = downsample_audio(data, original_fs, target_fs)\n",
    "    print(f\"Downsampled data length: {len(downsampled_data)} samples ({len(downsampled_data)/new_fs:.2f} seconds)\")\n",
    "\n",
    "    # Apply dehazing\n",
    "    dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "\n",
    "    # Apply Butterworth bandpass filter\n",
    "    filtered_data = apply_butter_bandpass(dehazed_data, new_fs, high_pass, low_pass)\n",
    "\n",
    "    # Detect potential seismic events\n",
    "    events, energy, threshold = detect_seismic_events(filtered_data, new_fs)\n",
    "    time = timedelta(minutes=10)\n",
    "    for event_start, _ in events:\n",
    "        if np.abs(time - event_start) < 10 :\n",
    "            catalogue.loc[catalogue['time'] == date_time,\"candidate\"] = True\n",
    "            break\n",
    "    if True :\n",
    "        # Create a separate figure for event detection results\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        window_size = 5.0  # seconds\n",
    "        for arrival, phase in zip(df['arrival_time'],df['phase']):\n",
    "            print(arrival, phase)\n",
    "            time = arrival - df['arrival_time'].iloc[0] + timedelta(minutes=10)\n",
    "            time = time.total_seconds()\n",
    "            plt.axvline(time, color='g', linestyle='--', label=phase)\n",
    "        time_axis = np.arange(len(energy)) * (window_size/2)\n",
    "        plt.plot(time_axis, energy)\n",
    "        plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "        plt.title('Signal Energy for Event Detection')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Energy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return filtered_data, events"
   ],
   "id": "b9c6b7aeda74548d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find candidates",
   "id": "a735c6d8976a9def"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "from tqdm.notebook import tqdm\n",
    "# PATH = f\"F:/OHASISBIO/2018/{name}\"\n",
    "\n",
    "PATH = f\"/media/rsafran/CORSAIR/MAHY/{name}\"\n",
    "ORIGINAL_FS = 240.0\n",
    "TARGET_FS = 60\n",
    "LOW_PASS = 1.5\n",
    "HIGH_PASS = 0.6\n",
    "TIME_TOLERANCE = 10  # seconds\n",
    "\n",
    "# Initialize candidate column\n",
    "catalogue['candidate'] = False\n",
    "catalogue['file_number'] = -1\n",
    "manager = DatFilesManager(PATH,kwargs='raw')\n",
    "\n",
    "def find_candidates(manager, catalogue):\n",
    "    ORIGINAL_FS = 240.0\n",
    "    TARGET_FS = 60\n",
    "    LOW_PASS = 1.5\n",
    "    HIGH_PASS = 0.6\n",
    "    TIME_TOLERANCE = 10\n",
    "    # Process each unique event in the catalogue\n",
    "    for date_time in tqdm(catalogue['time'].unique()):\n",
    "\n",
    "        event_df = catalogue[catalogue['time'] == date_time]\n",
    "        first_arrival = event_df['arrival_time'].min()\n",
    "\n",
    "        # Define time window: 10 minutes before and after first arrival\n",
    "        start = (first_arrival - timedelta(minutes=10)).replace(tzinfo=None)\n",
    "        end = (first_arrival + timedelta(minutes=10)).replace(tzinfo=None)\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess seismic data\n",
    "            data = manager.get_segment(start, end)\n",
    "            file_number = manager.find_file_name(start)\n",
    "            downsampled_data, new_fs = downsample_audio(data, ORIGINAL_FS, TARGET_FS)\n",
    "            dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "            filtered_data = apply_butter_bandpass(dehazed_data, new_fs, HIGH_PASS, LOW_PASS)\n",
    "\n",
    "            # Detect seismic events (adjust threshold as needed)\n",
    "            events, energy, threshold = detect_seismic_events(filtered_data, new_fs, threshold_factor=8.0)\n",
    "\n",
    "            # # Check if any detected event aligns with expected arrival time\n",
    "            expected_time_sec = 600  # 10 minutes into the segment\n",
    "            # for event_start, _ in events:\n",
    "            #     if abs(event_start - expected_time_sec) < TIME_TOLERANCE:\n",
    "            #         catalogue.loc[catalogue['time'] == datetime, 'candidate'] = True\n",
    "            #         break  # Stop checking once a match is found\n",
    "            event_starts = np.array([e[0] for e in events])\n",
    "            if np.any(np.abs(event_starts - 600) < TIME_TOLERANCE):\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'candidate'] = True\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'file_number'] = file_number\n",
    "\n",
    "            # Optional: Plot detection results for debugging\n",
    "            if False:  # Set to True to enable\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                time_axis = np.arange(len(energy)) * (5.0 / 2)  # Assuming 5s window\n",
    "                plt.plot(time_axis, energy)\n",
    "                plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "                plt.axvline(expected_time_sec, color='g', linestyle='--', label='Expected Arrival')\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('Energy')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_time}: {str(e)}\")\n",
    "            continue  # Skip to next event on failure\n",
    "    return data, catalogue\n",
    "\n",
    "data, catalogue = find_candidates(manager, catalogue)\n",
    "\n",
    "#first arrival is in the middle\n",
    "#second arrival plot will be\n",
    "#dt = sec - first + timedelta(minute=10) and ect.\n",
    "\n"
   ],
   "id": "9fc5e954c547fac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# data.shape",
   "id": "cf625382c8ad4001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# catalogue['candidate'] = False\n",
    "# catalogue.loc[catalogue[\"arrival_time\"]>='2018-02-13', ['candidate']] = True\n",
    "# catalogue.loc[catalogue[\"arrival_time\"]>='2018-07-20', ['candidate']] = False"
   ],
   "id": "2c1de6b7f895a2db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.figure()\n",
    "# time = np.arange(len(data))/240.\n",
    "# plt.plot(time, data)\n",
    "# df = catalogue[catalogue['time'] == datetime]\n",
    "# for arrivals in df['arrival_time']:\n",
    "#     time = arrivals - df['arrival_time'].iloc[0] + timedelta(minutes=10)\n",
    "#     time = time.total_seconds()\n",
    "#     plt.axvline(time, color='r', linestyle='--', label='Arrivals')"
   ],
   "id": "e31701035fd73059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue[catalogue[\"candidate\"]==True]['time'].unique()",
   "id": "c0c29cd76648f86b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue.to_csv(f'../../../../data/{name}_2018_M.csv',index=False)",
   "id": "7fc080b235bda350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue[catalogue['candidate'] == True]['file_number'].unique()",
   "id": "68c1c788b08d48c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *ISC ERROR Scrapping*\n",
   "id": "e8fdc6d2f8f0f8ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# List of event timestamps\n",
    "timestamps = [\n",
    "    \"2018-02-25 17:44:44.140000\",\n",
    "    \"2018-03-25 20:14:47.690000\",\n",
    "    \"2018-03-26 09:51:00.430000\",\n",
    "    \"2018-04-02 13:40:34.840000\",\n",
    "    \"2018-07-28 22:47:38.740000\",\n",
    "    \"2018-08-05 11:46:38.630000\",\n",
    "    \"2018-08-19 00:19:40.670000\",\n",
    "    \"2018-08-19 14:56:27.490000\",\n",
    "    \"2018-09-06 15:49:18.710000\",\n",
    "    \"2018-09-10 04:19:02.630000\",\n",
    "    \"2018-09-28 10:02:45.250000\",\n",
    "    \"2018-10-29 06:54:21.250000\",\n",
    "    \"2018-12-05 04:18:08.420000\",\n",
    "    \"2018-12-11 02:26:29.420000\",\n",
    "    \"2018-12-29 03:39:09.740000\",\n",
    "]\n",
    "\n",
    "# Base URL for ISC QuakeML queries\n",
    "BASE_URL = \"https://www.isc.ac.uk/cgi-bin/web-db-run\"\n",
    "\n",
    "# Function to query ISC and get uncertainty\n",
    "def get_uncertainty(event_time):\n",
    "    # Convert event time to ISC format\n",
    "    dt = datetime.strptime(event_time, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    search_time = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-4]  # Keep only the first two digits of milliseconds\n",
    "\n",
    "    # Construct API request URL\n",
    "    params = {\n",
    "        \"request\": \"COMPREHENSIVE\",\n",
    "        \"out_format\": \"QuakeML\",\n",
    "        \"start_year\": dt.year,\n",
    "        \"start_month\": dt.month,\n",
    "        \"start_day\": dt.day,\n",
    "        \"start_time\": (dt- timedelta(seconds=60)).strftime(\"%H:%M:%S\"),\n",
    "        \"end_year\": dt.year,\n",
    "        \"end_month\": dt.month,\n",
    "        \"end_day\": dt.day,\n",
    "        \"end_time\": (dt+ timedelta(seconds=60)).strftime(\"%H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request failed for {event_time} with status code {response.status_code}\")\n",
    "        return None  # Return None if request failed\n",
    "\n",
    "    # Check if the response body is empty\n",
    "    if not response.text.strip():\n",
    "        print(f\"Empty response for {event_time}\")\n",
    "        return None  # Return None if no content in the response\n",
    "\n",
    "    try:\n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(response.text)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parsing error for {event_time}: {e}\")\n",
    "        return None  # Return None if XML parsing fails\n",
    "\n",
    "    # Find the exact matching event\n",
    "    for origin in root.findall(\".//{*}origin\"):\n",
    "        time_elem = origin.find(\"{*}time/{*}value\")\n",
    "        uncertainty_elem = origin.find(\"{*}time/{*}uncertainty\")\n",
    "        depth_elem = origin.find(\"{*}depth/{*}value\")\n",
    "\n",
    "        if time_elem is not None and uncertainty_elem is not None:\n",
    "            event_time_utc = time_elem.text\n",
    "            event_time_utc = event_time_utc.replace(\"Z\", \"\")  # Remove 'Z' for direct comparison\n",
    "\n",
    "            # Match the timestamp to milliseconds (only first two digits of milliseconds)\n",
    "            event_time_utc_ms = event_time_utc[:23]  # First two digits of milliseconds\n",
    "            search_time_ms = search_time[:23]  # First two digits of milliseconds\n",
    "\n",
    "            # Debug print: show the event time comparison for debugging\n",
    "            print(f\"Checking: Requested Time: {search_time_ms} vs Event Time: {event_time_utc_ms}\")\n",
    "\n",
    "            # Compare timestamps with only first two digits of milliseconds\n",
    "            if event_time_utc_ms == search_time_ms:  # Compare only up to two digits of milliseconds\n",
    "                uncertainty = uncertainty_elem.text\n",
    "                depth = depth_elem.text\n",
    "                return event_time_utc, uncertainty, depth\n",
    "\n",
    "    return None\n",
    "if True :\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Loop through each timestamp and get uncertainty\n",
    "    for timestamp in key_times:\n",
    "        data = get_uncertainty(timestamp)\n",
    "        if data:\n",
    "            event_time, uncertainty, depth = data\n",
    "            depth = float(depth)*1e-3 #converting to Km\n",
    "            results.append([timestamp, event_time, uncertainty, depth])\n",
    "            print(f\"Event: {timestamp} -> Uncertainty: {uncertainty} | Depth: {depth} km\")\n",
    "        else:\n",
    "            results.append([timestamp, \"Not Found\", \"N/A\"])\n",
    "            print(f\"Event: {timestamp} -> No data found\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    csv_filename = f\"../../../../data/seismic_uncertainty_results_{name}.csv\"\n",
    "    with open(csv_filename, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Requested Time\", \"Event Time (UTC)\", \"Uncertainty\", \"Depth\"])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"\\nResults saved to {csv_filename}\")\n"
   ],
   "id": "49f7d8ece583f095",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare ISC / NEIC",
   "id": "10d13258d350c06a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NEIC",
   "id": "53ff18d39c69e205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalogue['key_time'] = catalogue['time'].copy()\n",
    "\n",
    "catalogue\n",
    "key_times =pd.unique( catalogue['key_time'].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\").values)"
   ],
   "id": "64817c541a99f691",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "key_times = key_times.tolist()",
   "id": "9746652727b4e028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ISC GEM",
   "id": "e69e566d5870af13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PATH = '../../../../data/ISC_cat'\n",
    "NAME_NEIC = '/GEM_merged_M6.csv'\n",
    "catalogue_gem = pd.read_csv(PATH + NAME_NEIC, parse_dates=['time', 'key_time'], date_format='ISO8601')\n",
    "\n",
    "# Expand DataFrame to include all phases\n",
    "catalogue_gem['phases'] = catalogue_gem.apply(calculate_phases, axis=1)\n",
    "catalogue_gem = catalogue_gem.explode('phases').reset_index(drop=True)\n",
    "\n",
    "# Extract phase details into columns\n",
    "phase_data = pd.json_normalize(catalogue_gem['phases'])\n",
    "catalogue_gem = pd.concat([catalogue_gem.drop('phases', axis=1), phase_data], axis=1)\n",
    "\n",
    "# Drop rows where phases were not found (optional)\n",
    "catalogue_gem = catalogue_gem.dropna(subset=['phase'])\n",
    "catalogue_gem"
   ],
   "id": "ccb1dd036a5c3ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ISC EHB",
   "id": "927abe8ad9dbe8d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PATH = '../../../../data/ISC_cat'\n",
    "NAME_NEIC = '/EHB_merged_M6.csv'\n",
    "catalogue_ebh = pd.read_csv(PATH + NAME_NEIC, parse_dates=['time', 'key_time'], date_format='ISO8601')\n",
    "\n",
    "# Expand DataFrame to include all phases\n",
    "catalogue_ebh['phases'] = catalogue_ebh.apply(calculate_phases, axis=1)\n",
    "catalogue_ebh = catalogue_ebh.explode('phases').reset_index(drop=True)\n",
    "\n",
    "# Extract phase details into columns\n",
    "phase_data = pd.json_normalize(catalogue_ebh['phases'])\n",
    "catalogue_ebh = pd.concat([catalogue_ebh.drop('phases', axis=1), phase_data], axis=1)\n",
    "\n",
    "# Drop rows where phases were not found (optional)\n",
    "catalogue_ebh = catalogue_ebh.dropna(subset=['phase'])\n",
    "catalogue_ebh"
   ],
   "id": "8d3a18145f09ab2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparaison",
   "id": "62cf5a96ec4090b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge catalogues",
   "id": "4d7dfca5dbaa1039"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: assume your three dataframes are named catalogue_ebh, catalogue, and catalogue_gem\n",
    "\n",
    "# Optionally, if you want to compare only one phase (e.g., only phase \"P\"):\n",
    "# catalogue_ebh = catalogue_ebh[catalogue_ebh['phase'] == 'P']\n",
    "# catalogue = catalogue[catalogue['phase'] == 'P']\n",
    "# catalogue_gem = catalogue_gem[catalogue_gem['phase'] == 'P']\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_ebh = catalogue_ebh.rename(columns={\n",
    "    'time': 'time_ebh',\n",
    "    'travel_time': 'travel_time_ebh',\n",
    "    'distance_deg': 'distance_deg_ebh',\n",
    "    'arrival_time': 'arrival_time_ebh'\n",
    "})\n",
    "df_neic = catalogue.rename(columns={\n",
    "    'time': 'time_neic',\n",
    "    'travel_time': 'travel_time_neic',\n",
    "    'distance_deg': 'distance_deg_neic',\n",
    "    'arrival_time': 'arrival_time_neic'\n",
    "})\n",
    "df_gem = catalogue_gem.rename(columns={\n",
    "    'time': 'time_gem',\n",
    "    'travel_time': 'travel_time_gem',\n",
    "    'distance_deg': 'distance_deg_gem',\n",
    "    'arrival_time': 'arrival_time_gem'\n",
    "})\n",
    "\n",
    "\n",
    "# Function to group and select the earliest phase arrival\n",
    "def select_earliest(df, time_col):\n",
    "    \"\"\"Sort by arrival time, then travel time as a fallback, and keep first.\"\"\"\n",
    "    return df.sort_values(by=[time_col], ascending=[True]).groupby(['key_time', 'phase'], as_index=False).first()\n",
    "\n",
    "# Apply the function to each catalogue\n",
    "df_ebh_grouped = select_earliest(df_ebh, 'arrival_time_ebh')\n",
    "df_neic_grouped = select_earliest(df_neic, 'arrival_time_neic')\n",
    "df_gem_grouped = select_earliest(df_gem, 'arrival_time_gem')\n",
    "\n",
    "# Merge using outer join to keep all phases\n",
    "merged = (\n",
    "    df_ebh_grouped.merge(df_neic_grouped, on=['key_time', 'phase'], how='outer', suffixes=('_ebh', '_neic'))\n",
    "                  .merge(df_gem_grouped, on=['key_time', 'phase'], how='outer', suffixes=('', '_gem'))\n",
    ")\n",
    "\n",
    "# Inspect the merged DataFrame\n",
    "print(merged.head())\n",
    "\n",
    "\n",
    "# Convert time columns to datetime (if not already in datetime format)\n",
    "merged['time_ebh'] = pd.to_datetime(merged['time_ebh'])\n",
    "merged['time_neic'] = pd.to_datetime(merged['time_neic'])\n",
    "merged['time_gem'] = pd.to_datetime(merged['time_gem'])\n",
    "\n",
    "merged['arrival_time_ebh'] =  pd.to_datetime(merged['arrival_time_ebh'])\n",
    "merged['arrival_time_neic'] = pd.to_datetime(merged['arrival_time_neic'])\n",
    "merged['arrival_time_gem'] =  pd.to_datetime(merged['arrival_time_gem'])\n",
    "\n",
    "# Calculate time differences in seconds between the catalogues\n",
    "merged['time_diff_neic_ebh'] = (merged['time_neic'] - merged['time_ebh']).dt.total_seconds()\n",
    "merged['time_diff_ebh_gem'] = (merged['time_ebh'] - merged['time_gem']).dt.total_seconds()\n",
    "merged['time_diff_neic_gem'] = (merged['time_neic'] - merged['time_gem']).dt.total_seconds()\n",
    "\n",
    "# Calculate time differences in seconds between the catalogues\n",
    "merged['arrival_time_diff_neic_ebh'] =(merged['arrival_time_neic'] - merged['arrival_time_ebh']).dt.total_seconds()\n",
    "merged['arrival_time_diff_ebh_gem'] = (merged['arrival_time_ebh'] - merged['arrival_time_gem']).dt.total_seconds()\n",
    "merged['arrival_time_diff_neic_gem'] =(merged['arrival_time_neic'] - merged['arrival_time_gem']).dt.total_seconds()\n",
    "\n",
    "# Calculate differences in travel time (assuming numeric values)\n",
    "merged['travel_time_diff_neic_ebh'] = merged['travel_time_neic'] - merged['travel_time_ebh']\n",
    "merged['travel_time_diff_ebh_gem'] = merged['travel_time_ebh'] - merged['travel_time_gem']\n",
    "merged['travel_time_diff_neic_gem'] = merged['travel_time_neic'] - merged['travel_time_gem']\n",
    "\n",
    "# Calculate differences in distance (distance_deg)\n",
    "merged['distance_diff_neic_ebh'] = merged['distance_deg_neic'] - merged['distance_deg_ebh']\n",
    "merged['distance_diff_ebh_gem'] = merged['distance_deg_ebh'] - merged['distance_deg_gem']\n",
    "merged['distance_diff_neic_gem'] = merged['distance_deg_neic'] - merged['distance_deg_gem']\n",
    "\n",
    "# Inspect the merged result\n",
    "merged.columns"
   ],
   "id": "4f6651e5b98c298b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### export corection file as catalogue_cor_{name}.csv",
   "id": "15b8355def280baa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "times = [\n",
    "'2018-02-25 17:44:44.14',\n",
    "'2018-03-25 20:14:47.69',\n",
    "'2018-03-26 09:51:00.43',\n",
    "'2018-04-02 13:40:34.84',\n",
    "'2018-07-28 22:47:38.74',\n",
    "'2018-08-05 11:46:38.63',\n",
    "'2018-08-19 00:19:40.67',\n",
    "'2018-08-19 14:56:27.49',\n",
    "'2018-09-06 15:49:18.71',\n",
    "'2018-09-10 04:19:02.63',\n",
    "'2018-09-28 10:02:45.25',\n",
    "'2018-10-29 06:54:21.25',\n",
    "'2018-12-05 04:18:08.42',\n",
    "'2018-12-11 02:26:29.42',\n",
    "'2018-12-29 03:39:09.74']\n",
    "#\n",
    "# times = pd.to_datetime(times, utc=True)\n",
    "# merged[merged['key_time'].isin(times)]['arrival_time_diff_neic_gem']\n",
    "merged[['key_time','phase','arrival_time_diff_neic_gem', 'arrival_time_diff_neic_ebh','timeError']].to_csv(f'../../../../data/catalogue_cor_{name}.csv',index=False)"
   ],
   "id": "d04099154e370f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plots",
   "id": "bb95344799e70154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Get unique phases\n",
    "phases = merged['phase'].unique()\n",
    "\n",
    "# Create subplots (rows = number of phases, 2 columns: Travel Time | Arrival Time)\n",
    "fig, axes = plt.subplots(len(phases), 2, figsize=(12, 4 * len(phases)), sharex=True, sharey=True)\n",
    "\n",
    "# Loop through each phase and plot in subplots\n",
    "for i, phase in enumerate(phases):\n",
    "    subset = merged[merged['phase'] == phase]\n",
    "\n",
    "    # Travel Time Differences (Left Column)\n",
    "    axes[i, 0].scatter(subset.index, subset['travel_time_diff_neic_ebh'], marker='o', color='r', label='NEIC vs GEM', alpha=0.6)\n",
    "    axes[i, 0].scatter(subset.index, subset['travel_time_diff_ebh_gem'], marker='s', color='b', label='EBH vs GEM', alpha=0.6)\n",
    "    axes[i, 0].scatter(subset.index, subset['travel_time_diff_neic_gem'], marker='^', color='g', label='NEIC vs GEM', alpha=0.6)\n",
    "    axes[i, 0].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[i, 0].set_title(f\"Phase: {phase} - Travel Time Diff\")\n",
    "    axes[i, 0].set_ylabel(\"Travel Time Diff (s)\")\n",
    "    if i == len(phases) - 1:\n",
    "        axes[i, 0].set_xlabel(\"Event Index\")\n",
    "\n",
    "    # Arrival Time Differences (Right Column)\n",
    "    axes[i, 1].scatter(subset.index, subset['arrival_time_diff_neic_ebh'], marker='o', color='r', label='NEIC vs EBH', alpha=0.6)\n",
    "    axes[i, 1].scatter(subset.index, subset['arrival_time_diff_ebh_gem'], marker='s', color='b', label='EBH vs GEM', alpha=0.6)\n",
    "    axes[i, 1].scatter(subset.index, subset['arrival_time_diff_neic_gem'], marker='^', color='g', label='NEIC vs GEM', alpha=0.6)\n",
    "    axes[i, 1].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[i, 1].set_title(f\"Phase: {phase} - Arrival Time Diff\")\n",
    "    if i == len(phases) - 1:\n",
    "        axes[i, 1].set_xlabel(\"Event Index\")\n",
    "\n",
    "# Adjust layout and add legend\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.show()\n"
   ],
   "id": "2dce7cbe086260bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Filter data for P-phase\n",
    "p_phase_merged = merged[merged['phase'] == 'P']\n",
    "\n",
    "# Create subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "\n",
    "### Travel Time Differences (Left)\n",
    "axes[0].scatter(p_phase_merged.index, p_phase_merged['travel_time_diff_neic_gem'], marker='o', color='r', alpha=0.7, label='NEIC vs EBH')\n",
    "axes[0].scatter(p_phase_merged.index, p_phase_merged['travel_time_diff_ebh_gem'], marker='s', color='b', alpha=0.7, label='EBH vs GEM')\n",
    "axes[0].scatter(p_phase_merged.index, p_phase_merged['travel_time_diff_neic_ebh'], marker='^', color='g', alpha=0.7, label='NEIC vs GEM')\n",
    "axes[0].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0].set_title(\"P-Phase Travel Time Differences\")\n",
    "axes[0].set_xlabel(\"Event Index\")\n",
    "axes[0].set_ylabel(\"Travel Time Difference (s)\")\n",
    "axes[0].legend()\n",
    "\n",
    "### Arrival Time Differences (Right)\n",
    "axes[1].scatter(p_phase_merged.index, p_phase_merged['arrival_time_diff_neic_ebh'], marker='o', color='r', alpha=0.7, label='NEIC vs EBH')\n",
    "axes[1].scatter(p_phase_merged.index, p_phase_merged['arrival_time_diff_ebh_gem'], marker='s', color='b', alpha=0.7, label='EBH vs GEM')\n",
    "axes[1].scatter(p_phase_merged.index, p_phase_merged['arrival_time_diff_neic_gem'], marker='^', color='g', alpha=0.7, label='NEIC vs GEM')\n",
    "axes[1].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_title(\"P-Phase Arrival Time Differences\")\n",
    "axes[1].set_xlabel(\"Event Index\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7ec7b6c95ae19a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## On sigle phase",
   "id": "4150e68f8e92c214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_ebh = catalogue_ebh.rename(columns={\n",
    "    'time': 'time_ebh',\n",
    "    'travel_time': 'travel_time_ebh',\n",
    "    'distance_deg': 'distance_deg_ebh',\n",
    "    'arrival_time': 'arrival_time_ebh'\n",
    "})\n",
    "df_neic = catalogue.rename(columns={\n",
    "    'time': 'time_neic',\n",
    "    'travel_time': 'travel_time_neic',\n",
    "    'distance_deg': 'distance_deg_neic',\n",
    "    'arrival_time': 'arrival_time_neic'\n",
    "})\n",
    "df_gem = catalogue_gem.rename(columns={\n",
    "    'time': 'time_gem',\n",
    "    'travel_time': 'travel_time_gem',\n",
    "    'distance_deg': 'distance_deg_gem',\n",
    "    'arrival_time': 'arrival_time_gem'\n",
    "})\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Step 1: Filter each catalogue to only include P-phase events ---\n",
    "phase = 'P'\n",
    "# For GEM catalogue:\n",
    "catalogue_gem_p = df_gem[df_gem['phase'] == phase].copy()\n",
    "# Sort by GEM arrival time (assuming the column is named 'arrival_time_gem')\n",
    "catalogue_gem_p = catalogue_gem_p.sort_values(by='arrival_time_gem')\n",
    "# For events with multiple P arrivals, keep the earliest arrival\n",
    "catalogue_gem_p = catalogue_gem_p.drop_duplicates(subset='event_id', keep='first')\n",
    "\n",
    "# For NEIC catalogue:\n",
    "catalogue_neic_p = df_neic[df_neic['phase'] == phase].copy()\n",
    "# Sort by NEIC arrival time (assuming the column 'time' represents arrival time)\n",
    "catalogue_neic_p = catalogue_neic_p.sort_values(by='arrival_time_neic')\n",
    "catalogue_neic_p = catalogue_neic_p.drop_duplicates(subset='key_time', keep='first')\n",
    "\n",
    "# For EBH catalogue:\n",
    "catalogue_ebh_p = df_ebh[df_ebh['phase'] == phase].copy()\n",
    "# Sort by EBH arrival time (assuming 'time' is the arrival time here)\n",
    "catalogue_ebh_p = catalogue_ebh_p.sort_values(by='arrival_time_ebh')\n",
    "catalogue_ebh_p = catalogue_ebh_p.drop_duplicates(subset='event_id', keep='first')\n",
    "\n",
    "# --- Step 2: Merge NEIC and GEM catalogues ---\n",
    "# First, merge the filtered NEIC and GEM on 'key_time'\n",
    "merged_neic_gem = pd.merge(\n",
    "    catalogue_neic_p,\n",
    "    catalogue_gem_p,\n",
    "    on='key_time',\n",
    "    how='left',\n",
    "    suffixes=('_neic', '_gem')\n",
    ")\n",
    "\n",
    "# --- Step 3: Merge with EBH catalogue ---\n",
    "# Then, merge the result with the filtered EBH catalogue on 'event_id'\n",
    "final_merged = pd.merge(\n",
    "    merged_neic_gem,\n",
    "    catalogue_ebh_p,\n",
    "    on='event_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_ebh')\n",
    ")\n",
    "\n"
   ],
   "id": "eececf67d33367a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert time columns to datetime (if not already in datetime format)\n",
    "final_merged['time_ebh'] = pd.to_datetime(final_merged['time_ebh'])\n",
    "final_merged['time_neic'] = pd.to_datetime(final_merged['time_neic'])\n",
    "final_merged['time_gem'] = pd.to_datetime(final_merged['time_gem'])\n",
    "\n",
    "final_merged['arrival_time_ebh'] =  pd.to_datetime(final_merged['arrival_time_ebh'])\n",
    "final_merged['arrival_time_neic'] = pd.to_datetime(final_merged['arrival_time_neic'])\n",
    "final_merged['arrival_time_gem'] =  pd.to_datetime(final_merged['arrival_time_gem'])\n",
    "\n",
    "# Calculate time differences in seconds between the catalogues\n",
    "final_merged['time_diff_neic_ebh'] = (final_merged['time_neic'] - final_merged['time_ebh']).dt.total_seconds()\n",
    "final_merged['time_diff_ebh_gem'] = (final_merged['time_ebh'] - final_merged['time_gem']).dt.total_seconds()\n",
    "final_merged['time_diff_neic_gem'] = (final_merged['time_neic'] - final_merged['time_gem']).dt.total_seconds()\n",
    "\n",
    "# Calculate time differences in seconds between the catalogues\n",
    "final_merged['arrival_time_diff_neic_ebh'] =(final_merged['arrival_time_neic'] - final_merged['arrival_time_ebh']).dt.total_seconds()\n",
    "final_merged['arrival_time_diff_ebh_gem'] = (final_merged['arrival_time_ebh'] - final_merged['arrival_time_gem']).dt.total_seconds()\n",
    "final_merged['arrival_time_diff_neic_gem'] =(final_merged['arrival_time_neic'] - final_merged['arrival_time_gem']).dt.total_seconds()\n",
    "\n",
    "# Calculate differences in travel time (assuming numeric values)\n",
    "final_merged['travel_time_diff_neic_ebh'] = final_merged['travel_time_neic'] - final_merged['travel_time_ebh']\n",
    "final_merged['travel_time_diff_ebh_gem'] = final_merged['travel_time_ebh'] - final_merged['travel_time_gem']\n",
    "final_merged['travel_time_diff_neic_gem'] = final_merged['travel_time_neic'] - final_merged['travel_time_gem']\n",
    "\n",
    "# Calculate differences in distance (distance_deg)\n",
    "final_merged['distance_diff_neic_ebh'] = final_merged['distance_deg_ebh'] - final_merged['distance_deg_neic']\n",
    "final_merged['distance_diff_ebh_gem'] = final_merged['distance_deg_ebh'] - final_merged['distance_deg_gem']\n",
    "final_merged['distance_diff_neic_gem'] = final_merged['distance_deg_neic'] - final_merged['distance_deg_gem']"
   ],
   "id": "d950a4359742d2bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Filter data for P-phase\n",
    "p_phase_df = final_merged\n",
    "\n",
    "# Create subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "\n",
    "### Travel Time Differences (Left)\n",
    "axes[0].scatter(p_phase_df.index, p_phase_df['travel_time_diff_neic_ebh'], marker='o', color='r', alpha=0.7, label='NEIC vs EBH')\n",
    "axes[0].scatter(p_phase_df.index, p_phase_df['travel_time_diff_ebh_gem'], marker='s', color='b', alpha=0.7, label='EBH vs GEM')\n",
    "axes[0].scatter(p_phase_df.index, p_phase_df['travel_time_diff_neic_gem'], marker='^', color='g', alpha=0.7, label='NEIC vs GEM')\n",
    "axes[0].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0].set_title(f\"{phase}-Phase Travel Time Differences\")\n",
    "axes[0].set_xlabel(\"Event Index\")\n",
    "axes[0].set_ylabel(\"Travel Time Difference (s)\")\n",
    "axes[0].legend()\n",
    "\n",
    "### Arrival Time Differences (Right)\n",
    "axes[1].scatter(p_phase_df.index, p_phase_df['arrival_time_diff_neic_ebh'], marker='o', color='r', alpha=0.7, label='NEIC vs EBH')\n",
    "axes[1].scatter(p_phase_df.index, p_phase_df['arrival_time_diff_ebh_gem'], marker='s', color='b', alpha=0.7, label='EBH vs GEM')\n",
    "axes[1].scatter(p_phase_df.index, p_phase_df['arrival_time_diff_neic_gem'], marker='^', color='g', alpha=0.7, label='NEIC vs GEM')\n",
    "axes[1].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_title(f\"{phase}-Phase Arrival Time Differences\")\n",
    "axes[1].set_xlabel(\"Event Index\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e979d192ca4807a8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
