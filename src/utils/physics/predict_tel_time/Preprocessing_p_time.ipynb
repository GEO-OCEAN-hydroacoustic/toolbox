{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T23:58:21.074297Z",
     "start_time": "2025-03-10T23:58:17.215379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from haversine import haversine, Unit\n",
    "from obspy.taup import TauPyModel\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np"
   ],
   "id": "14c8ff373a927ac9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T23:58:34.283053Z",
     "start_time": "2025-03-10T23:58:34.252577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FILE = 'C:/Users/Romain/PycharmProjects/toolbox/data/recensement_stations_OHASISBIO_RS.csv'\n",
    "stations = pd.read_csv(FILE )\n",
    "stations = stations.loc[stations.year==2018]\n",
    "stations\n",
    "station_dic = {}\n",
    "for name in stations.station_name :\n",
    "    st_lat, st_lon  = stations.loc[stations.station_name==name].lat.values[0],\\\n",
    "    stations.loc[stations.station_name==name].lon.values[0]\n",
    "    station_dic[name] = [st_lat, st_lon]\n",
    "    print(name , st_lat, st_lon)"
   ],
   "id": "f86f87c4b94d1186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAN -56.4602 62.976\n",
      "MADE -24.2053 63.0102\n",
      "MADW -29.0473 54.258\n",
      "NEAMS -31.5758 83.2423\n",
      "RTJ -24.3792 72.372\n",
      "SSEIR -33.5175 70.8657\n",
      "SSWIR -38.5465 52.9287\n",
      "SWAMS-bot -42.9513 74.5975\n",
      "WKER2 -46.6015 60.5475\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T00:22:05.771686Z",
     "start_time": "2025-03-11T00:22:05.749265Z"
    }
   },
   "cell_type": "code",
   "source": "station_dic['ELAN']",
   "id": "bac92e0f508b1585",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-56.4602, 62.976]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T00:22:11.087782Z",
     "start_time": "2025-03-11T00:22:06.804901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define station coordinates and initialize model\n",
    "\n",
    "name = 'ELAN'\n",
    "model = TauPyModel(model=\"ak135\")\n",
    "\n",
    "[st_lat, st_lon]  =  station_dic[name]\n",
    "def calculate_phases(row):\n",
    "    \"\"\"\n",
    "    Compute travel times and metadata for all phases in `phase_list`.\n",
    "    Returns a list of dictionaries (one per phase).\n",
    "    \"\"\"\n",
    "\n",
    "    event_lat = row['latitude']\n",
    "    event_lon = row['longitude']\n",
    "    event_depth = row['depth']  # Ensure depth is in kilometers\n",
    "    origin_time = row['time']\n",
    "\n",
    "    # Calculate angular distance between event and station\n",
    "    dist_deg = haversine(\n",
    "        (event_lat, event_lon), (st_lat, st_lon), unit=Unit.DEGREES\n",
    "    )\n",
    "\n",
    "    # Get travel times for all phases in the list\n",
    "    try:\n",
    "        arrivals = model.get_travel_times(\n",
    "            source_depth_in_km=event_depth,\n",
    "            distance_in_degree=dist_deg,\n",
    "            phase_list=[\"P\", \"PKP\", \"PKiKP\", \"PKIKP\"]\n",
    "        )\n",
    "    except ValueError:\n",
    "        return []  # Return empty list if no arrivals\n",
    "\n",
    "    phases = []\n",
    "    for arrival in arrivals:\n",
    "        phase_info = {\n",
    "            'phase': arrival.name,\n",
    "            'travel_time': arrival.time,\n",
    "            'arrival_time': origin_time + pd.to_timedelta(arrival.time, unit='s'),\n",
    "            'distance_deg': dist_deg\n",
    "        }\n",
    "        phases.append(phase_info)\n",
    "\n",
    "    return phases\n",
    "\n",
    "# Load catalogue and compute phases\n",
    "PATH = 'C:/Users/Romain/PycharmProjects/NEIC_ISC_join/data'\n",
    "NAME_NEIC = '/NEIC_2018_M6.csv'\n",
    "catalogue = pd.read_csv(PATH + NAME_NEIC, parse_dates=['time'], date_format='ISO8601')\n",
    "\n",
    "# Expand DataFrame to include all phases\n",
    "catalogue['phases'] = catalogue.apply(calculate_phases, axis=1)\n",
    "catalogue = catalogue.explode('phases').reset_index(drop=True)\n",
    "\n",
    "# Extract phase details into columns\n",
    "phase_data = pd.json_normalize(catalogue['phases'])\n",
    "catalogue = pd.concat([catalogue.drop('phases', axis=1), phase_data], axis=1)\n",
    "\n",
    "# Drop rows where phases were not found (optional)\n",
    "catalogue = catalogue.dropna(subset=['phase'])\n",
    "\n",
    "# Save the extended catalogue\n",
    "# catalogue.to_csv(PATH + '/NEIC_2018_M6_with_all_phases.csv', index=False)"
   ],
   "id": "9949045ca032fe47",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T00:22:16.473026Z",
     "start_time": "2025-03-11T00:22:16.453049Z"
    }
   },
   "cell_type": "code",
   "source": "catalogue.sort_values(\"time\", inplace=True, ignore_index=True)\n",
   "id": "38a8861b4cf73d49",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T00:23:35.125331Z",
     "start_time": "2025-03-11T00:23:35.096608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dat = ['2018-02-25 17:44:44.140000','2018-03-25 20:14:47.690000','2018-03-26 09:51:00.430000','2018-04-02 13:40:34.840000','2018-07-28 22:47:38.740000','2018-08-05 11:46:38.630000','2018-08-19 00:19:40.670000','2018-08-19 14:56:27.490000','2018-09-06 15:49:18.710000','2018-09-10 04:19:02.630000','2018-09-28 10:02:45.250000','2018-10-29 06:54:21.250000','2018-12-05 04:18:08.420000','2018-12-11 02:26:29.420000','2018-12-29 03:39:09.740000']\n",
    "Err =  [2.66,2.21,2.39,2.01,2.32 ,2.14,1.61,1.78,2.16,1.93,2.03,1.60,1.5,1.88,2.94]\n",
    "# for i, l in enumerate(dat) :\n",
    "#     print(catalogue.loc[catalogue['time']==l]['rms'].unique()*2.0)\n",
    "\n",
    "catalogue.loc[catalogue['time']==dat[6]]"
   ],
   "id": "43c5c0b9db56e960",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                time  latitude  longitude  depth  mag magType  \\\n",
       "134 2018-08-19 00:19:40.670000+00:00  -18.1125   -178.153  600.0  8.2     mww   \n",
       "135 2018-08-19 00:19:40.670000+00:00  -18.1125   -178.153  600.0  8.2     mww   \n",
       "\n",
       "     nst   gap  dmin   rms  ... depthError magError magNst    status  \\\n",
       "134  NaN  13.0  3.63  0.79  ...        1.9    0.045   47.0  reviewed   \n",
       "135  NaN  13.0  3.63  0.79  ...        1.9    0.045   47.0  reviewed   \n",
       "\n",
       "    locationSource  magSource  phase  travel_time  \\\n",
       "134             us         us      P   715.080061   \n",
       "135             us         us  PKiKP  1007.658253   \n",
       "\n",
       "                           arrival_time distance_deg  \n",
       "134 2018-08-19 00:31:35.750061216+00:00    89.681135  \n",
       "135 2018-08-19 00:36:28.328253192+00:00    89.681135  \n",
       "\n",
       "[2 rows x 26 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>...</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "      <th>phase</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>distance_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2018-08-19 00:19:40.670000+00:00</td>\n",
       "      <td>-18.1125</td>\n",
       "      <td>-178.153</td>\n",
       "      <td>600.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>mww</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.045</td>\n",
       "      <td>47.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>P</td>\n",
       "      <td>715.080061</td>\n",
       "      <td>2018-08-19 00:31:35.750061216+00:00</td>\n",
       "      <td>89.681135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2018-08-19 00:19:40.670000+00:00</td>\n",
       "      <td>-18.1125</td>\n",
       "      <td>-178.153</td>\n",
       "      <td>600.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>mww</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.045</td>\n",
       "      <td>47.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>PKiKP</td>\n",
       "      <td>1007.658253</td>\n",
       "      <td>2018-08-19 00:36:28.328253192+00:00</td>\n",
       "      <td>89.681135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to downsample audio data\n",
    "def downsample_audio(data, original_fs, target_fs):\n",
    "    \"\"\"Downsample audio data to the target frequency\"\"\"\n",
    "    # print(f\"Downsampling from {original_fs}Hz to {target_fs}Hz\")\n",
    "    # Calculate downsampling factor\n",
    "    factor = int(original_fs / target_fs)\n",
    "    # Apply anti-aliasing filter before downsampling\n",
    "    b, a = signal.butter(5, target_fs/2, fs=original_fs, btype='low')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    # Downsample by taking every 'factor' sample\n",
    "    downsampled_data = filtered_data[::factor]\n",
    "    return downsampled_data, target_fs\n",
    "\n",
    "# Function to apply dehazing (using spectral subtraction)\n",
    "def dehaze_audio(data, fs, frame_size=1024, overlap=0.8):\n",
    "    \"\"\"Apply spectral subtraction for dehazing\"\"\"\n",
    "    # print(\"Applying dehazing using spectral subtraction\")\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    # Estimate noise profile from first few frames\n",
    "    num_noise_frames = 5\n",
    "    noise_estimate = np.zeros(frame_size // 2 + 1)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(0, len(data) - frame_size, hop_size):\n",
    "        frame = data[i:i+frame_size]\n",
    "        if len(frame) < frame_size:\n",
    "            frame = np.pad(frame, (0, frame_size - len(frame)))\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Estimate noise from first few frames\n",
    "    for i in range(min(num_noise_frames, len(frames))):\n",
    "        noise_frame = frames[i]\n",
    "        noise_spectrum = np.abs(np.fft.rfft(noise_frame * np.hanning(frame_size)))\n",
    "        noise_estimate += noise_spectrum / num_noise_frames\n",
    "\n",
    "    # Apply spectral subtraction\n",
    "    result = np.zeros(len(data))\n",
    "    window = np.hanning(frame_size)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        windowed_frame = frame * window\n",
    "        spectrum = np.fft.rfft(windowed_frame)\n",
    "        magnitude = np.abs(spectrum)\n",
    "        phase = np.angle(spectrum)\n",
    "\n",
    "        # Subtract noise and ensure no negative values\n",
    "        magnitude = np.maximum(magnitude - noise_estimate * 1.5, 0.01 * magnitude)\n",
    "\n",
    "        # Reconstruct frame\n",
    "        enhanced_spectrum = magnitude * np.exp(1j * phase)\n",
    "        enhanced_frame = np.fft.irfft(enhanced_spectrum)\n",
    "\n",
    "        # Overlap-add\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        result[start:end] += enhanced_frame\n",
    "\n",
    "    # Normalize\n",
    "    result = result / np.max(np.abs(result))\n",
    "    return result\n",
    "\n",
    "# Function to apply Butterworth bandpass filter\n",
    "def apply_butter_bandpass(data, fs, lowcut, highcut, order=5):\n",
    "    \"\"\"Apply Butterworth bandpass filter\"\"\"\n",
    "    # print(f\"Applying bandpass filter: {lowcut}-{highcut}Hz, order {order}\")\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to create spectrogram\n",
    "def create_spectrogram(data, fs, nperseg=256, noverlap=128, cmap='viridis'):\n",
    "    \"\"\"Create and return spectrogram of the data\"\"\"\n",
    "    # print(\"Creating spectrogram\")\n",
    "    f, t, Sxx = signal.spectrogram(data, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    return f, t, Sxx\n",
    "\n",
    "# Function to detect potential seismic events using energy\n",
    "def detect_seismic_events(data, fs, window_size=5.0, threshold_factor=5.0):\n",
    "    \"\"\"Detect potential seismic events based on energy threshold\"\"\"\n",
    "    print(\"Detecting potential seismic events\")\n",
    "    window_samples = int(window_size * fs)\n",
    "    energy = []\n",
    "\n",
    "    # Calculate energy in sliding windows\n",
    "    for i in range(0, len(data) - window_samples, window_samples // 2):\n",
    "        window = data[i:i+window_samples]\n",
    "        window_energy = np.sum(window**2) / len(window)\n",
    "        energy.append(window_energy)\n",
    "\n",
    "    # Set threshold as a factor of the median energy\n",
    "    energy = np.array(energy)\n",
    "    threshold = np.median(energy) * threshold_factor\n",
    "\n",
    "    # Find events that exceed threshold\n",
    "    events = []\n",
    "    in_event = False\n",
    "    event_start = 0\n",
    "\n",
    "    for i, e in enumerate(energy):\n",
    "        if e > threshold and not in_event:\n",
    "            in_event = True\n",
    "            event_start = i * (window_samples // 2) / fs\n",
    "        elif e <= threshold and in_event:\n",
    "            in_event = False\n",
    "            event_end = i * (window_samples // 2) / fs\n",
    "            events.append((event_start, event_end))\n",
    "\n",
    "    # Handle if we're still in an event at the end\n",
    "    if in_event:\n",
    "        event_end = len(data) / fs\n",
    "        events.append((event_start, event_end))\n",
    "\n",
    "    return events, energy, threshold\n",
    "\n",
    "# Main processing function\n",
    "def process_underwater_recording(data,df,date_time,original_fs=240., target_fs=50, low_pass=1.5, high_pass=0.6):\n",
    "    \"\"\"Process underwater recording to visualize seismic events\"\"\"\n",
    "    # Load data\n",
    "\n",
    "    print(f\"Original sampling rate: {original_fs}Hz\")\n",
    "    print(f\"Original data length: {len(data)} samples ({len(data)/original_fs:.2f} seconds)\")\n",
    "\n",
    "    # Downsample to 50Hz\n",
    "    downsampled_data, new_fs = downsample_audio(data, original_fs, target_fs)\n",
    "    print(f\"Downsampled data length: {len(downsampled_data)} samples ({len(downsampled_data)/new_fs:.2f} seconds)\")\n",
    "\n",
    "    # Apply dehazing\n",
    "    dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "\n",
    "    # Apply Butterworth bandpass filter\n",
    "    filtered_data = apply_butter_bandpass(dehazed_data, new_fs, high_pass, low_pass)\n",
    "\n",
    "    # Detect potential seismic events\n",
    "    events, energy, threshold = detect_seismic_events(filtered_data, new_fs)\n",
    "    time = timedelta(minutes=10)\n",
    "    for event_start, _ in events:\n",
    "        if np.abs(time - event_start) < 10 :\n",
    "            catalogue.loc[catalogue['time'] == date_time,\"candidate\"] = True\n",
    "            break\n",
    "    if True :\n",
    "        # Create a separate figure for event detection results\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        window_size = 5.0  # seconds\n",
    "        for arrival, phase in zip(df['arrival_time'],df['phase']):\n",
    "            print(arrival, phase)\n",
    "            time = arrival - df['arrival_time'].iloc[0] + timedelta(minutes=10)\n",
    "            time = time.total_seconds()\n",
    "            plt.axvline(time, color='g', linestyle='--', label=phase)\n",
    "        time_axis = np.arange(len(energy)) * (window_size/2)\n",
    "        plt.plot(time_axis, energy)\n",
    "        plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "        plt.title('Signal Energy for Event Detection')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Energy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return filtered_data, events"
   ],
   "id": "b9c6b7aeda74548d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "from tqdm.notebook import tqdm\n",
    "PATH = f\"F:/OHASISBIO/2018/{name}\"\n",
    "ORIGINAL_FS = 240.0\n",
    "TARGET_FS = 60\n",
    "LOW_PASS = 1.5\n",
    "HIGH_PASS = 0.6\n",
    "TIME_TOLERANCE = 10  # seconds\n",
    "\n",
    "# Initialize candidate column\n",
    "catalogue['candidate'] = False\n",
    "catalogue['file_number'] = -1\n",
    "manager = DatFilesManager(PATH)\n",
    "\n",
    "def find_candidates(manager, catalogue):\n",
    "    ORIGINAL_FS = 240.0\n",
    "    TARGET_FS = 60\n",
    "    LOW_PASS = 1.5\n",
    "    HIGH_PASS = 0.6\n",
    "    TIME_TOLERANCE = 10\n",
    "    # Process each unique event in the catalogue\n",
    "    for date_time in tqdm(catalogue['time'].unique()):\n",
    "\n",
    "        event_df = catalogue[catalogue['time'] == date_time]\n",
    "        first_arrival = event_df['arrival_time'].min()\n",
    "\n",
    "        # Define time window: 10 minutes before and after first arrival\n",
    "        start = (first_arrival - timedelta(minutes=10)).replace(tzinfo=None)\n",
    "        end = (first_arrival + timedelta(minutes=10)).replace(tzinfo=None)\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess seismic data\n",
    "            data = manager.get_segment(start, end)\n",
    "            file_number = manager.find_file_name(start)\n",
    "            downsampled_data, new_fs = downsample_audio(data, ORIGINAL_FS, TARGET_FS)\n",
    "            dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "            filtered_data = apply_butter_bandpass(dehazed_data, new_fs, HIGH_PASS, LOW_PASS)\n",
    "\n",
    "            # Detect seismic events (adjust threshold as needed)\n",
    "            events, energy, threshold = detect_seismic_events(filtered_data, new_fs, threshold_factor=8.0)\n",
    "\n",
    "            # # Check if any detected event aligns with expected arrival time\n",
    "            # expected_time_sec = 600  # 10 minutes into the segment\n",
    "            # for event_start, _ in events:\n",
    "            #     if abs(event_start - expected_time_sec) < TIME_TOLERANCE:\n",
    "            #         catalogue.loc[catalogue['time'] == datetime, 'candidate'] = True\n",
    "            #         break  # Stop checking once a match is found\n",
    "            event_starts = np.array([e[0] for e in events])\n",
    "            if np.any(np.abs(event_starts - 600) < TIME_TOLERANCE):\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'candidate'] = True\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'file_number'] = file_number\n",
    "\n",
    "            # Optional: Plot detection results for debugging\n",
    "            if False:  # Set to True to enable\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                time_axis = np.arange(len(energy)) * (5.0 / 2)  # Assuming 5s window\n",
    "                plt.plot(time_axis, energy)\n",
    "                plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "                plt.axvline(expected_time_sec, color='g', linestyle='--', label='Expected Arrival')\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('Energy')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_time}: {str(e)}\")\n",
    "            continue  # Skip to next event on failure\n",
    "    return data, catalogue\n",
    "\n",
    "data, catalogue = find_candidates(manager, catalogue)\n",
    "\n",
    "#first arrival is in the middle\n",
    "#second arrival plot will be\n",
    "#dt = sec - first + timedelta(minute=10) and ect.\n",
    "\n"
   ],
   "id": "9fc5e954c547fac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# data.shape",
   "id": "cf625382c8ad4001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c1de6b7f895a2db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.figure()\n",
    "# time = np.arange(len(data))/240.\n",
    "# plt.plot(time, data)\n",
    "# df = catalogue[catalogue['time'] == datetime]\n",
    "# for arrivals in df['arrival_time']:\n",
    "#     time = arrivals - df['arrival_time'].iloc[0] + timedelta(minutes=10)\n",
    "#     time = time.total_seconds()\n",
    "#     plt.axvline(time, color='r', linestyle='--', label='Arrivals')"
   ],
   "id": "e31701035fd73059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue[catalogue[\"candidate\"]==True]['time'].unique()",
   "id": "c0c29cd76648f86b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue.to_csv(f'../../../../data/{name}_2018_M6.csv',index=False)",
   "id": "7fc080b235bda350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue[catalogue['candidate'] == True]['file_number'].unique()",
   "id": "68c1c788b08d48c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *ISC ERROR Scrapping*\n",
   "id": "e8fdc6d2f8f0f8ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T00:17:13.799253Z",
     "start_time": "2025-03-11T00:15:08.096982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# List of event timestamps\n",
    "timestamps = [\n",
    "    \"2018-02-25 17:44:44.140000\",\n",
    "    \"2018-03-25 20:14:47.690000\",\n",
    "    \"2018-03-26 09:51:00.430000\",\n",
    "    \"2018-04-02 13:40:34.840000\",\n",
    "    \"2018-07-28 22:47:38.740000\",\n",
    "    \"2018-08-05 11:46:38.630000\",\n",
    "    \"2018-08-19 00:19:40.670000\",\n",
    "    \"2018-08-19 14:56:27.490000\",\n",
    "    \"2018-09-06 15:49:18.710000\",\n",
    "    \"2018-09-10 04:19:02.630000\",\n",
    "    \"2018-09-28 10:02:45.250000\",\n",
    "    \"2018-10-29 06:54:21.250000\",\n",
    "    \"2018-12-05 04:18:08.420000\",\n",
    "    \"2018-12-11 02:26:29.420000\",\n",
    "    \"2018-12-29 03:39:09.740000\",\n",
    "]\n",
    "\n",
    "# Base URL for ISC QuakeML queries\n",
    "BASE_URL = \"https://www.isc.ac.uk/cgi-bin/web-db-run\"\n",
    "\n",
    "# Function to query ISC and get uncertainty\n",
    "def get_uncertainty(event_time):\n",
    "    # Convert event time to ISC format\n",
    "    dt = datetime.strptime(event_time, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    search_time = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-4]  # Keep only the first two digits of milliseconds\n",
    "\n",
    "    # Construct API request URL\n",
    "    params = {\n",
    "        \"request\": \"COMPREHENSIVE\",\n",
    "        \"out_format\": \"QuakeML\",\n",
    "        \"start_year\": dt.year,\n",
    "        \"start_month\": dt.month,\n",
    "        \"start_day\": dt.day,\n",
    "        \"start_time\": (dt- timedelta(seconds=60)).strftime(\"%H:%M:%S\"),\n",
    "        \"end_year\": dt.year,\n",
    "        \"end_month\": dt.month,\n",
    "        \"end_day\": dt.day,\n",
    "        \"end_time\": (dt+ timedelta(seconds=60)).strftime(\"%H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request failed for {event_time} with status code {response.status_code}\")\n",
    "        return None  # Return None if request failed\n",
    "\n",
    "    # Check if the response body is empty\n",
    "    if not response.text.strip():\n",
    "        print(f\"Empty response for {event_time}\")\n",
    "        return None  # Return None if no content in the response\n",
    "\n",
    "    try:\n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(response.text)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parsing error for {event_time}: {e}\")\n",
    "        return None  # Return None if XML parsing fails\n",
    "\n",
    "    # Find the exact matching event\n",
    "    for origin in root.findall(\".//{*}origin\"):\n",
    "        time_elem = origin.find(\"{*}time/{*}value\")\n",
    "        uncertainty_elem = origin.find(\"{*}time/{*}uncertainty\")\n",
    "        depth_elem = origin.find(\"{*}depth/{*}value\")\n",
    "\n",
    "        if time_elem is not None and uncertainty_elem is not None:\n",
    "            event_time_utc = time_elem.text\n",
    "            event_time_utc = event_time_utc.replace(\"Z\", \"\")  # Remove 'Z' for direct comparison\n",
    "\n",
    "            # Match the timestamp to milliseconds (only first two digits of milliseconds)\n",
    "            event_time_utc_ms = event_time_utc[:23]  # First two digits of milliseconds\n",
    "            search_time_ms = search_time[:23]  # First two digits of milliseconds\n",
    "\n",
    "            # Debug print: show the event time comparison for debugging\n",
    "            print(f\"Checking: Requested Time: {search_time_ms} vs Event Time: {event_time_utc_ms}\")\n",
    "\n",
    "            # Compare timestamps with only first two digits of milliseconds\n",
    "            if event_time_utc_ms == search_time_ms:  # Compare only up to two digits of milliseconds\n",
    "                uncertainty = uncertainty_elem.text\n",
    "                depth = depth_elem.text\n",
    "                return event_time_utc, uncertainty, depth\n",
    "\n",
    "    return None\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through each timestamp and get uncertainty\n",
    "for timestamp in timestamps:\n",
    "    data = get_uncertainty(timestamp)\n",
    "    if data:\n",
    "        event_time, uncertainty, depth = data\n",
    "        depth = float(depth)*1e-3 #converting to Km\n",
    "        results.append([timestamp, event_time, uncertainty, depth])\n",
    "        print(f\"Event: {timestamp} -> Uncertainty: {uncertainty} | Depth: {depth} km\")\n",
    "    else:\n",
    "        results.append([timestamp, \"Not Found\", \"N/A\"])\n",
    "        print(f\"Event: {timestamp} -> No data found\")\n",
    "\n",
    "# Save results to CSV\n",
    "csv_filename = \"seismic_uncertainty_results.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Requested Time\", \"Event Time (UTC)\", \"Uncertainty\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"\\nResults saved to {csv_filename}\")\n"
   ],
   "id": "49f7d8ece583f095",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: Requested Time: 2018-02-25T17:44:44.14 vs Event Time: 2018-02-25T17:44:44.32\n",
      "Checking: Requested Time: 2018-02-25T17:44:44.14 vs Event Time: 2018-02-25T17:44:39.65\n",
      "Checking: Requested Time: 2018-02-25T17:44:44.14 vs Event Time: 2018-02-25T17:44:40.60\n",
      "Checking: Requested Time: 2018-02-25T17:44:44.14 vs Event Time: 2018-02-25T17:44:43.82\n",
      "Checking: Requested Time: 2018-02-25T17:44:44.14 vs Event Time: 2018-02-25T17:44:44.14\n",
      "Event: 2018-02-25 17:44:44.140000 -> Uncertainty: 2.66 | Depth: 25.21 km\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:15.72\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:15.23\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:16.08\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:28.31\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:32.50\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:47.67\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:45.10\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:47.30\n",
      "Checking: Requested Time: 2018-03-25T20:14:47.69 vs Event Time: 2018-03-25T20:14:47.69\n",
      "Event: 2018-03-25 20:14:47.690000 -> Uncertainty: 2.21 | Depth: 169.0 km\n",
      "Checking: Requested Time: 2018-03-26T09:51:00.43 vs Event Time: 2018-03-26T09:51:00.97\n",
      "Checking: Requested Time: 2018-03-26T09:51:00.43 vs Event Time: 2018-03-26T09:50:59.20\n",
      "Checking: Requested Time: 2018-03-26T09:51:00.43 vs Event Time: 2018-03-26T09:51:00.11\n",
      "Checking: Requested Time: 2018-03-26T09:51:00.43 vs Event Time: 2018-03-26T09:51:00.43\n",
      "Event: 2018-03-26 09:51:00.430000 -> Uncertainty: 2.39 | Depth: 40.0 km\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:13.41\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:35.13\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:22.70\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:29.70\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:33.50\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:33.69\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:33.90\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:34.00\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:34.40\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:34.61\n",
      "Checking: Requested Time: 2018-04-02T13:40:34.84 vs Event Time: 2018-04-02T13:40:34.84\n",
      "Event: 2018-04-02 13:40:34.840000 -> Uncertainty: 2.01 | Depth: 559.0 km\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:39.17\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:34.95\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:38.07\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:38.40\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:38.70\n",
      "Checking: Requested Time: 2018-07-28T22:47:38.74 vs Event Time: 2018-07-28T22:47:38.74\n",
      "Event: 2018-07-28 22:47:38.740000 -> Uncertainty: 2.32 | Depth: 14.0 km\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:36.44\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:34.86\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:36.50\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:37.00\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:37.30\n",
      "Checking: Requested Time: 2018-08-05T11:46:38.63 vs Event Time: 2018-08-05T11:46:38.63\n",
      "Event: 2018-08-05 11:46:38.630000 -> Uncertainty: 2.14 | Depth: 34.0 km\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:39.25\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:35.70\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:37.28\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:37.90\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:38.65\n",
      "Checking: Requested Time: 2018-08-19T00:19:40.67 vs Event Time: 2018-08-19T00:19:40.67\n",
      "Event: 2018-08-19 00:19:40.670000 -> Uncertainty: 1.61 | Depth: 600.0 km\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:19.60\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:27.67\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:22.94\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:23.30\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:26.63\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:27.00\n",
      "Checking: Requested Time: 2018-08-19T14:56:27.49 vs Event Time: 2018-08-19T14:56:27.49\n",
      "Event: 2018-08-19 14:56:27.490000 -> Uncertainty: 1.78 | Depth: 21.0 km\n",
      "Checking: Requested Time: 2018-09-06T15:49:18.71 vs Event Time: 2018-09-06T15:49:18.79\n",
      "Checking: Requested Time: 2018-09-06T15:49:18.71 vs Event Time: 2018-09-06T15:49:14.30\n",
      "Checking: Requested Time: 2018-09-06T15:49:18.71 vs Event Time: 2018-09-06T15:49:17.20\n",
      "Checking: Requested Time: 2018-09-06T15:49:18.71 vs Event Time: 2018-09-06T15:49:17.29\n",
      "Checking: Requested Time: 2018-09-06T15:49:18.71 vs Event Time: 2018-09-06T15:49:18.71\n",
      "Event: 2018-09-06 15:49:18.710000 -> Uncertainty: 2.16 | Depth: 670.8100000000001 km\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:01.83\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:18:54.55\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:00.00\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:00.87\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:00.90\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:02.47\n",
      "Checking: Requested Time: 2018-09-10T04:19:02.63 vs Event Time: 2018-09-10T04:19:02.63\n",
      "Event: 2018-09-10 04:19:02.630000 -> Uncertainty: 1.97 | Depth: 115.0 km\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:01:51.28\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:14.43\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:19.53\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:28.33\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:43.80\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:40.20\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:42.00\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:43.60\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:43.70\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:44.15\n",
      "Checking: Requested Time: 2018-09-28T10:02:45.25 vs Event Time: 2018-09-28T10:02:45.25\n",
      "Event: 2018-09-28 10:02:45.250000 -> Uncertainty: 2.03 | Depth: 20.0 km\n",
      "Checking: Requested Time: 2018-10-29T06:54:21.25 vs Event Time: 2018-10-29T06:54:22.12\n",
      "Checking: Requested Time: 2018-10-29T06:54:21.25 vs Event Time: 2018-10-29T06:54:18.00\n",
      "Checking: Requested Time: 2018-10-29T06:54:21.25 vs Event Time: 2018-10-29T06:54:19.19\n",
      "Checking: Requested Time: 2018-10-29T06:54:21.25 vs Event Time: 2018-10-29T06:54:20.20\n",
      "Checking: Requested Time: 2018-10-29T06:54:21.25 vs Event Time: 2018-10-29T06:54:21.25\n",
      "Event: 2018-10-29 06:54:21.250000 -> Uncertainty: 1.60 | Depth: 10.0 km\n",
      "Checking: Requested Time: 2018-12-05T04:18:08.42 vs Event Time: 2018-12-05T04:18:08.00\n",
      "Checking: Requested Time: 2018-12-05T04:18:08.42 vs Event Time: 2018-12-05T04:18:06.10\n",
      "Checking: Requested Time: 2018-12-05T04:18:08.42 vs Event Time: 2018-12-05T04:18:06.10\n",
      "Checking: Requested Time: 2018-12-05T04:18:08.42 vs Event Time: 2018-12-05T04:18:08.42\n",
      "Event: 2018-12-05 04:18:08.420000 -> Uncertainty: 1.50 | Depth: 10.0 km\n",
      "Checking: Requested Time: 2018-12-11T02:26:29.42 vs Event Time: 2018-12-11T02:25:41.60\n",
      "Checking: Requested Time: 2018-12-11T02:26:29.42 vs Event Time: 2018-12-11T02:26:25.20\n",
      "Checking: Requested Time: 2018-12-11T02:26:29.42 vs Event Time: 2018-12-11T02:26:31.44\n",
      "Checking: Requested Time: 2018-12-11T02:26:29.42 vs Event Time: 2018-12-11T02:26:29.42\n",
      "Event: 2018-12-11 02:26:29.420000 -> Uncertainty: 1.88 | Depth: 133.0 km\n",
      "Checking: Requested Time: 2018-12-29T03:39:09.74 vs Event Time: 2018-12-29T03:39:09.96\n",
      "Checking: Requested Time: 2018-12-29T03:39:09.74 vs Event Time: 2018-12-29T03:39:08.09\n",
      "Checking: Requested Time: 2018-12-29T03:39:09.74 vs Event Time: 2018-12-29T03:39:09.00\n",
      "Checking: Requested Time: 2018-12-29T03:39:09.74 vs Event Time: 2018-12-29T03:39:09.74\n",
      "Event: 2018-12-29 03:39:09.740000 -> Uncertainty: 2.94 | Depth: 60.21 km\n",
      "\n",
      "Results saved to seismic_uncertainty_results.csv\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
