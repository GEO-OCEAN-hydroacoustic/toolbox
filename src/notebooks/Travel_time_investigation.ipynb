{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "# from numpy.doc.constants import lines\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PATH_ISC = \"/home/rsafran/PycharmProjects/toolbox/data/ISC/ISC_Ocean_indien_2018.csv\"\n",
    "cnames = ['EVENTID','TYPE','AUTHOR','DATE','TIME','LAT','LON' ,'DEPTH','DEPFIX','DEPQUAL','AUTHOR_MG','TYPE_MG','MAG',\"14\",'15','16','17','18','19','20']\n",
    "isc = pd.read_csv(PATH_ISC, comment='#',sep=',',header=None, names=cnames)\n",
    "isc['datetime'] = pd.to_datetime(isc['DATE']+' '+ isc['TIME'])\n",
    "isc.drop(['TYPE','AUTHOR','DATE','TIME',\"14\",'15','16','17','18','19','20'], axis=1, inplace=True)"
   ],
   "id": "e3ec3a91edeb55b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalog_path = '/media/rsafran/CORSAIR/Association/validated/refined_s_-60-5,35-120,350,0.8,0.6_final_filtered.npy'\n",
    "catalogue = np.load(catalog_path,allow_pickle=True).item()\n",
    "\n"
   ],
   "id": "c52a818e9f4923e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat = pd.DataFrame.from_dict(catalogue['filtered_events'], orient='columns')\n",
    "cat['lat']=cat['source_point'].apply(lambda x: x[1])\n",
    "cat['lon']=cat['source_point'].apply(lambda x: x[0])\n",
    "cat['uid']= pd.to_datetime( cat.uid.apply(lambda x: x.split('_')[0]))\n",
    "cat.sort_values(\"uid\", inplace=True, ignore_index=True)"
   ],
   "id": "3c284cd8cdbbda50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax1.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "ax1.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax1.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "cat.plot('lat','lon',ax=plt.gca(),style='.' )\n",
    "isc.plot('LON','LAT', ax=plt.gca(),style='.')\n"
   ],
   "id": "f676039296f1a514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.subplot(2,1,1)\n",
    "cat.uid.hist(bins=200)\n",
    "isc.datetime.hist(bins=200)\n",
    "plt.subplot(2,1,2)\n",
    "cat.uid.hist(bins=100)"
   ],
   "id": "96a3661de180e2c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "join_cat = pd.merge_asof(cat, isc, left_on='uid',right_on=\"datetime\",tolerance=pd.Timedelta(\"10min\"))",
   "id": "45e1feb6873a6263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "join_cat[\"time_error\"] = join_cat[\"uid\"]-join_cat[\"datetime\"]",
   "id": "d46ff0ea1001db21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "join_cat.dropna(inplace=True)\n",
    "ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax1.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "ax1.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax1.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "join_cat.plot('lat','lon',ax=plt.gca(),style='.' )\n",
    "join_cat.plot('LON','LAT', ax=plt.gca(),style='.')"
   ],
   "id": "fd6a0218a82bbb37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.mean(join_cat.time_error)",
   "id": "82063d50f3243386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arrivals",
   "id": "cba7520ef7f4016d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "\n",
    "DETECTIONS_DIR = \"/home/rsafran/Bureau/tissnet/2018\"\n",
    "ASSOCIATION_OUTPUT_DIR = \"../../../data/detection/association\"\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "STATIONS = STATIONS.by_dataset('2018')\n",
    "for st in STATIONS :\n",
    "    print(st.name)\n",
    "    print(st.get_pos(include_depth=False))"
   ],
   "id": "8d7fd6477821443b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "from pyproj import Geod\n",
    "from multiprocessing import Manager\n",
    "\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "\n",
    "GRID_LAT_BOUNDS = [-60, 5]\n",
    "GRID_LON_BOUNDS = [35, 120]\n",
    "DEPTH = 1250               # meters\n",
    "SOUND_SPEED = 1480\n",
    "PICKING_ERROR_BASE = 2\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "\n",
    "def get_isas_data(month):\n",
    "    \"\"\"Load ISAS data if not already loaded in this process\"\"\"\n",
    "    global process_local_isas_cache\n",
    "    if month not in process_local_isas_cache:\n",
    "        process_local_isas_cache[month] = isg.load_ISAS_TS(\n",
    "            ISAS_PATH, month, GRID_LAT_BOUNDS, GRID_LON_BOUNDS, fast=False\n",
    "        )\n",
    "    return process_local_isas_cache[month]\n",
    "\n",
    "def compute_travel_time(lat, lon, station_lat, station_lon, month, velocity_dict=None):\n",
    "    \"\"\"Travel time calculation using ISAS grid, loading data as needed\"\"\"\n",
    "    ds = velocity_dict[month]\n",
    "    # Error modeling with multiple components\n",
    "    picking_err = PICKING_ERROR_BASE  # Base error in picking arrival times\n",
    "    try:\n",
    "        tt, total_err, dist_m= isg.compute_travel_time(\n",
    "            lat, lon, station_lat, station_lon,\n",
    "            DEPTH, ds,\n",
    "            resolution=30,\n",
    "            verbose=False,\n",
    "            interpolate_missing=True\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(f\"Error in ISAS calculation for lat={lat}, lon={lon}, station_lat={station_lat}, station_lon={station_lon}\")\n",
    "        _, _, dist_m = geod.inv(lon, lat, station_lon, station_lat)\n",
    "        tt = dist_m / SOUND_SPEED\n",
    "        total_err = tt * 0.1\n",
    "    total_err = np.sqrt(picking_err**2 + total_err**2)\n",
    "\n",
    "    return tt, total_err, dist_m\n",
    "\n",
    "# Global cache for ISAS data per process\n",
    "process_local_isas_cache = {}\n",
    "# Create a manager for sharing data between processes\n",
    "manager = Manager()\n",
    "shared_velocity_grid = manager.dict()\n",
    "# Load ISAS data into the shared dictionary\n",
    "for m in range(1, 13):\n",
    "    print(f\"Loading ISAS data for month {m}...\")\n",
    "    shared_velocity_grid[m] = get_isas_data(m)\n"
   ],
   "id": "b57140e2d65003e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for st in STATIONS :\n",
    "    print(st.name)\n",
    "    name = st.name\n",
    "    lat, lon = st.get_pos(include_depth=False)\n",
    "    isc[f\"travel_time_{name}\"]=isc.apply(lambda x: compute_travel_time(x[\"LAT\"], x[\"LON\"], lat, lon, x.datetime.month, velocity_dict=shared_velocity_grid), axis=1)"
   ],
   "id": "3cfae8a93e9481d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "isc.columns",
   "id": "439f2a668c411e7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detections",
   "id": "3e09a2e756940494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.detection.association import load_detections\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Detections loading parameters\n",
    "RELOAD_DETECTIONS = True # if False, load files called \"detections.npy\" and \"detections_merged.npy\" containing everything instead of the raw detection output. Leave at True by default\n",
    "MIN_P_TISSNET_PRIMARY = 0.4  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.1  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 10 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "if RELOAD_DETECTIONS:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    # DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/refined_detections_merged.npy\", allow_pickle=True)"
   ],
   "id": "7a375b18cf8accf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def extract_times(detections, station_name):\n",
    "    # Extract the station mapping\n",
    "    station_mapping = {station_obj.name: station_obj for station_obj in detections.keys()}\n",
    "\n",
    "    if station_name not in station_mapping:\n",
    "        print(f\"Station {station_name} not found. Available: {list(station_mapping.keys())}\")\n",
    "        return None\n",
    "\n",
    "    station_obj = station_mapping[station_name]\n",
    "    # Extract the detection times\n",
    "    times = [row[0] for row in detections[station_obj]]\n",
    "    return times\n",
    "\n",
    "\n",
    "def check_detection(detections_df, catalogue_arrival_times, time_tolerance_seconds=15):\n",
    "    \"\"\"\n",
    "    Check each detection time against the catalogue's arrival times and return a boolean column.\n",
    "\n",
    "    Args:\n",
    "        detections_df (pd.DataFrame): The dataframe with detection times.\n",
    "        catalogue_arrival_times (list): List of datetime objects representing theoretical arrival times.\n",
    "        time_tolerance_seconds (int): Time tolerance (in seconds) for matching arrival times.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with the 'is_teleseismic' column.\n",
    "    \"\"\"\n",
    "    # Convert the catalogue arrival times to a pandas Series\n",
    "    catalogue_series = pd.to_datetime(catalogue_arrival_times)\n",
    "\n",
    "    # Expand the catalogue arrival times into a DataFrame column for comparison\n",
    "    detections_df['is_in_isc'] = detections_df['detection_time'].apply(\n",
    "        lambda detection_time: any(\n",
    "            abs(detection_time - catalogue_series) <= timedelta(seconds=time_tolerance_seconds)\n",
    "        )\n",
    "    )\n",
    "    return detections_df"
   ],
   "id": "bd7bc238a844c624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_tol= [5,10,15,20,25,30,50]\n",
    "res = []\n",
    "for time_tolerance_seconds in time_tol:\n",
    "    for st in STATIONS:\n",
    "        name = st.name\n",
    "        print(name)\n",
    "\n",
    "        isc[f\"arrival_time_{name}\"] = isc.apply(lambda x: x.datetime + pd.Timedelta(seconds=x[f'travel_time_{name}'][0]), axis=1)\n",
    "        catalogue = isc[[f\"arrival_time_{name}\",\"EVENTID\"]]\n",
    "        detection_times = extract_times(DETECTIONS, name)\n",
    "        detections_df = pd.DataFrame(detection_times, columns=['detection_time'])\n",
    "        detections_df['detection_time'] = pd.to_datetime(detections_df['detection_time'])  # Ensure correct datetime format\n",
    "\n",
    "        updated_df = check_detection(detections_df, catalogue[f'arrival_time_{name}'], time_tolerance_seconds)\n",
    "        # updated_df['is_in_isc'] = updated_df['is_in_isc']\n",
    "        print(updated_df['is_in_isc'].value_counts())\n",
    "        res.append({\"tol\":time_tolerance_seconds, name : updated_df['is_in_isc'].value_counts()[1] })"
   ],
   "id": "d056ae4fc8ea55b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stations = ['ELAN', 'MADE', 'MADW', 'NEAMS', 'RTJ', 'SSEIR', 'SSWIR', 'SWAMSbot', 'WKER2']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(9, 6),\n",
    "                        subplot_kw={'xticks': [5,10,15,20,25,30,50]})\n",
    "for j in range(len(res)):\n",
    "    for ax, interp_method in zip(axs.flat, stations):\n",
    "        name =['ELAN', 'MADE', 'MADW', 'NEAMS', 'RTJ', 'SSEIR', 'SSWIR', 'SWAMSbot', 'WKER2'][j%9]\n",
    "        ax.plot(res[j]['tol'], res[j][name],'o')\n",
    "        ax.set_title(str(interp_method))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f8aa374ab0f0461c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arrival times catalogue",
   "id": "d8b8e084bd5a907e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "isc.rename({\"EVENTID\":\"id\",\"LAT\":'latitude',\"LON\":'longitude',\"DEPTH\":'depth',\n",
    "            'datetime': 'time', 'MAG':'mag','TYPE_MG':\"magType\"}, axis = 'columns', inplace=True)"
   ],
   "id": "ab925e0a027ffcf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def isc_to_csv_cat(isc, name):\n",
    "    catalogue = isc[['time', 'latitude', 'longitude', 'depth', 'mag', 'magType']]\n",
    "    catalogue['phase'] = \"T\"\n",
    "    catalogue['travel_time'] = isc[f'travel_time_{name}'][0][0]\n",
    "    catalogue['arrival_time']= isc[f'arrival_time_{name}']\n",
    "    catalogue['distance_deg']= isc[f'travel_time_{name}'][0][-1]\n",
    "    return catalogue"
   ],
   "id": "c9416032d0f91362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue = isc_to_csv_cat(isc, 'ELAN')",
   "id": "56f74d2cb42cc60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import signal\n",
    "\n",
    "\n",
    "# Function to downsample audio data\n",
    "def downsample_audio(data, original_fs, target_fs):\n",
    "    \"\"\"Downsample audio data to the target frequency\"\"\"\n",
    "    # print(f\"Downsampling from {original_fs}Hz to {target_fs}Hz\")\n",
    "    # Calculate downsampling factor\n",
    "    factor = int(original_fs / target_fs)\n",
    "    # Apply anti-aliasing filter before downsampling\n",
    "    b, a = signal.butter(5, target_fs/2, fs=original_fs, btype='low')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    # Downsample by taking every 'factor' sample\n",
    "    downsampled_data = filtered_data[::factor]\n",
    "    return downsampled_data, target_fs\n",
    "\n",
    "# Function to apply dehazing (using spectral subtraction)\n",
    "def dehaze_audio(data, fs, frame_size=1024, overlap=0.8):\n",
    "    \"\"\"Apply spectral subtraction for dehazing\"\"\"\n",
    "    # print(\"Applying dehazing using spectral subtraction\")\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    # Estimate noise profile from first few frames\n",
    "    num_noise_frames = 5\n",
    "    noise_estimate = np.zeros(frame_size // 2 + 1)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(0, len(data) - frame_size, hop_size):\n",
    "        frame = data[i:i+frame_size]\n",
    "        if len(frame) < frame_size:\n",
    "            frame = np.pad(frame, (0, frame_size - len(frame)))\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Estimate noise from first few frames\n",
    "    for i in range(min(num_noise_frames, len(frames))):\n",
    "        noise_frame = frames[i]\n",
    "        noise_spectrum = np.abs(np.fft.rfft(noise_frame * np.hanning(frame_size)))\n",
    "        noise_estimate += noise_spectrum / num_noise_frames\n",
    "\n",
    "    # Apply spectral subtraction\n",
    "    result = np.zeros(len(data))\n",
    "    window = np.hanning(frame_size)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        windowed_frame = frame * window\n",
    "        spectrum = np.fft.rfft(windowed_frame)\n",
    "        magnitude = np.abs(spectrum)\n",
    "        phase = np.angle(spectrum)\n",
    "\n",
    "        # Subtract noise and ensure no negative values\n",
    "        magnitude = np.maximum(magnitude - noise_estimate * 1.5, 0.01 * magnitude)\n",
    "\n",
    "        # Reconstruct frame\n",
    "        enhanced_spectrum = magnitude * np.exp(1j * phase)\n",
    "        enhanced_frame = np.fft.irfft(enhanced_spectrum)\n",
    "\n",
    "        # Overlap-add\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        result[start:end] += enhanced_frame\n",
    "\n",
    "    # Normalize\n",
    "    result = result / np.max(np.abs(result))\n",
    "    return result\n",
    "\n",
    "# Function to apply Butterworth bandpass filter\n",
    "def apply_butter_bandpass(data, fs, lowcut, highcut, order=5):\n",
    "    \"\"\"Apply Butterworth bandpass filter\"\"\"\n",
    "    # print(f\"Applying bandpass filter: {lowcut}-{highcut}Hz, order {order}\")\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to create spectrogram\n",
    "def create_spectrogram(data, fs, nperseg=256, noverlap=128, cmap='viridis'):\n",
    "    \"\"\"Create and return spectrogram of the data\"\"\"\n",
    "    # print(\"Creating spectrogram\")\n",
    "    f, t, Sxx = signal.spectrogram(data, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    return f, t, Sxx\n",
    "\n",
    "# Function to detect potential seismic events using energy\n",
    "def detect_seismic_events(data, fs, window_size=5.0, threshold_factor=5.0):\n",
    "    \"\"\"Detect potential seismic events based on energy threshold\"\"\"\n",
    "    print(\"Detecting potential seismic events\")\n",
    "    window_samples = int(window_size * fs)\n",
    "    energy = []\n",
    "\n",
    "    # Calculate energy in sliding windows\n",
    "    for i in range(0, len(data) - window_samples, window_samples // 2):\n",
    "        window = data[i:i+window_samples]\n",
    "        window_energy = np.sum(window**2) / len(window)\n",
    "        energy.append(window_energy)\n",
    "\n",
    "    # Set threshold as a factor of the median energy\n",
    "    energy = np.array(energy)\n",
    "    threshold = np.median(energy) * threshold_factor\n",
    "\n",
    "    # Find events that exceed threshold\n",
    "    events = []\n",
    "    in_event = False\n",
    "    event_start = 0\n",
    "\n",
    "    for i, e in enumerate(energy):\n",
    "        if e > threshold and not in_event:\n",
    "            in_event = True\n",
    "            event_start = i * (window_samples // 2) / fs\n",
    "        elif e <= threshold and in_event:\n",
    "            in_event = False\n",
    "            event_end = i * (window_samples // 2) / fs\n",
    "            events.append((event_start, event_end))\n",
    "\n",
    "    # Handle if we're still in an event at the end\n",
    "    if in_event:\n",
    "        event_end = len(data) / fs\n",
    "        events.append((event_start, event_end))\n",
    "\n",
    "    return events, energy, threshold\n",
    "\n",
    "# Main processing function\n",
    "def process_underwater_recording(data,df,date_time,original_fs=240., target_fs=50, low_pass=1.5, high_pass=0.6):\n",
    "    \"\"\"Process underwater recording to visualize seismic events\"\"\"\n",
    "    # Load data\n",
    "\n",
    "    print(f\"Original sampling rate: {original_fs}Hz\")\n",
    "    print(f\"Original data length: {len(data)} samples ({len(data)/original_fs:.2f} seconds)\")\n",
    "\n",
    "    # Downsample to 50Hz\n",
    "    downsampled_data, new_fs = downsample_audio(data, original_fs, target_fs)\n",
    "    print(f\"Downsampled data length: {len(downsampled_data)} samples ({len(downsampled_data)/new_fs:.2f} seconds)\")\n",
    "\n",
    "    # Apply dehazing\n",
    "    dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "\n",
    "    # Apply Butterworth bandpass filter\n",
    "    filtered_data = apply_butter_bandpass(dehazed_data, new_fs, high_pass, low_pass)\n",
    "\n",
    "    # Detect potential seismic events\n",
    "    events, energy, threshold = detect_seismic_events(filtered_data, new_fs)\n",
    "    time = timedelta(minutes=10)\n",
    "    for event_start, _ in events:\n",
    "        if np.abs(time - event_start) < 10 :\n",
    "            catalogue.loc[catalogue['time'] == date_time,\"candidate\"] = True\n",
    "            break\n",
    "    if True :\n",
    "        # Create a separate figure for event detection results\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        window_size = 5.0  # seconds\n",
    "        for arrival, phase in zip(df['arrival_time'],df['phase']):\n",
    "            print(arrival, phase)\n",
    "            time = arrival - df['arrival_time'].iloc[0] + timedelta(minutes=10)\n",
    "            time = time.total_seconds()\n",
    "            plt.axvline(time, color='g', linestyle='--', label=phase)\n",
    "        time_axis = np.arange(len(energy)) * (window_size/2)\n",
    "        plt.plot(time_axis, energy)\n",
    "        plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "        plt.title('Signal Energy for Event Detection')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Energy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return filtered_data, events"
   ],
   "id": "412983615052fef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "# Configuration\n",
    "from tqdm.notebook import tqdm\n",
    "# PATH = f\"F:/OHASISBIO/2018/{name}\"\n",
    "name = \"ELAN\"\n",
    "PATH = f\"/media/rsafran/CORSAIR/OHASISBIO/2018/{name}\"\n",
    "ORIGINAL_FS = 240.0\n",
    "TARGET_FS = 60\n",
    "LOW_PASS = 1.5\n",
    "HIGH_PASS = 0.6\n",
    "TIME_TOLERANCE = 10  # seconds\n",
    "\n",
    "# Initialize candidate column\n",
    "catalogue['candidate'] = False\n",
    "catalogue['file_number'] = -1\n",
    "manager = DatFilesManager(PATH,kwargs='raw')\n",
    "\n",
    "def find_candidates(manager, catalogue):\n",
    "    ORIGINAL_FS = 240.0\n",
    "    TARGET_FS = 60\n",
    "    LOW_PASS = 1.5\n",
    "    HIGH_PASS = 0.6\n",
    "    TIME_TOLERANCE = 10\n",
    "    # Process each unique event in the catalogue\n",
    "    for date_time in tqdm(catalogue['time'].unique()):\n",
    "\n",
    "        event_df = catalogue[catalogue['time'] == date_time]\n",
    "        first_arrival = event_df['arrival_time'].min()\n",
    "\n",
    "        # Define time window: 10 minutes before and after first arrival\n",
    "        start = (first_arrival - timedelta(minutes=10)).replace(tzinfo=None)\n",
    "        end = (first_arrival + timedelta(minutes=10)).replace(tzinfo=None)\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess seismic data\n",
    "            data = manager.get_segment(start, end)\n",
    "            file_number = manager.find_file_name(start)\n",
    "            downsampled_data, new_fs = downsample_audio(data, ORIGINAL_FS, TARGET_FS)\n",
    "            dehazed_data = dehaze_audio(downsampled_data, new_fs)\n",
    "            filtered_data = apply_butter_bandpass(dehazed_data, new_fs, HIGH_PASS, LOW_PASS)\n",
    "\n",
    "            # Detect seismic events (adjust threshold as needed)\n",
    "            events, energy, threshold = detect_seismic_events(filtered_data, new_fs, threshold_factor=8.0)\n",
    "\n",
    "            # # Check if any detected event aligns with expected arrival time\n",
    "            expected_time_sec = 600  # 10 minutes into the segment\n",
    "            # for event_start, _ in events:\n",
    "            #     if abs(event_start - expected_time_sec) < TIME_TOLERANCE:\n",
    "            #         catalogue.loc[catalogue['time'] == datetime, 'candidate'] = True\n",
    "            #         break  # Stop checking once a match is found\n",
    "            event_starts = np.array([e[0] for e in events])\n",
    "            if True :#np.any(np.abs(event_starts - 600) < TIME_TOLERANCE):\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'candidate'] = True\n",
    "                catalogue.loc[catalogue['time'] == date_time, 'file_number'] = file_number\n",
    "\n",
    "            # Optional: Plot detection results for debugging\n",
    "            if False:  # Set to True to enable\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                time_axis = np.arange(len(energy)) * (5.0 / 2)  # Assuming 5s window\n",
    "                plt.plot(time_axis, energy)\n",
    "                plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "                plt.axvline(expected_time_sec, color='g', linestyle='--', label='Expected Arrival')\n",
    "                plt.xlabel('Time (s)')\n",
    "                plt.ylabel('Energy')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_time}: {str(e)}\")\n",
    "            continue  # Skip to next event on failure\n",
    "    return data, catalogue\n",
    "\n",
    "data, catalogue = find_candidates(manager, catalogue)\n",
    "\n",
    "#first arrival is in the middle\n",
    "#second arrival plot will be\n",
    "#dt = sec - first + timedelta(minute=10) and ect."
   ],
   "id": "afb619a57c902d00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "catalogue.to_csv(f'../../data/{name}_2018_T.csv',index=False)",
   "id": "556a8b4e53e2c024",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
