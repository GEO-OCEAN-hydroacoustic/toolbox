{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from numpy.doc.constants import lines\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PATH_ISC = \"/home/rsafran/PycharmProjects/toolbox/data/ISC/ISC_Ocean_indien_2018.csv\"\n",
    "cnames = ['EVENTID','TYPE','AUTHOR','DATE','TIME','LAT','LON' ,'DEPTH','DEPFIX','DEPQUAL','AUTHOR_MG','TYPE_MG','MAG',\"14\",'15','16','17','18','19','20']\n",
    "isc = pd.read_csv(PATH_ISC, comment='#',sep=',',header=None, names=cnames)\n",
    "isc['datetime'] = pd.to_datetime(isc['DATE']+' '+ isc['TIME'])\n",
    "isc.drop(['TYPE','AUTHOR','DATE','TIME',\"14\",'15','16','17','18','19','20'], axis=1, inplace=True)"
   ],
   "id": "e3ec3a91edeb55b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalog_path = '/media/rsafran/CORSAIR/Association/validated/refined_s_-60-5,35-120,350,0.8,0.6_final_filtered.npy'\n",
    "catalogue = np.load(catalog_path,allow_pickle=True).item()\n",
    "\n"
   ],
   "id": "c52a818e9f4923e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat = pd.DataFrame.from_dict(catalogue['filtered_events'], orient='columns')\n",
    "cat['lat']=cat['source_point'].apply(lambda x: x[1])\n",
    "cat['lon']=cat['source_point'].apply(lambda x: x[0])\n",
    "cat['uid']= pd.to_datetime( cat.uid.apply(lambda x: x.split('_')[0]))\n",
    "cat.sort_values(\"uid\", inplace=True, ignore_index=True)"
   ],
   "id": "3c284cd8cdbbda50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax1.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "ax1.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax1.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "cat.plot('lat','lon',ax=plt.gca(),style='.' )\n",
    "isc.plot('LON','LAT', ax=plt.gca(),style='.')\n"
   ],
   "id": "f676039296f1a514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.subplot(2,1,1)\n",
    "cat.uid.hist(bins=200)\n",
    "isc.datetime.hist(bins=200)\n",
    "plt.subplot(2,1,2)\n",
    "cat.uid.hist(bins=100)"
   ],
   "id": "96a3661de180e2c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "join_cat = pd.merge_asof(cat, isc, left_on='uid',right_on=\"datetime\",tolerance=pd.Timedelta(\"30min\"))",
   "id": "45e1feb6873a6263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "join_cat[\"time_error\"] = join_cat[\"uid\"]-join_cat[\"datetime\"]",
   "id": "d46ff0ea1001db21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "join_cat.dropna(inplace=True)\n",
    "ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax1.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "ax1.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax1.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "join_cat.plot('lat','lon',ax=plt.gca(),style='.' )\n",
    "join_cat.plot('LON','LAT', ax=plt.gca(),style='.')"
   ],
   "id": "fd6a0218a82bbb37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.mean(join_cat.time_error)",
   "id": "82063d50f3243386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arrivals",
   "id": "cba7520ef7f4016d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "\n",
    "DETECTIONS_DIR = \"/home/rsafran/Bureau/tissnet/2018\"\n",
    "ASSOCIATION_OUTPUT_DIR = \"../../../data/detection/association\"\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "STATIONS = STATIONS.by_dataset('2018')\n",
    "for st in STATIONS :\n",
    "    print(st.name)\n",
    "    print(st.get_pos(include_depth=False))"
   ],
   "id": "8d7fd6477821443b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "from pyproj import Geod\n",
    "from multiprocessing import Manager\n",
    "\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "\n",
    "GRID_LAT_BOUNDS = [-60, 5]\n",
    "GRID_LON_BOUNDS = [35, 120]\n",
    "DEPTH = 1250               # meters\n",
    "SOUND_SPEED = 1480\n",
    "PICKING_ERROR_BASE = 2\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "\n",
    "def get_isas_data(month):\n",
    "    \"\"\"Load ISAS data if not already loaded in this process\"\"\"\n",
    "    global process_local_isas_cache\n",
    "    if month not in process_local_isas_cache:\n",
    "        process_local_isas_cache[month] = isg.load_ISAS_TS(\n",
    "            ISAS_PATH, month, GRID_LAT_BOUNDS, GRID_LON_BOUNDS, fast=False\n",
    "        )\n",
    "    return process_local_isas_cache[month]\n",
    "\n",
    "def compute_travel_time(lat, lon, station_lat, station_lon, month, velocity_dict=None):\n",
    "    \"\"\"Travel time calculation using ISAS grid, loading data as needed\"\"\"\n",
    "    ds = velocity_dict[month]\n",
    "    # Error modeling with multiple components\n",
    "    picking_err = PICKING_ERROR_BASE  # Base error in picking arrival times\n",
    "    try:\n",
    "        tt, total_err, dist_m= isg.compute_travel_time(\n",
    "            lat, lon, station_lat, station_lon,\n",
    "            DEPTH, ds,\n",
    "            resolution=30,\n",
    "            verbose=False,\n",
    "            interpolate_missing=True\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(f\"Error in ISAS calculation for lat={lat}, lon={lon}, station_lat={station_lat}, station_lon={station_lon}\")\n",
    "        _, _, dist_m = geod.inv(lon, lat, station_lon, station_lat)\n",
    "        tt = dist_m / SOUND_SPEED\n",
    "        total_err = tt * 0.1\n",
    "    total_err = np.sqrt(picking_err**2 + total_err**2)\n",
    "\n",
    "    return tt, total_err, dist_m\n",
    "\n",
    "# Global cache for ISAS data per process\n",
    "process_local_isas_cache = {}\n",
    "# Create a manager for sharing data between processes\n",
    "manager = Manager()\n",
    "shared_velocity_grid = manager.dict()\n",
    "# Load ISAS data into the shared dictionary\n",
    "for m in range(1, 13):\n",
    "    print(f\"Loading ISAS data for month {m}...\")\n",
    "    shared_velocity_grid[m] = get_isas_data(m)\n"
   ],
   "id": "b57140e2d65003e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for st in STATIONS :\n",
    "    print(st.name)\n",
    "    name = st.name\n",
    "    lat, lon = st.get_pos(include_depth=False)\n",
    "    isc[f\"travel_time_{name}\"]=isc.apply(lambda x: compute_travel_time(x[\"LAT\"], x[\"LON\"], lat, lon, x.datetime.month, velocity_dict=shared_velocity_grid), axis=1)"
   ],
   "id": "3cfae8a93e9481d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "isc",
   "id": "439f2a668c411e7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detections",
   "id": "3e09a2e756940494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.detection.association import load_detections\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Detections loading parameters\n",
    "RELOAD_DETECTIONS = True # if False, load files called \"detections.npy\" and \"detections_merged.npy\" containing everything instead of the raw detection output. Leave at True by default\n",
    "MIN_P_TISSNET_PRIMARY = 0.4  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.1  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 10 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "if RELOAD_DETECTIONS:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    # DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/refined_detections_merged.npy\", allow_pickle=True)"
   ],
   "id": "7a375b18cf8accf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def extract_times(detections, station_name):\n",
    "    # Extract the station mapping\n",
    "    station_mapping = {station_obj.name: station_obj for station_obj in detections.keys()}\n",
    "\n",
    "    if station_name not in station_mapping:\n",
    "        print(f\"Station {station_name} not found. Available: {list(station_mapping.keys())}\")\n",
    "        return None\n",
    "\n",
    "    station_obj = station_mapping[station_name]\n",
    "    # Extract the detection times\n",
    "    times = [row[0] for row in detections[station_obj]]\n",
    "    return times\n",
    "\n",
    "\n",
    "def check_detection(detections_df, catalogue_arrival_times, time_tolerance_seconds=15):\n",
    "    \"\"\"\n",
    "    Check each detection time against the catalogue's arrival times and return a boolean column.\n",
    "\n",
    "    Args:\n",
    "        detections_df (pd.DataFrame): The dataframe with detection times.\n",
    "        catalogue_arrival_times (list): List of datetime objects representing theoretical arrival times.\n",
    "        time_tolerance_seconds (int): Time tolerance (in seconds) for matching arrival times.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with the 'is_teleseismic' column.\n",
    "    \"\"\"\n",
    "    # Convert the catalogue arrival times to a pandas Series\n",
    "    catalogue_series = pd.to_datetime(catalogue_arrival_times)\n",
    "\n",
    "    # Expand the catalogue arrival times into a DataFrame column for comparison\n",
    "    detections_df['is_in_isc'] = detections_df['detection_time'].apply(\n",
    "        lambda detection_time: any(\n",
    "            abs(detection_time - catalogue_series) <= timedelta(seconds=time_tolerance_seconds)\n",
    "        )\n",
    "    )\n",
    "    return detections_df"
   ],
   "id": "bd7bc238a844c624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_tol= [5,10,15,20,25,30,50]\n",
    "res = []\n",
    "for time_tolerance_seconds in time_tol:\n",
    "    for st in STATIONS:\n",
    "        name = st.name\n",
    "        print(name)\n",
    "\n",
    "        isc[f\"arrival_time_{name}\"] = isc.apply(lambda x: x.datetime + pd.Timedelta(seconds=x[f'travel_time_{name}'][0]), axis=1)\n",
    "        catalogue = isc[[f\"arrival_time_{name}\",\"EVENTID\"]]\n",
    "        detection_times = extract_times(DETECTIONS, name)\n",
    "        detections_df = pd.DataFrame(detection_times, columns=['detection_time'])\n",
    "        detections_df['detection_time'] = pd.to_datetime(detections_df['detection_time'])  # Ensure correct datetime format\n",
    "\n",
    "        updated_df = check_detection(detections_df, catalogue[f'arrival_time_{name}'], time_tolerance_seconds)\n",
    "        # updated_df['is_in_isc'] = updated_df['is_in_isc']\n",
    "        print(updated_df['is_in_isc'].value_counts())\n",
    "        res.append({\"tol\":time_tolerance_seconds, name : updated_df['is_in_isc'].value_counts()[1] })"
   ],
   "id": "d056ae4fc8ea55b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res",
   "id": "b91146e119c1a970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig,axes = plt.subplots(ncols=3,nrows=3)\n",
    "for j in range(len(res)):\n",
    "    name =['ELAN', 'MADE', 'MADW', 'NEAMS', 'RTJ', 'SSEIR', 'SSWIR', 'SWAMSbot', 'WKER2'][j%9]\n",
    "    plt.plot(res[j]['tol'], res[j][name])"
   ],
   "id": "437552c906815f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stations = ['ELAN', 'MADE', 'MADW', 'NEAMS', 'RTJ', 'SSEIR', 'SSWIR', 'SWAMSbot', 'WKER2']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(9, 6),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "for j in range(len(res)):\n",
    "    for ax, interp_method in zip(axs.flat, stations):\n",
    "        name =['ELAN', 'MADE', 'MADW', 'NEAMS', 'RTJ', 'SSEIR', 'SSWIR', 'SWAMSbot', 'WKER2'][j%9]\n",
    "        ax.plot(res[j]['tol'], res[j][name],'o')\n",
    "        ax.set_title(str(interp_method))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f8aa374ab0f0461c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res[0:9][[0]]",
   "id": "3e26b45c6c488a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eac200c5103d54cd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
