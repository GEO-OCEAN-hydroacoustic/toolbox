{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook aims at applying TiSSNet on all the demo data.",
   "id": "447206cd5027c17d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.signal.make_spectrogram import make_spectrogram\n",
    "from utils.detection.TiSSNet import TiSSNet, process_batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalog_path = \"../../../data/demo\"\n",
    "tissnet_checkpoint = \"../../../data/models/TiSSNet/torch_save\"\n",
    "out_dir = \"../../../data/detection/TiSSNet/demo\"  # where files will be saved\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)  # create output directory if needed\n",
    "\n",
    "stations = StationsCatalog(catalog_path).filter_out_undated() # remove stations with no start / end dates\n",
    "\n",
    "model_det = TiSSNet()\n",
    "model_det.load_state_dict(torch.load(tissnet_checkpoint))\n",
    "\n",
    "DELTA = datetime.timedelta(seconds=3600)  # duration of segments that are given to TiSSNet\n",
    "OVERLAP = 0.02   # overlap between those segments (no link with STFT)\n",
    "STEP = (1 - OVERLAP) * DELTA\n",
    "batch_size = 1  # number of segments that are fed together to TiSSNet\n",
    "\n",
    "# parameters of peak finding (TiSSNet outputs 1 value per spectrogram time bin, we use a peak finding algorithm to save only the peaks)\n",
    "TISSNET_PROMINENCE = 0.05\n",
    "ALLOWED_ERROR_S = 5\n",
    "MIN_HEIGHT = 0.05\n",
    "\n",
    "TIME_RES = 0.5342  # duration of each spectrogram pixel in seconds\n",
    "FREQ_RES = 0.9375  # f of each spectrogram pixel in Hz\n",
    "\n",
    "device = \"cpu\"  # if there is a GPU and CUDA is installed, device can be set to \"cuda\" instead"
   ],
   "id": "dd5df7fb2321b1b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for station in stations:\n",
    "    manager = station.get_manager()\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\" # results are saved as stacked pickle files\n",
    "\n",
    "    print(f\"Starting detection on {manager.name}\")\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start)/STEP))\n",
    "    start_idx = 0\n",
    "    batch_dates, batch_process = [], []\n",
    "\n",
    "    # if some detection has already been run, we start where it was stopped\n",
    "    already_done = []\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    already_done.append(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        last_date = already_done[-1][0]\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    # we start detection\n",
    "    for i in tqdm(range(steps)):\n",
    "        if i < start_idx:\n",
    "            continue # this is just to fill tqdm progress bar in case we loaded an old detection file\n",
    "\n",
    "        # important : prefer index multiplication over incrementation to avoid rounding errors\n",
    "        # (i.e. seg_start = start + i * STEP is way better than seg_start = seg_start + STEP)\n",
    "        seg_start = start + i * STEP\n",
    "        seg_end = min(end, seg_start + DELTA)\n",
    "        if seg_start >= seg_end:\n",
    "            break\n",
    "\n",
    "        # add data to batch\n",
    "        data = manager.get_segment(seg_start, seg_end)\n",
    "        spectrogram = make_spectrogram(data, manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "        spectrogram = spectrogram[np.newaxis, :, :]  # add a dummy dimension, this stands for the channel number (here we are in grayscale, i.e. only one value for each pixel)\n",
    "        input_data = Resize((128, spectrogram.shape[-1]))(torch.from_numpy(spectrogram)) # resize data\n",
    "        batch_dates.append(seg_start)\n",
    "        batch_process.append(input_data)\n",
    "\n",
    "        # check if the batch is ready to be processed\n",
    "        if len(batch_process) == batch_size or i == steps-1:\n",
    "            if batch_size > 1 and (batch_process[-1].shape != batch_process[0].shape or batch_process[-2].shape != batch_process[-1].shape):\n",
    "                # last (and probably the one before because of overlaps) batch has a last element shorter than the others, we thus make three batches\n",
    "                rlastlast = process_batch(batch_process[-2], device, model_det)\n",
    "                rlast = process_batch(batch_process[-1], device, model_det)\n",
    "                rfirst = process_batch(batch_process[:-2], device, model_det)\n",
    "                res = list(rfirst) + [rlastlast] + [rlast]\n",
    "            else:\n",
    "                res = process_batch(batch_process, device, model_det)\n",
    "\n",
    "            # now proceed to peak finding for each window to keep only the peaks\n",
    "            for i, (seg_start, r) in enumerate(zip(batch_dates, res)):\n",
    "                peaks = find_peaks(r, height=0, distance=ALLOWED_ERROR_S / TIME_RES, prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * TIME_RES\n",
    "                peaks = [(seg_start + datetime.timedelta(seconds=time_s[j]), peaks[1][\"peak_heights\"][j]) for j in range(len(time_s)) if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT]\n",
    "\n",
    "                with open(out_file, \"ab\") as f:\n",
    "                    for i, (d, p) in enumerate(peaks):\n",
    "                        pickle.dump([d, p.astype(np.float16)], f)  # we write detections as a list of (date, peak probability)\n",
    "\n",
    "            batch_dates, batch_process = [], []"
   ],
   "id": "1514ed570bed70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Take a look at the results",
   "id": "9f5a9a184ef2087a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detection_file = f\"{out_dir}/MAHY0_MAHY01.pkl\"\n",
    "d = []\n",
    "with open(detection_file, \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            d.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "print(f\"{len(detection_file)} detections found\")\n",
    "\n",
    "\n",
    "dates_plot = np.array(d)[:,0]\n",
    "offset = 1\n",
    "df = pd.DataFrame({'date': dates_plot})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "counts = df.resample('1H', on='date').size().asfreq('1H', fill_value=0)\n",
    "\n",
    "sns.barplot(x=counts.index.strftime(\"%Hh\"), y=counts.values)\n",
    "plt.title(f\"Hourly detections from {dates_plot[0].day:02d}/{dates_plot[0].month:02d}/{dates_plot[0].year}\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of events\")"
   ],
   "id": "7be9c377810e6a4c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
