{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "\n",
    "from utils.physics.sound_model.ellipsoidal_sound_model import GridEllipsoidalSoundModel\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "import utils.physics.sound_model.ISAS_grid as isg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2020\"\n",
    "DETECTIONS_DIR = \"/media/rsafran/CORSAIR/detections_CTBT/\"\n",
    "lat_bounds = [-60, 5]\n",
    "lon_bounds = [35, 120]\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "grid_size = 400\n",
    "# Define start and end points\n",
    "lat1, lon1 = -31.5758,83.2423    # Example: Station MADE\n",
    "lat2, lon2 = -59.99254334995582,35.00354027003104  # Example: Station NEAMS\n",
    "depth = 1200    # Depth in meters\n",
    "\n",
    "method = 'min'\n",
    "year = '2018'\n",
    "PATH = f\"/media/rsafran/CORSAIR/ISAS/86442/field/{year}\"\n",
    "out_dir = f\"/media/rsafran/CORSAIR/ISAS/extracted/{year}/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "result = isg.load_ISAS_extracted(out_dir, 4)\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/extracted/2018\"\n",
    "arr = os.listdir(ISAS_PATH)\n",
    "file_list = [os.path.join(ISAS_PATH, fname) for fname in arr if fname.endswith('.nc')]\n",
    "SOUND_MODEL = GridEllipsoidalSoundModel(file_list)\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "if False:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    det_files = [f for f in det_files if \"2018\" in f ]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    # DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/refined_detections_merged.npy\", allow_pickle=True)\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e455940b714fb43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "def compute_latitude_line_irregular(lat_idx, lat, lons_for_this_lat, stations, sound_model, date):\n",
    "    \"\"\"\n",
    "    Calcule tous les temps de trajet pour une ligne de latitude donnée\n",
    "    avec un nombre variable de points de longitude\n",
    "    \"\"\"\n",
    "    line_results = {}\n",
    "\n",
    "    # Pré-calcul des positions des stations pour éviter les appels répétés\n",
    "    station_positions = {s: s.get_pos() for s in stations}\n",
    "\n",
    "    for s in stations:\n",
    "        station_pos = station_positions[s]\n",
    "        line_times = np.zeros(len(lons_for_this_lat))\n",
    "\n",
    "        # Calcul vectorisé pour tous les points de longitude de cette latitude\n",
    "        for lon_idx, lon in enumerate(lons_for_this_lat):\n",
    "            line_times[lon_idx] = sound_model.get_sound_travel_time(\n",
    "                [lat, lon], station_pos, date=date\n",
    "            )\n",
    "\n",
    "        line_results[s] = line_times\n",
    "\n",
    "    return lat_idx, line_results\n",
    "\n",
    "def compute_grids_by_latitude_irregular(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                                       pick_uncertainty=5, sound_speed_uncertainty=2,\n",
    "                                       n_workers=None, lons_per_lat=None):\n",
    "    \"\"\"\n",
    "    Version optimisée pour grilles irrégulières où chaque latitude peut avoir\n",
    "    un nombre différent de points de longitude\n",
    "\n",
    "    Args:\n",
    "        lat_bounds: tuple (min_lat, max_lat)\n",
    "        lon_bounds: tuple (min_lon, max_lon) - utilisé seulement si lons_per_lat est None\n",
    "        grid_size: nombre de points de latitude\n",
    "        sound_model: modèle de propagation du son\n",
    "        stations: liste des stations\n",
    "        pick_uncertainty: incertitude de picking en secondes\n",
    "        sound_speed_uncertainty: incertitude sur la vitesse du son\n",
    "        n_workers: nombre de workers pour le multiprocessing\n",
    "        lons_per_lat: liste de listes ou dict, où lons_per_lat[i] contient les longitudes\n",
    "                     pour la latitude d'index i (du nord au sud)\n",
    "    \"\"\"\n",
    "    # Génération des latitudes (régulières)\n",
    "    pts_lat = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "\n",
    "    # Gestion des longitudes irrégulières\n",
    "    if lons_per_lat is None:\n",
    "        # Grille régulière par défaut\n",
    "        pts_lon_regular = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "        pts_lon = [pts_lon_regular for _ in range(len(pts_lat))]\n",
    "    else:\n",
    "        # Vérification de la cohérence\n",
    "        if len(lons_per_lat) != len(pts_lat):\n",
    "            raise ValueError(f\"lons_per_lat doit avoir {len(pts_lat)} entrées, une pour chaque latitude\")\n",
    "        pts_lon = lons_per_lat\n",
    "\n",
    "    # Calcul de la tolérance de grille\n",
    "    grid_max_res_time = (0.5 * np.sqrt(2) * (pts_lat[1] - pts_lat[0]) * 111_000) / (\n",
    "                sound_model.constant_velocity - sound_speed_uncertainty)\n",
    "    grid_tolerance = grid_max_res_time + pick_uncertainty\n",
    "    print(f\"Grid tolerance of {grid_tolerance:.2f}s\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = min(len(pts_lat), mp.cpu_count())\n",
    "\n",
    "    # Information sur la grille\n",
    "    total_points = sum(len(lons) for lons in pts_lon)\n",
    "    print(f\"Processing {len(pts_lat)} latitude lines with {total_points} total points using {n_workers} workers\")\n",
    "    print(f\"Points per latitude: min={min(len(lons) for lons in pts_lon)}, \"\n",
    "          f\"max={max(len(lons) for lons in pts_lon)}, \"\n",
    "          f\"avg={total_points/len(pts_lat):.1f}\")\n",
    "\n",
    "    # Date fixe pour tous les calculs\n",
    "    calc_date = datetime.datetime(year=2020, month=1, day=1)\n",
    "\n",
    "    # Initialisation des structures de données avec des listes de arrays numpy\n",
    "    # car chaque latitude peut avoir un nombre différent de points\n",
    "    grid_station_travel_time = {\n",
    "        s: [np.zeros(len(pts_lon[i])) for i in range(len(pts_lat))]\n",
    "        for s in stations\n",
    "    }\n",
    "\n",
    "    # Préparation de la fonction pour multiprocessing\n",
    "    compute_line_func = partial(\n",
    "        compute_latitude_line_irregular,\n",
    "        stations=stations,\n",
    "        sound_model=sound_model,\n",
    "        date=calc_date\n",
    "    )\n",
    "\n",
    "    # Traitement parallèle ligne par ligne\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Soumettre toutes les tâches\n",
    "        futures = []\n",
    "        for lat_idx, lat in enumerate(pts_lat):\n",
    "            future = executor.submit(compute_line_func, lat_idx, lat, pts_lon[lat_idx])\n",
    "            futures.append(future)\n",
    "\n",
    "        # Récupération des résultats avec barre de progression\n",
    "        completed = 0\n",
    "        for future in futures:\n",
    "            lat_idx, line_results = future.result()\n",
    "\n",
    "            # Assemblage des résultats dans la grille finale\n",
    "            for station in stations:\n",
    "                grid_station_travel_time[station][lat_idx] = line_results[station]\n",
    "\n",
    "            completed += 1\n",
    "            if completed % max(1, len(pts_lat) // 10) == 0:\n",
    "                print(f\"Completed {completed}/{len(pts_lat)} latitude lines ({100*completed/len(pts_lat):.1f}%)\")\n",
    "\n",
    "    print(\"Computing station couple travel times...\")\n",
    "    # Calcul des différences de temps de trajet entre paires de stations\n",
    "    grid_station_couple_travel_time = {}\n",
    "    for s in stations:\n",
    "        grid_station_couple_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            # Création d'une liste de différences pour chaque latitude\n",
    "            grid_station_couple_travel_time[s][s2] = [\n",
    "                grid_station_travel_time[s2][lat_idx] - grid_station_travel_time[s][lat_idx]\n",
    "                for lat_idx in range(len(pts_lat))\n",
    "            ]\n",
    "\n",
    "    print(\"Computing station max travel times...\")\n",
    "    # Calcul des temps de trajet maximum entre stations\n",
    "    station_max_travel_time = {}\n",
    "    for s in stations:\n",
    "        station_max_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            station_max_travel_time[s][s2] = sound_model.get_sound_travel_time(\n",
    "                s.get_pos(), s2.get_pos(), date=calc_date\n",
    "            )\n",
    "\n",
    "    return (pts_lat, pts_lon, station_max_travel_time, grid_station_travel_time,\n",
    "            grid_station_couple_travel_time, grid_tolerance)\n",
    "\n",
    "\n",
    "def compute_grids_chunked_latitude(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                                  pick_uncertainty=5, sound_speed_uncertainty=2,\n",
    "                                  n_workers=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Version avec chunking adaptatif pour optimiser l'équilibrage de charge\n",
    "    Traite plusieurs lignes de latitude par chunk\n",
    "    \"\"\"\n",
    "    pts_lat = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "    pts_lon = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "\n",
    "    grid_max_res_time = (0.5 * np.sqrt(2) * (pts_lat[1] - pts_lat[0]) * 111_000) / (\n",
    "                sound_model.constant_velocity - sound_speed_uncertainty)\n",
    "    grid_tolerance = grid_max_res_time + pick_uncertainty\n",
    "    print(f\"Grid tolerance of {grid_tolerance:.2f}s\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = mp.cpu_count()\n",
    "\n",
    "    if chunk_size is None:\n",
    "        # Chunk size adaptatif basé sur le nombre de workers\n",
    "        chunk_size =  max(1, grid_size // (n_workers // 2))\n",
    "\n",
    "    print(f\"Processing {len(pts_lat)} latitude lines in chunks of {chunk_size} using {n_workers} workers\")\n",
    "\n",
    "    calc_date = datetime.datetime(year=2020, month=1, day=1)\n",
    "\n",
    "    def compute_latitude_chunk(lat_indices_chunk):\n",
    "        \"\"\"Traite un chunk de lignes de latitude\"\"\"\n",
    "        chunk_results = {}\n",
    "        station_positions = {s: s.get_pos() for s in stations}\n",
    "\n",
    "        for lat_idx in lat_indices_chunk:\n",
    "            lat = pts_lat[lat_idx]\n",
    "            chunk_results[lat_idx] = {}\n",
    "\n",
    "            for s in stations:\n",
    "                station_pos = station_positions[s]\n",
    "                line_times = np.zeros(len(pts_lon))\n",
    "\n",
    "                for lon_idx, lon in enumerate(pts_lon):\n",
    "                    line_times[lon_idx] = sound_model.get_sound_travel_time(\n",
    "                        [lat, lon], station_pos, date=calc_date\n",
    "                    )\n",
    "\n",
    "                chunk_results[lat_idx][s] = line_times\n",
    "\n",
    "        return chunk_results\n",
    "\n",
    "    # Création des chunks\n",
    "    lat_indices = list(range(len(pts_lat)))\n",
    "    chunks = [lat_indices[i:i + chunk_size] for i in range(0, len(lat_indices), chunk_size)]\n",
    "\n",
    "    # Initialisation des structures de données\n",
    "    grid_station_travel_time = {s: [np.zeros((len(pts_lat), len(pts_lon[i]))) for i in(len(pts_lat))] for s in stations}\n",
    "\n",
    "    # Traitement parallèle par chunks\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_futures = [executor.submit(compute_latitude_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "        completed_chunks = 0\n",
    "        for future in chunk_futures:\n",
    "            chunk_results = future.result()\n",
    "\n",
    "            # Assemblage des résultats\n",
    "            for lat_idx, lat_results in chunk_results.items():\n",
    "                for station, line_times in lat_results.items():\n",
    "                    grid_station_travel_time[station][lat_idx, :] = line_times\n",
    "\n",
    "            completed_chunks += 1\n",
    "            completed_lines = completed_chunks * chunk_size\n",
    "            print(f\"Completed ~{min(completed_lines, len(pts_lat))}/{len(pts_lat)} latitude lines \"\n",
    "                  f\"({100*min(completed_lines, len(pts_lat))/len(pts_lat):.1f}%)\")\n",
    "\n",
    "    # Calcul des différences et temps max (identique à la version précédente)\n",
    "    print(\"Computing station couple travel times...\")\n",
    "    grid_station_couple_travel_time = {}\n",
    "    for s in stations:\n",
    "        grid_station_couple_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            grid_station_couple_travel_time[s][s2] = (\n",
    "                grid_station_travel_time[s2] - grid_station_travel_time[s]\n",
    "            )\n",
    "\n",
    "    print(\"Computing station max travel times...\")\n",
    "    station_max_travel_time = {}\n",
    "    for s in stations:\n",
    "        station_max_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            station_max_travel_time[s][s2] = sound_model.get_sound_travel_time(\n",
    "                s.get_pos(), s2.get_pos(), date=calc_date\n",
    "            )\n",
    "\n",
    "    return (pts_lat, pts_lon, station_max_travel_time, grid_station_travel_time,\n",
    "            grid_station_couple_travel_time, grid_tolerance)\n",
    "\n",
    "\n",
    "# Version avec monitoring de performance\n",
    "def compute_grids_monitored(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                           pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None,lons_per_lat=None):\n",
    "    \"\"\"\n",
    "    Version avec monitoring détaillé des performances\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = compute_grids_by_latitude_irregular(\n",
    "        lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "        pick_uncertainty, sound_speed_uncertainty, n_workers, lons_per_lat=lons_per_lat\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_calculations = grid_size * grid_size * len(stations)\n",
    "\n",
    "    print(f\"\\n=== Performance Report ===\")\n",
    "    print(f\"Total execution time: {total_time:.2f}s\")\n",
    "    print(f\"Total calculations: {total_calculations:,}\")\n",
    "    print(f\"Calculations per second: {total_calculations/total_time:,.0f}\")\n",
    "    print(f\"Grid size: {grid_size}x{grid_size}\")\n",
    "    print(f\"Number of stations: {len(stations)}\")\n",
    "    print(f\"Workers used: {n_workers or mp.cpu_count()}\")\n",
    "\n",
    "    return result"
   ],
   "id": "2f5b8c1fd6b779d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import multiprocessing as mp\n",
    "mp.active_children()  # Voir les processus actifs\n",
    "\n",
    "compute_grids_monitored(lat_bounds, lon_bounds, grid_size, SOUND_MODEL, STATIONS,\n",
    "                           pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None)"
   ],
   "id": "247b45c27c231b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "dorsal_db = \"/home/rsafran/Documents/Database_geo/\"\n",
    "dorsal_db_f = os.listdir(dorsal_db)\n",
    "df = [pd.read_csv(dorsal_db + f, comment=\">\",sep='\\s+', names=[\"lat\",\"lon\",\"n\"])for f in dorsal_db_f]\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]], crs=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "for i in range(len(df)):\n",
    "    plt.plot(df[i].lat, df[i].lon, transform=ccrs.PlateCarree())\n"
   ],
   "id": "a78892b46f1e48f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_ridge_data(dorsal_db_path):\n",
    "    \"\"\"\n",
    "    Charge les données des dorsales océaniques\n",
    "    \"\"\"\n",
    "    dorsal_files = [f for f in os.listdir(dorsal_db_path) if f.endswith('.xy')]\n",
    "    print(f\"Loading {len(dorsal_files)} ridge files: {dorsal_files}\")\n",
    "    ridge_data = {}\n",
    "    all_ridge_points = []\n",
    "    for f in dorsal_files:\n",
    "        ridge_name = f.replace('axe-', '').replace('-tout.xy', '')\n",
    "        df = pd.read_csv(os.path.join(dorsal_db_path, f),\n",
    "                        comment=\">\", sep=r'\\s+', names=[\"lon\", \"lat\", \"n\"])\n",
    "        ridge_points = df[['lat', 'lon']].values\n",
    "        ridge_data[ridge_name] = ridge_points\n",
    "        all_ridge_points.append(ridge_points)\n",
    "        print(f\"  {ridge_name}: {len(ridge_points)} points\")\n",
    "    # Combinaison\n",
    "    all_ridge_points = np.vstack(all_ridge_points)\n",
    "    print(f\"Total ridge points: {len(all_ridge_points)}\")\n",
    "    return ridge_data, all_ridge_points\n",
    "\n",
    "def create_regular_grid(lat_bounds, lon_bounds, grid_size):\n",
    "    \"\"\"\n",
    "    Crée une grille régulière de base\n",
    "    \"\"\"\n",
    "    lats = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "    lons = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "    lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
    "    grid_points = np.column_stack([lat_grid.ravel(), lon_grid.ravel()])\n",
    "    return grid_points, lats, lons\n",
    "\n",
    "def filter_points_near_ridges_chunk(grid_chunk, ridge_points, max_distance_deg):\n",
    "    \"\"\"\n",
    "    Filtre un chunk de points de grille selon la distance aux dorsales\n",
    "    \"\"\"\n",
    "    if len(grid_chunk) == 0:\n",
    "        return []\n",
    "    # Calcul des distances (approximation euclidienne rapide)\n",
    "    distances = cdist(grid_chunk, ridge_points, metric='euclidean')\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "    # Points à conserver (distance < seuil)\n",
    "    valid_mask = min_distances <= max_distance_deg\n",
    "    valid_points = grid_chunk[valid_mask]\n",
    "    return valid_points.tolist()\n",
    "\n",
    "def create_ridge_based_grid(lat_bounds, lon_bounds, grid_size, ridge_points,\n",
    "                           max_distance_deg=4.0, n_workers=None):\n",
    "    \"\"\"\n",
    "    Crée une grille irrégulière basée sur la proximité des dorsales océaniques\n",
    "    \"\"\"\n",
    "    print(f\"Creating ridge-based grid:\")\n",
    "    print(f\"  Bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    print(f\"  Grid size: {grid_size}x{grid_size}\")\n",
    "    print(f\"  Max distance from ridges: {max_distance_deg}°\")\n",
    "\n",
    "    grid_points, _, _ = create_regular_grid(lat_bounds, lon_bounds, grid_size)\n",
    "    print(f\"  Initial regular grid: {len(grid_points)} points\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = mp.cpu_count()\n",
    "    chunk_size = max(1000, len(grid_points) // (n_workers * 4))\n",
    "    chunks = [grid_points[i:i + chunk_size] for i in range(0, len(grid_points), chunk_size)]\n",
    "\n",
    "    print(f\"  Processing {len(chunks)} chunks using {n_workers} workers...\")\n",
    "    # Filtrage parallèle\n",
    "    filter_func = partial(filter_points_near_ridges_chunk,\n",
    "                         ridge_points=ridge_points,\n",
    "                         max_distance_deg=max_distance_deg)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_results = list(executor.map(filter_func, chunks))\n",
    "    # Combinaison des résultats\n",
    "    irregular_points = []\n",
    "    for chunk_result in chunk_results:\n",
    "        irregular_points.extend(chunk_result)\n",
    "    irregular_points = np.array(irregular_points)\n",
    "    reduction_factor = len(irregular_points) / len(grid_points) * 100\n",
    "    print(f\"  Final irregular grid: {len(irregular_points)} points ({reduction_factor:.1f}% of original)\")\n",
    "    return irregular_points\n",
    "\n",
    "\n",
    "def visualize_ridge_grid(ridge_data, irregular_points, lat_bounds, lon_bounds,\n",
    "                        max_distance_deg=4.0, figsize=(12, 8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, (ridge_name, ridge_points) in enumerate(ridge_data.items()):\n",
    "        plt.scatter(ridge_points[:, 1], ridge_points[:, 0],\n",
    "                   c=colors[i % len(colors)], s=1, alpha=0.7,\n",
    "                   label=f'{ridge_name} ridge')\n",
    "    # Plot de la grille irrégulière\n",
    "    plt.scatter(irregular_points[:, 1], irregular_points[:, 0],\n",
    "               c='black', s=0.5, alpha=0.3, label='Grid points')\n",
    "    plt.xlim(lon_bounds)\n",
    "    plt.ylim(lat_bounds)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'Ridge-based irregular grid (≤{max_distance_deg}° from ridges)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gcf()\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Exemple complet d'utilisation\n",
    "    \"\"\"\n",
    "    # Paramètres\n",
    "    dorsal_db = \"/home/rsafran/Documents/Database_geo/\"\n",
    "    lat_bounds = [-60, 5]\n",
    "    lon_bounds = [35, 120]\n",
    "    grid_size = 400  # Grille régulière de base\n",
    "    max_distance_deg = 4.0\n",
    "    # Chargement des dorsales\n",
    "    ridge_data, all_ridge_points = load_ridge_data(dorsal_db)\n",
    "\n",
    "    # Génération de la grille irrégulière\n",
    "    irregular_points = create_ridge_based_grid(\n",
    "        lat_bounds, lon_bounds, grid_size, all_ridge_points, max_distance_deg, n_workers=1\n",
    "    )\n",
    "    # # Visualisation (optionnel)\n",
    "    fig = visualize_ridge_grid(ridge_data, irregular_points, lat_bounds, lon_bounds, max_distance_deg)\n",
    "    plt.show()\n",
    "\n",
    "    # mp.set_start_method(\"spawn\", force=True)\n",
    "    # Calcul complet avec votre sound_model et stations\n",
    "    # result = compute_grids_by_latitude(lat_bounds, lon_bounds, grid_size, SOUND_MODEL, STATIONS,\n",
    "    #                        pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=24, irregular_points=irregular_points)\n",
    "\n",
    "    return irregular_points, ridge_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    irregular_points, ridge_data = example_usage()"
   ],
   "id": "b803977972b16496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.save(\"irregular_points.npy\",irregular_points )",
   "id": "b4264cb00d658020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "irregular_points = np.load(\"irregular_points.npy\")",
   "id": "f87161a5e48b5f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"../../../data/T-pick/grid_to_coords_ridges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(irregular_points,f)"
   ],
   "id": "d3f5e32beebace1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def grid_by_latitude(lat_bounds, lon_bounds, grid_size,\n",
    "                              irregular_points=None,\n",
    "                              fill_missing='regular',\n",
    "                              regular_lon_count=None,\n",
    "                              tol=None):\n",
    "\n",
    "    # 1) création des latitudes linéaires demandées (uniques)\n",
    "    lats = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "\n",
    "    # 2) réglage des paramètres\n",
    "    if regular_lon_count is None:\n",
    "        regular_lon_count = grid_size\n",
    "\n",
    "    if grid_size > 1:\n",
    "        lat_spacing = lats[1] - lats[0]\n",
    "    else:\n",
    "        lat_spacing = 0.0\n",
    "    if tol is None:\n",
    "        tol = lat_spacing / 2.0 + 1e-12\n",
    "\n",
    "    # cas sans irregular_points : retourner grille régulière complète\n",
    "    if irregular_points is None or len(irregular_points) == 0:\n",
    "        regular_lons = np.linspace(lon_bounds[0], lon_bounds[1], regular_lon_count)\n",
    "        lons_per_lat = [regular_lons.copy() for _ in range(len(lats))]\n",
    "        lat_col = np.repeat(lats, [len(regular_lons)] * len(lats))\n",
    "        lon_col = np.tile(regular_lons, len(lats))\n",
    "        grid_points_flat = np.column_stack([lat_col, lon_col])\n",
    "        return lats, lons_per_lat, grid_points_flat\n",
    "\n",
    "    # 3) associer chaque point irrégulier à la latitude la plus proche si dans tol\n",
    "    irr_lats = irregular_points[:, 0]\n",
    "    irr_lons = irregular_points[:, 1]\n",
    "\n",
    "    # distances entre chaque point irrégulier et chaque lat cible (abs difference)\n",
    "    # opération vectorisée efficace\n",
    "    abs_diff = np.abs(irr_lats[:, None] - lats[None, :])   # shape (N, grid_size)\n",
    "    idx_nearest = np.argmin(abs_diff, axis=1)              # index de la lat la plus proche\n",
    "    dist_nearest = abs_diff[np.arange(len(irr_lats)), idx_nearest]\n",
    "    assigned_mask = dist_nearest <= tol\n",
    "\n",
    "    # préparer liste vide\n",
    "    lons_per_lat = [np.array([], dtype=float) for _ in range(len(lats))]\n",
    "\n",
    "    # remplir\n",
    "    for i_pt, assigned in enumerate(assigned_mask):\n",
    "        if not assigned:\n",
    "            continue\n",
    "        lat_idx = idx_nearest[i_pt]\n",
    "        lons_per_lat[lat_idx] = np.append(lons_per_lat[lat_idx], irr_lons[i_pt])\n",
    "\n",
    "    # unique & tri\n",
    "    for i in range(len(lons_per_lat)):\n",
    "        if lons_per_lat[i].size > 0:\n",
    "            lons_per_lat[i] = np.unique(lons_per_lat[i])\n",
    "        else:\n",
    "            lons_per_lat[i] = np.array([], dtype=float)\n",
    "\n",
    "    # 4) remplir les lat vides si demandé\n",
    "    if fill_missing == 'regular':\n",
    "        regular_lons = np.linspace(lon_bounds[0], lon_bounds[1], regular_lon_count)\n",
    "        for i in range(len(lons_per_lat)):\n",
    "            if lons_per_lat[i].size == 0:\n",
    "                lons_per_lat[i] = regular_lons.copy()\n",
    "\n",
    "    elif fill_missing == 'nearest':\n",
    "        # trouver indices non vides\n",
    "        non_empty_idxs = [i for i, arr in enumerate(lons_per_lat) if arr.size > 0]\n",
    "        if len(non_empty_idxs) > 0:\n",
    "            # pour chaque vide, copier les longitudes du plus proche non-vide\n",
    "            for i in range(len(lons_per_lat)):\n",
    "                if lons_per_lat[i].size == 0:\n",
    "                    # distance en index (proxi en lat)\n",
    "                    nearest = min(non_empty_idxs, key=lambda j: abs(j - i))\n",
    "                    lons_per_lat[i] = lons_per_lat[nearest].copy()\n",
    "        else:\n",
    "            # aucun point existant : si aucun non-empty, on peut remplir par régulier si souhaité\n",
    "            if fill_missing == 'nearest':\n",
    "                regular_lons = np.linspace(lon_bounds[0], lon_bounds[1], regular_lon_count)\n",
    "                lons_per_lat = [regular_lons.copy() for _ in range(len(lats))]\n",
    "\n",
    "    # else fill_missing == 'none' -> laisser vides\n",
    "\n",
    "    # 5) construire grid_points_flat\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    for i_lat, lon_arr in enumerate(lons_per_lat):\n",
    "        if lon_arr.size == 0:\n",
    "            continue\n",
    "        lat_list.append(np.full(lon_arr.shape, lats[i_lat]))\n",
    "        lon_list.append(lon_arr)\n",
    "    if len(lat_list) == 0:\n",
    "        grid_points_flat = np.zeros((0, 2))\n",
    "    else:\n",
    "        lat_col = np.concatenate(lat_list)\n",
    "        lon_col = np.concatenate(lon_list)\n",
    "        grid_points_flat = np.column_stack([lat_col, lon_col])\n",
    "\n",
    "    return lats, lons_per_lat, grid_points_flat\n",
    "lat_bounds = [-60, 5]\n",
    "lon_bounds = [35, 120]\n",
    "grid_size = 400\n",
    "\n",
    "# irregular_points = create_ridge_based_grid(...)  # déjà obtenu\n",
    "\n",
    "lats, lons_per_lat, grid_flat = grid_by_latitude(\n",
    "    lat_bounds, lon_bounds, grid_size,\n",
    "    irregular_points=irregular_points,\n",
    "    fill_missing='nearest',       # ou 'regular' / 'none'\n",
    "    regular_lon_count=200         # optionnel si 'regular'\n",
    ")\n",
    "\n",
    "print(\"Nombre de latitudes:\", len(lats))\n",
    "print(\"Exemple lat[120]:\", lats[120])\n",
    "print(\"Nombre de longitudes pour cette latitude:\", len(lons_per_lat[120]))\n",
    "print(\"Total points reconstruits:\", grid_flat.shape)\n"
   ],
   "id": "99b02315bd35d848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(lons_per_lat)",
   "id": "7fe883f3a08902e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = compute_grids_monitored(lat_bounds, lon_bounds, grid_size, SOUND_MODEL, STATIONS,\n",
    "                           pick_uncertainty=2, sound_speed_uncertainty=1, n_workers=None,lons_per_lat=lons_per_lat )"
   ],
   "id": "2a1e8784cbef81a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res",
   "id": "f2248eaed7974491",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
