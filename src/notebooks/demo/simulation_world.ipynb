{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from src.notebooks.demo.dev_association import FIRSTS_DETECTIONS\n",
    "from src.utils.simulation.synthetic import RealStationDataGenerator\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from src.utils.data_reading.sound_data.station import StationsCatalog\n",
    "from src.utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from src.utils.detection.association import compute_candidates, association_is_new, update_valid_grid, update_results\n",
    "from src.utils.detection.association import compute_grids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72b718388af3f4e5",
   "metadata": {},
   "source": [
    "catalog_path = \"../../../data/recensement_stations_OHASISBIO_RS.csv\"\n",
    "dataset =\"OHASISBIO-2018\"\n",
    "STATION = StationsCatalog(catalog_path).by_dataset(dataset)\n",
    "SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1a8ebc8cc99987a",
   "metadata": {},
   "source": [
    "gen = RealStationDataGenerator(STATION, SOUND_MODEL)\n",
    "data = gen.generate_events(datetime.datetime(year=2018, month=1, day=1))\n",
    "gen.plot_stations_and_events()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d347a9bc1be4d830",
   "metadata": {},
   "source": [
    "STATION.get_coordinate_list()[:,1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3146d1c9e9a03b58",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "def load_ridge_data(dorsal_db_path):\n",
    "    \"\"\"\n",
    "    Charge les données des dorsales océaniques\n",
    "    \"\"\"\n",
    "    dorsal_files = [f for f in os.listdir(dorsal_db_path) if f.endswith('.xy')]\n",
    "    print(f\"Loading {len(dorsal_files)} ridge files: {dorsal_files}\")\n",
    "\n",
    "    ridge_data = {}\n",
    "    all_ridge_points = []\n",
    "\n",
    "    for f in dorsal_files:\n",
    "        ridge_name = f.replace('axe-', '').replace('.xy', '')\n",
    "        df = pd.read_csv(os.path.join(dorsal_db_path, f),\n",
    "                        comment=\">\", sep=r'\\s+')\n",
    "\n",
    "        # Nettoyage des données\n",
    "        # df = df.dropna()\n",
    "        ridge_points = df[['x', 'y']].values\n",
    "\n",
    "        ridge_data[ridge_name] = ridge_points\n",
    "        all_ridge_points.append(ridge_points)\n",
    "\n",
    "        print(f\"  {ridge_name}: {len(ridge_points)} points\")\n",
    "\n",
    "    # Combinaison de toutes les dorsales\n",
    "    all_ridge_points = np.vstack(all_ridge_points)\n",
    "    print(f\"Total ridge points: {len(all_ridge_points)}\")\n",
    "\n",
    "    return ridge_data, all_ridge_points\n",
    "\n",
    "ridge_data, all_ridge_points = load_ridge_data(\"../../../data/dorsales/\")\n",
    "\n",
    "\n",
    "def generate_events_near_ridges(n_events, ridge_points, std_km=50):\n",
    "    \"\"\"\n",
    "    Génère des événements proches des dorsales océaniques\n",
    "    :param n_events: nombre d'événements à générer\n",
    "    :param ridge_points: array Nx2 des coordonnées des dorsales (lat, lon)\n",
    "    :param std_km: écart-type du bruit autour des dorsales en km\n",
    "    :return: array Nx2 des positions des événements\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for _ in range(n_events):\n",
    "        # choisir un point aléatoire sur les dorsales\n",
    "        ridge_idx = np.random.randint(0, len(ridge_points))\n",
    "        base_point = ridge_points[ridge_idx]\n",
    "\n",
    "        # ajouter du bruit normal autour du point (en degrés approximativement)\n",
    "        # 1 deg ~ 111 km, donc std_deg = std_km / 111\n",
    "        std_deg = std_km / 111.0\n",
    "        evt_lat = base_point[1] + np.random.normal(0, std_deg)\n",
    "        evt_lon = base_point[0] + np.random.normal(0, std_deg)\n",
    "        events.append([evt_lat, evt_lon])\n",
    "\n",
    "    return np.array(events)\n",
    "\n",
    "n_events = 100\n",
    "simulated_events = generate_events_near_ridges(n_events, all_ridge_points, std_km=150)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68e2ece61e457b07",
   "metadata": {},
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lon_max, lat_max = all_ridge_points.max(axis=0)+5\n",
    "lon_min, lat_min = all_ridge_points.min(axis=0)-5\n",
    "# Create a map projection (PlateCarree is a simple projection for global data)\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add coastlines and land features\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "for name in ridge_data.keys():\n",
    "    plt.plot(ridge_data[name][:,0], ridge_data[name][:,1])\n",
    "\n",
    "# Visualisation rapide\n",
    "plt.scatter(simulated_events[:,1], simulated_events[:,0], c='red', s=20, label='Simulated Events')\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend()\n",
    "plt.title(\"Simulation d'événements proches des dorsales\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage with enhanced features\n",
    "\n",
    "# Create generator with ridge-based events and clock drift\n",
    "generator = RealStationDataGenerator(\n",
    "    stations=STATION,\n",
    "    sound_model=SOUND_MODEL,\n",
    "    n_real_events=100,\n",
    "    n_noise_detections=0,\n",
    "    ridge_data_path=\"../../../data/dorsales/\",\n",
    "    ridge_std_km=100,  # Events within 100km of ridges\n",
    "    perfect_events=True,  # Add timing noise\n",
    "    apply_clock_drift=True,  # Apply clock drift errors\n",
    "    reference_time_years=1,  # 2 years reference time\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate events\n",
    "start_time = datetime.datetime(2023, 1, 1)\n",
    "events, ground_truth = generator.generate_events(start_time, duration_hours=96)\n",
    "\n",
    "# Plot results\n",
    "generator.plot_stations_and_events()\n",
    "\n",
    "# Show simulation info\n",
    "print(\"Simulation parameters:\")\n",
    "for key, value in generator.get_simulation_info().items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "f7f073fcd853bc39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def process_synthetic_detections(events_df,\n",
    "                                min_p_tissnet_primary=0.1,\n",
    "                                min_p_tissnet_secondary=0.1,\n",
    "                                merge_delta=timedelta(seconds=5)):\n",
    "    \"\"\"\n",
    "    Process synthetic detections from the RealStationDataGenerator\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    events_df : DataFrame\n",
    "        With columns ['datetime', 'station', 'probability', 'true_event']\n",
    "    min_p_tissnet_primary : float\n",
    "        Minimum probability threshold for primary detections\n",
    "    min_p_tissnet_secondary : float\n",
    "        Minimum probability threshold for secondary detections\n",
    "    merge_delta : timedelta\n",
    "        Time delta for merging close detections\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    detections : dict\n",
    "        station -> np.array([[datetime, probability], ...])\n",
    "    detections_merged : np.ndarray\n",
    "        Array of [datetime, probability, station], sorted by datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # Garder uniquement les événements qui passent le seuil secondaire\n",
    "    filtered_events = events_df[events_df['probability'] > min_p_tissnet_secondary].copy()\n",
    "\n",
    "    detections = {}\n",
    "    detections_merged_list = []\n",
    "\n",
    "    # Groupement par station\n",
    "    for station in filtered_events['station'].unique():\n",
    "        station_events = filtered_events[filtered_events['station'] == station].copy()\n",
    "        station_events = station_events.sort_values('datetime')\n",
    "\n",
    "        d = station_events[['datetime', 'probability']].values\n",
    "\n",
    "        if len(d) == 0:\n",
    "            detections[station] = np.array([]).reshape(0, 2)\n",
    "            continue\n",
    "\n",
    "        # Nettoyage des détections proches ou régulières\n",
    "        new_d = [d[0]]\n",
    "        for i in range(1, len(d)):\n",
    "            dt = (d[i, 0] - d[i - 1, 0]).total_seconds()\n",
    "            if dt > merge_delta.total_seconds():\n",
    "                if i < 3:\n",
    "                    new_d.append(d[i])\n",
    "                else:\n",
    "                    dt1 = (d[i, 0] - d[i - 1, 0]).total_seconds()\n",
    "                    dt2 = (d[i - 1, 0] - d[i - 2, 0]).total_seconds()\n",
    "\n",
    "                    condition1 = abs(dt1 - dt2) > merge_delta.total_seconds()\n",
    "\n",
    "                    if i >= 4:\n",
    "                        dt3 = (d[i - 1, 0] - d[i - 3, 0]).total_seconds()\n",
    "                        dt4 = (d[i - 2, 0] - d[i - 4, 0]).total_seconds()\n",
    "                        condition2 = abs(dt3 - dt4) > merge_delta.total_seconds()\n",
    "                    else:\n",
    "                        condition2 = True\n",
    "\n",
    "                    if condition1 and condition2:\n",
    "                        new_d.append(d[i])\n",
    "\n",
    "        d = np.array(new_d, dtype=object)\n",
    "        detections[station] = d\n",
    "\n",
    "        print(f\"Found {len(d)} detections for station {station}\")\n",
    "\n",
    "        # Ajouter à la liste globale\n",
    "        for det in d:\n",
    "            detections_merged_list.append([det[0], det[1], station])\n",
    "\n",
    "    # Création du tableau final\n",
    "    if len(detections_merged_list) == 0:\n",
    "        detections_merged = np.array([]).reshape(0, 3)\n",
    "    else:\n",
    "        detections_merged = np.array(detections_merged_list, dtype=object)\n",
    "\n",
    "        # Filtrer sur le seuil primaire\n",
    "        detections_merged = detections_merged[detections_merged[:, 1] > min_p_tissnet_primary]\n",
    "\n",
    "        # Trier par datetime\n",
    "        if len(detections_merged) > 0:\n",
    "            detections_merged = detections_merged[np.argsort(detections_merged[:, 0])]\n",
    "\n",
    "    return detections, detections_merged\n",
    "\n",
    "\n",
    "def analyze_synthetic_detections(detections, detections_merged, ground_truth_df=None):\n",
    "    \"\"\"\n",
    "    Analyze the processed synthetic detections\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "\n",
    "    # Basic statistics\n",
    "    total_detections = sum(len(det) for det in detections.values())\n",
    "    analysis['total_detections'] = total_detections\n",
    "    analysis['total_merged_detections'] = len(detections_merged)\n",
    "    analysis['num_stations_with_detections'] = len([s for s in detections if len(detections[s]) > 0])\n",
    "\n",
    "    # Per station statistics\n",
    "    station_stats = {}\n",
    "    for station, dets in detections.items():\n",
    "        station_stats[str(station)] = {\n",
    "            'num_detections': len(dets),\n",
    "            'avg_probability': np.mean(dets[:, 1]) if len(dets) > 0 else 0,\n",
    "            'std_probability': np.std(dets[:, 1]) if len(dets) > 0 else 0\n",
    "        }\n",
    "    analysis['station_stats'] = station_stats\n",
    "\n",
    "    # Time span analysis\n",
    "    if len(detections_merged) > 1:\n",
    "        start_time = detections_merged[0, 0]\n",
    "        end_time = detections_merged[-1, 0]\n",
    "        time_span_seconds = (end_time - start_time).total_seconds()\n",
    "        analysis['time_span_hours'] = time_span_seconds / 3600\n",
    "        analysis['detection_rate_per_hour'] = len(detections_merged) / (time_span_seconds / 3600)\n",
    "\n",
    "    # Ground truth comparison if available\n",
    "    if ground_truth_df is not None:\n",
    "        analysis['num_true_events'] = len(ground_truth_df)\n",
    "        analysis['detection_efficiency'] = total_detections / len(ground_truth_df) if len(ground_truth_df) > 0 else 0\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "\n",
    "def plot_detection_timeline(detections_merged, ground_truth_df=None):\n",
    "    \"\"\"\n",
    "    Plot timeline of detections and optionally ground truth events\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.dates import DateFormatter\n",
    "\n",
    "    if len(detections_merged) == 0:\n",
    "        print(\"No detections to plot\")\n",
    "        return\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "    # Use datetime directly\n",
    "    timestamps = detections_merged[:, 0]\n",
    "    probabilities = detections_merged[:, 1].astype(float)\n",
    "\n",
    "    # Plot detections timeline\n",
    "    ax1.scatter(timestamps, probabilities, alpha=0.6, s=30)\n",
    "    ax1.set_ylabel('Detection Probability')\n",
    "    ax1.set_title('Synthetic Detections Timeline')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot detections per station\n",
    "    station_objects = detections_merged[:, 2]\n",
    "    station_strings = [str(s) for s in station_objects]\n",
    "    unique_station_strings = list(set(station_strings))\n",
    "\n",
    "    # Mapping string -> original station object\n",
    "    station_map = {}\n",
    "    for i, station_str in enumerate(station_strings):\n",
    "        if station_str not in station_map:\n",
    "            station_map[station_str] = station_objects[i]\n",
    "\n",
    "    unique_stations = [station_map[s] for s in unique_station_strings]\n",
    "    station_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_stations)))\n",
    "\n",
    "    for i, station in enumerate(unique_stations):\n",
    "        station_mask = np.array([s == station for s in station_objects])\n",
    "        station_times = np.array(timestamps)[station_mask]\n",
    "        ax2.scatter(station_times, [i] * len(station_times),\n",
    "                   c=[station_colors[i]], alpha=0.7, s=50,\n",
    "                   label=str(station))\n",
    "\n",
    "    ax2.set_ylabel('Station')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_title('Detections by Station')\n",
    "    ax2.set_yticks(range(len(unique_stations)))\n",
    "    ax2.set_yticklabels([str(s) for s in unique_stations])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n",
    "\n",
    "    # Add ground truth if available\n",
    "    if ground_truth_df is not None:\n",
    "        for idx, event in ground_truth_df.iterrows():\n",
    "            ax1.axvline(event['origin_time'], color='red', alpha=0.5, linestyle='--')\n",
    "            ax2.axvline(event['origin_time'], color='red', alpha=0.5, linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Traiter les détections synthétiques\n",
    "    DETECTIONS, DETECTIONS_MERGED = process_synthetic_detections(\n",
    "        events,\n",
    "        min_p_tissnet_primary=0.1,\n",
    "        min_p_tissnet_secondary=0.1,\n",
    "        merge_delta=timedelta(seconds=1)\n",
    "    )\n",
    "    #\n",
    "    # # Analyser les résultats\n",
    "    # analysis = analyze_synthetic_detections(DETECTIONS, DETECTIONS_MERGED, ground_truth)\n",
    "    #\n",
    "    # print(\"\\nDetection Analysis:\")\n",
    "    # for key, value in analysis.items():\n",
    "    #     if key != 'station_stats':\n",
    "    #         print(f\"{key}: {value}\")\n",
    "    #\n",
    "    # # Visualiser la timeline\n",
    "    # plot_detection_timeline(DETECTIONS_MERGED, ground_truth)"
   ],
   "id": "afeecb2c36aa841d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "FIRSTS_DETECTIONS",
   "id": "585782365eca0d9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Association",
   "id": "2207272f66c3c19b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "id": "3a8c5aada59c19ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ASSOCIATION_OUTPUT_DIR = \"../../../data\"\n",
    "DETECTIONS_DIR_NAME = \"synthetic_detections\"\n",
    "MIN_P_TISSNET_PRIMARY = 0.1\n",
    "MIN_P_TISSNET_SECONDARY = 0.1\n",
    "MERGE_DELTA_S = 10 # threshold below which we consider two events should be merged\n",
    "REQ_CLOSEST_STATIONS = 0  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "RUN_ASSOCIATION = True # set to False to load previous associations without processing it again\n",
    "NCPUS = 8\n",
    "SAVE_PATH_ROOT = None"
   ],
   "id": "b859ff4edbdd380c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lon_max, lat_max = all_ridge_points.max(axis=0)+5\n",
    "lon_min, lat_min = all_ridge_points.min(axis=0)-5\n",
    "LAT_BOUNDS = [lat_min, lat_max]\n",
    "LON_BOUNDS = [lon_min, lon_max]\n",
    "GRID_SIZE = 400  # number of points along each axis\n",
    "\n",
    "(PTS_LAT, PTS_LON, STATION_MAX_TRAVEL_TIME, GRID_STATION_TRAVEL_TIME,\n",
    " GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE) = compute_grids(LAT_BOUNDS, LON_BOUNDS, GRID_SIZE, SOUND_MODEL, STATIONS, pick_uncertainty=0, sound_speed_uncertainty=0)"
   ],
   "id": "181adfd7f27ae184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "print(\"starting association\")\n",
    "\n",
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/refined_s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "\n",
    "association_hashlist = set()\n",
    "associations = {}\n",
    "processed = set()\n",
    "def process_detection(arg):\n",
    "    detection, local_association_hashlist, processed = arg\n",
    "    local_association = {}\n",
    "    date1, p1, s1 = detection\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1+ timedelta(seconds=4*GRID_TOLERANCE) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - timedelta(seconds=4*GRID_TOLERANCE) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([STATION_MAX_TRAVEL_TIME[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:date1}\n",
    "    candidates =  compute_candidates(other_stations, current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    if len(other_stations) < 2:\n",
    "        return local_association, local_association_hashlist, date1\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "        for idx in candidates[station]:\n",
    "            date, p = DETECTIONS[station][idx]\n",
    "            if date in processed:\n",
    "                continue\n",
    "            if not association_is_new(current_association, date, local_association_hashlist):\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE, save_path, LON_BOUNDS, LAT_BOUNDS)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date)\n",
    "\n",
    "                if len(current_association) > 2:\n",
    "                    update_results(date1, current_association, valid_points_new, local_association, GRID_STATION_COUPLE_TRAVEL_TIME)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, save_path)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations, save_path)\n",
    "        return\n",
    "    backtrack(0, current_association, None, associations, save_path=save_path)\n",
    "    return local_association, local_association_hashlist, date1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Version séquentielle\n",
    "    if RUN_ASSOCIATION:\n",
    "        try:\n",
    "            DETECTIONS_MERGED = DETECTIONS_MERGED[np.argsort(DETECTIONS_MERGED[:,1])][::-1]\n",
    "            for det in tqdm(DETECTIONS_MERGED, desc=\"Processing detections\"):\n",
    "                local_association, local_association_hashlist, date1 = process_detection((det, association_hashlist, processed))\n",
    "                processed.add(date1)\n",
    "                association_hashlist = association_hashlist.union(local_association_hashlist)\n",
    "                associations = associations | local_association\n",
    "        finally:\n",
    "            # save the associations no matter if the execution stopped properly\n",
    "            print(f\"Sauvegarde des associations dans {OUT_FILE}\")\n",
    "            np.save(OUT_FILE, associations)"
   ],
   "id": "318a174826b3de2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## show results map",
   "id": "1843d75639af84e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob2\n",
    "\n",
    "# OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "# Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# OUT_FILE = f\"{OUT_DIR}/refined_s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 3\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) > MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1])\n",
    "im = plt.imshow(valid[::-1], aspect=1, cmap=\"inferno\", extent=extent, interpolation=None, vmax =200)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "for s in STATIONS:\n",
    "    p = s.get_pos()\n",
    "\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    plt.plot(p[1], p[0], 'wx', alpha=0.75)\n",
    "    plt.annotate(s.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/15, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/100), textcoords=\"data\", color='w', alpha=0.9)"
   ],
   "id": "a356839634908844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE =9\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) < MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "# Create a figure with cartopy's PlateCarree projection\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "# Set the extent of the map (min_lon, max_lon, min_lat, max_lat)\n",
    "ax.set_extent([LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1]], crs=projection)\n",
    "\n",
    "# Add natural features: land, ocean, and coastlines.\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "# ax.add_feature(cfeature.OCEAN, facecolor='lightblue', zorder=2)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "\n",
    "# Plot the georeferenced image.\n",
    "# Set a lower zorder (e.g., 1) so that the map features drawn with higher zorders remain visible.\n",
    "# Adjust alpha to add a bit of transparency if desired.\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1])\n",
    "im = ax.imshow(valid[::-1],\n",
    "               cmap=\"winter\",\n",
    "               extent=extent,\n",
    "               interpolation=\"nearest\",\n",
    "               origin='upper',\n",
    "               transform=projection,\n",
    "               zorder=1,\n",
    "               alpha=1, vmax=1)\n",
    "\n",
    "# Add a colorbar for the image.\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.05)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "# Plot station markers and add annotations using the axes methods.\n",
    "for s in STATIONS:\n",
    "    lat, lon = s.get_pos()\n",
    "    if lat > LAT_BOUNDS[1] or lat < LAT_BOUNDS[0] or lon > LON_BOUNDS[1] or lon < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    # Plot a marker with a higher zorder so it's on top of the image\n",
    "    ax.plot(lon, lat, 'wx', alpha=0.75, markersize=8, transform=projection, zorder=4)\n",
    "    ax.text(lon - (LON_BOUNDS[1] - LON_BOUNDS[0]) / 15,\n",
    "            lat + (LAT_BOUNDS[1] - LAT_BOUNDS[0]) / 100,\n",
    "            s.name,\n",
    "            color='white',\n",
    "            alpha=0.9,\n",
    "            transform=projection,\n",
    "            zorder=4)\n",
    "\n",
    "plt.title(\"Association Data with Land and Sea\")\n",
    "plt.show()\n"
   ],
   "id": "1b3fca103e9ec138",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7606b5493670bdbb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
