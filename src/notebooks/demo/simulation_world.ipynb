{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialisation",
   "id": "3b639ece607cc52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob2\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta, timezone\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from src.utils.simulation.synthetic import RealStationDataGenerator\n",
    "# from src.utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from src.utils.physics.sound_model.ellipsoidal_sound_model import HomogeneousEllipsoidalSoundModel as HomogeneousSoundModel\n",
    "from src.utils.physics.sound_model.ellipsoidal_sound_model import GridEllipsoidalSoundModel as HomogeneousSoundModel\n",
    "\n",
    "from utils.detection.association_geodesic_ridges import compute_candidates, update_valid_grid, update_results, load_detections, compute_grids\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"figure.titlesize\": 12,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42\n",
    "})"
   ],
   "id": "cc0005a55aff1de2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72b718388af3f4e5",
   "metadata": {},
   "source": [
    "catalog_path = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "dataset =\"OHASISBIO-2018\"\n",
    "STATION = StationsCatalog(catalog_path).by_dataset(dataset)\n",
    "# SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/extracted/2018\"\n",
    "arr = os.listdir(ISAS_PATH)\n",
    "file_list = [os.path.join(ISAS_PATH, fname) for fname in arr if fname.endswith('.nc')]\n",
    "SOUND_MODEL = HomogeneousSoundModel(file_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for st in STATION :\n",
    "    print(st.get_pos())"
   ],
   "id": "930280666e20e2c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1a8ebc8cc99987a",
   "metadata": {},
   "source": [
    "gen = RealStationDataGenerator(STATION, SOUND_MODEL)\n",
    "data = gen.generate_events(datetime(year=2018, month=1, day=1))\n",
    "gen.plot_stations_and_events()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "[lat_min, lat_max] = [-60, 5]\n",
    "[lon_min, lon_max] = [35, 120]\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n"
   ],
   "id": "4ebbcf52fe98a867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creation",
   "id": "f32fe81927e89967"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage with enhanced features\n",
    "\n",
    "# Create generator with ridge-based events and clock drift\n",
    "generator = RealStationDataGenerator(\n",
    "    stations=STATION,\n",
    "    sound_model=SOUND_MODEL,\n",
    "    n_real_events=100,\n",
    "    n_noise_detections=450,\n",
    "    ridge_data_path=\"../../../data/dorsales/\",\n",
    "    events_bounds=np.array([[lat_min,0*lat_max-1], [lon_min, lon_max]]),\n",
    "    ridge_std_km=100,  # Events within 100km of ridges\n",
    "    perfect_events=False,  # Add timing noise\n",
    "    apply_clock_drift=True,  # Apply clock drift errors\n",
    "    reference_time_years=1,  # 2 years reference time\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate events\n",
    "start_time = datetime(2023, 1, 1)\n",
    "events, ground_truth = generator.generate_events(start_time, duration_hours=2*24) #100 en 48h c'est assez réaliste\n",
    "\n",
    "# Plot results\n",
    "generator.plot_stations_and_events()\n",
    "\n",
    "# Show simulation info\n",
    "print(\"Simulation parameters:\")\n",
    "for key, value in generator.get_simulation_info().items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "f7f073fcd853bc39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NEAMS = STATION.by_name(\"NEAMS\").stations[0]\n",
    "NEAMS.get_clock_error(datetime(year=2019, month=2, day=5))\n",
    "# NEAMS.date_start\n",
    "n_real_events = generator.get_simulation_info()[\"n_real_events\"]"
   ],
   "id": "f697acbb74039973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "events.station[0].other_kwargs",
   "id": "9112fe41286c26ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pos_diff= []\n",
    "time_diff = []\n",
    "pos_events = []\n",
    "quality = []\n",
    "all_residus = []\n",
    "\n",
    "chi2_stats = []\n",
    "p_values = []\n",
    "passes = []\n",
    "outliers = []\n",
    "\n",
    "def analyze_residuals(result):\n",
    "    \"\"\"Analyse la qualité des résidus pour évaluer la pertinence de l'association\"\"\"\n",
    "    residuals = result.fun\n",
    "    num_params = 2\n",
    "    num_obs = len(residuals)\n",
    "    dof = max(1, num_obs - num_params)\n",
    "\n",
    "    if num_obs == 0:\n",
    "        print(\"error\")\n",
    "\n",
    "    # Chi-carré réduit\n",
    "    chi2_reduced = np.sum(residuals**2) / dof\n",
    "\n",
    "    # Z-score des résidus (résidus standardisés)\n",
    "    residual_std = np.std(residuals) if np.std(residuals) > 1e-10 else 1e-10\n",
    "    z_scores = np.abs(residuals) / residual_std\n",
    "    max_zscore = np.max(z_scores)\n",
    "\n",
    "    # Fraction d'outliers (|z-score| > 2.5)\n",
    "    outlier_fraction = np.mean(z_scores > 2.0)\n",
    "\n",
    "    # Test unitaire de variance\n",
    "    p = 2  # nombre de paramètres ajustés (ici a et b)\n",
    "    sigma_theorique = 3\n",
    "    s2 = np.var(residuals, ddof=p)  # variance sans biais\n",
    "\n",
    "    # --- 5. Test unitaire de variance ---\n",
    "    chi2_stat = (num_obs - p) * s2 / sigma_theorique**2\n",
    "\n",
    "    # Intervalle de confiance au seuil 95%\n",
    "    alpha = 0.05\n",
    "    chi2_inf = chi2.ppf(alpha / 2, df=num_obs - p)\n",
    "    chi2_sup = chi2.ppf(1 - alpha / 2, df=num_obs - p)\n",
    "    if  chi2_inf < chi2_stat < chi2_sup:\n",
    "        H0 = chi2_stat\n",
    "        #print(\"✅ On ne rejette pas H0 : la variance est compatible avec σ² théorique\")\n",
    "    else:\n",
    "        H0 = -chi2_stat\n",
    "        #print(\"❌ On rejette H0 : la variance diffère significativement de σ² théorique\")\n",
    "\n",
    "    # Test de normalité des résidus (Shapiro-Wilk si n < 5000, sinon Kolmogorov-Smirnov)\n",
    "    if num_obs >= 3 and num_obs < 5000:\n",
    "         _, normality_pvalue = stats.shapiro(residuals)\n",
    "\n",
    "    # Score de qualité composite (plus c'est haut, mieux c'est)\n",
    "    quality_score = 1.0 / (1.0 + chi2_reduced)  # Pénalise chi2 élevé\n",
    "    quality_score *= (1.0 - outlier_fraction)    # Pénalise les outliers\n",
    "    quality_score *= 1.0 / (1.0 + max_zscore/10.0)  # Pénalise les résidus extrêmes\n",
    "    quality_score+=H0\n",
    "\n",
    "    return {\n",
    "        'chi2_reduced': float(chi2_reduced),\n",
    "        'max_residual_zscore': float(max_zscore),\n",
    "        'outlier_fraction': float(outlier_fraction),\n",
    "        'normality_pvalue': float(normality_pvalue),\n",
    "        'quality_score': float(quality_score),\n",
    "        'cost':float(res.cost),\n",
    "        'H0': float(H0)\n",
    "    }\n",
    "\n",
    "for i in range(n_real_events):\n",
    "\n",
    "    event_extracted = events[9*i:9*(1+i)]\n",
    "    pos = [e.get_pos() for e in event_extracted.station]\n",
    "    det = [e for e in event_extracted.datetime]\n",
    "    time_elapsed_seconds = 365.25*24*3600*2\n",
    "    drift = [e.other_kwargs[\"clock_drift_ppm\"] * 1e-6 * time_elapsed_seconds  if e.other_kwargs['gps_sync'] != 'ok' else 0 for e in event_extracted.station]\n",
    "    drift = np.abs(drift) / np.sqrt(3)\n",
    "    min_date = np.argmin(det)\n",
    "    c0 = [-8.,68.]\n",
    "    t0 = -1 * SOUND_MODEL.get_sound_travel_time(c0, pos[min_date], det[min_date])\n",
    "    pick_uncertainties = [3]*len(det)\n",
    "\n",
    "    x0 = [t0]+list(c0)\n",
    "    x0 = np.array(x0)\n",
    "    # print(x0)\n",
    "    res= SOUND_MODEL.localize_with_uncertainties(\n",
    "        pos, det, initial_pos=x0, drift_uncertainties = drift, pick_uncertainties = pick_uncertainties, x_min=lat_min-1, x_max=lat_max+1,y_min=lon_min-1,y_max=lon_max+1\n",
    "    )\n",
    "    out = SOUND_MODEL.detect_outliers(res)\n",
    "        # Test chi²\n",
    "    chi2_stat, p_val, pass_test, dof = SOUND_MODEL.test_chi_square(res)\n",
    "    chi2_stats.append(chi2_stat)\n",
    "    p_values.append(p_val)\n",
    "    passes.append(pass_test)\n",
    "    outliers.append(out)\n",
    "\n",
    "\n",
    "\n",
    "    all_residus.append(res)\n",
    "    quality.append(analyze_residuals(res))\n",
    "    # print(res.x[1:], datetime.datetime.utcfromtimestamp(res.x[0]))\n",
    "    err = res.x[1:] - ground_truth.iloc[i][['lat','lon']].values\n",
    "    t_err = (ground_truth.iloc[i].origin_time.tz_localize(\"UTC\")-datetime.fromtimestamp(res.x[0], tz=timezone.utc)).total_seconds()\n",
    "    pos_ev = res.x[1:]\n",
    "    pos_events.append(pos_ev)\n",
    "    pos_diff.append(err)\n",
    "    time_diff.append(t_err)\n",
    "\n",
    "\n",
    "# Analyse\n",
    "print(f\"Taux de réussite: {np.mean(passes)*100:.1f}%\")\n",
    "print(f\"Chi² moyen: {np.mean(chi2_stats):.2f} (attendu: ~{dof})\")\n",
    "print(f\"Chi²/dof moyen: {np.mean(chi2_stats)/dof:.2f} (attendu: ~1)\")\n",
    "quality  = pd.DataFrame(quality)\n",
    "pos_events = np.array(pos_events)\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.subplots_adjust(left=3, right=4.0)\n",
    "plt.hist(np.array(pos_diff)[:,1],alpha=0.5, label='diff lon (°)', bins=50, density=True)\n",
    "plt.hist(np.array(pos_diff)[:,0],alpha=0.5, label='diff lat (°)', bins=50, density=True)\n",
    "plt.legend()\n",
    "plt.subplot(312)\n",
    "plt.subplots_adjust(left=3, right=4.0)\n",
    "plt.hist(np.array(time_diff), label='time diff', bins=50, density=True)\n",
    "plt.legend()\n",
    "plt.subplot(313)\n",
    "plt.subplots_adjust(left=3, right=4.0)\n",
    "plt.hist(quality['chi2_reduced'], label='chi2_reduced', bins=50, density=True)\n",
    "plt.legend()"
   ],
   "id": "47292ca67292d6f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# FONCTION DE DIAGNOSTIC\n",
    "def diagnose_localization_quality(events, ground_truth, n_real_events, pick_uncertainties_sigma=3):\n",
    "    \"\"\"\n",
    "    Diagnostic complet pour identifier le problème de p-value\n",
    "    \"\"\"\n",
    "    chi2_stats = []\n",
    "    p_values = []\n",
    "    passes = []\n",
    "    chi2_reduced = []\n",
    "    position_errors = []\n",
    "    time_errors = []\n",
    "    outliers = []\n",
    "    for i in range(n_real_events):\n",
    "        event_extracted = events[9*i:9*(1+i)]\n",
    "        pos = [e.get_pos() for e in event_extracted.station]\n",
    "        det = [e for e in event_extracted.datetime]\n",
    "        min_date = np.argmin(det)\n",
    "        c0 = [-8., 68.]\n",
    "        t0 = -1 * SOUND_MODEL.get_sound_travel_time(c0, pos[min_date], det[min_date])\n",
    "        pick_uncertainties = [pick_uncertainties_sigma] * len(det)\n",
    "        x0 = [t0] + list(c0)\n",
    "        (lat_max, lat_max, lon_min, lon_max) = (-69.85174, 19.605793, 25.0, 130.0)\n",
    "        res = SOUND_MODEL.localize_with_uncertainties(\n",
    "            pos, det, initial_pos=x0, pick_uncertainties=pick_uncertainties, x_min=lat_min, x_max=lat_max,y_min=lon_min,y_max=lon_max\n",
    "        )\n",
    "        out = SOUND_MODEL.detect_outliers(res)\n",
    "        outliers.append(out)\n",
    "        # Erreurs de position\n",
    "        err = res.x[1:] - ground_truth.iloc[i][['lat','lon']].values\n",
    "        position_errors.append(np.linalg.norm(err))\n",
    "\n",
    "        # Erreur de temps\n",
    "        t_err = (ground_truth.iloc[i].origin_time.tz_localize(\"UTC\")-datetime.fromtimestamp(res.x[0], tz=timezone.utc)).total_seconds()\n",
    "        time_errors.append(t_err)\n",
    "\n",
    "        # Test chi²\n",
    "        chi2_stat, p_val, pass_test, dof = SOUND_MODEL.test_chi_square(res)\n",
    "        chi2_stats.append(chi2_stat)\n",
    "        p_values.append(p_val)\n",
    "        passes.append(pass_test)\n",
    "        chi2_reduced.append(chi2_stat / dof)\n",
    "\n",
    "    # Résultats\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DIAGNOSTIC DE QUALITÉ DE LA LOCALISATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n1. TEST CHI² (α=0.05)\")\n",
    "    print(f\"   Taux de réussite: {np.mean(passes)*100:.1f}% (attendu: 95%)\")\n",
    "    print(f\"   DOF: {dof}\")\n",
    "    print(f\"   Chi² moyen: {np.mean(chi2_stats):.2f} (attendu: ~{dof})\")\n",
    "    print(f\"   Chi²/DOF moyen: {np.mean(chi2_reduced):.3f} (attendu: ~1.0)\")\n",
    "\n",
    "    print(f\"\\n2. INTERPRÉTATION\")\n",
    "    if np.mean(chi2_reduced) > 1.5:\n",
    "        print(f\"   ⚠️  Chi²/DOF > 1.5: Incertitudes SOUS-ESTIMÉES\")\n",
    "        print(f\"   → Les résidus sont trop grands par rapport aux incertitudes\")\n",
    "        print(f\"   → Solutions: augmenter pick_uncertainties ou vérifier le modèle\")\n",
    "    elif np.mean(chi2_reduced) < 0.5:\n",
    "        print(f\"   ⚠️  Chi²/DOF < 0.5: Incertitudes SUR-ESTIMÉES\")\n",
    "        print(f\"   → Les résidus sont trop petits par rapport aux incertitudes\")\n",
    "        print(f\"   → Solutions: diminuer pick_uncertainties\")\n",
    "    else:\n",
    "        print(f\"   ✓ Chi²/DOF proche de 1: Incertitudes bien calibrées\")\n",
    "\n",
    "    print(f\"\\n3. ERREURS DE POSITION\")\n",
    "    print(f\"   Erreur moyenne: {np.mean(position_errors):.3f}°\")\n",
    "    print(f\"   Erreur médiane: {np.median(position_errors):.3f}°\")\n",
    "    print(f\"   Erreur max: {np.max(position_errors):.3f}°\")\n",
    "\n",
    "    print(f\"\\n4. ERREURS DE TEMPS\")\n",
    "    print(f\"   Erreur moyenne: {np.mean(np.abs(time_errors)):.3f} s\")\n",
    "    print(f\"   Erreur médiane: {np.median(np.abs(time_errors)):.3f} s\")\n",
    "\n",
    "    print(f\"\\n5. P-VALUES\")\n",
    "    print(f\"   P-value médiane: {np.median(p_values):.3f}\")\n",
    "    print(f\"   P-value < 0.05: {np.sum(np.array(p_values) < 0.05)} événements\")\n",
    "\n",
    "    return {\n",
    "        'chi2_stats': chi2_stats,\n",
    "        'p_values': p_values,\n",
    "        'passes': passes,\n",
    "        'chi2_reduced': chi2_reduced,\n",
    "        'position_errors': position_errors,\n",
    "        'time_errors': time_errors\n",
    "    }\n",
    "\n",
    "results = diagnose_localization_quality(events, ground_truth, n_real_events, pick_uncertainties_sigma=3)\n",
    "\n",
    "# Pour visualiser la distribution des p-values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(results['p_values'], bins=20, edgecolor='black')\n",
    "plt.axvline(0.05, color='red', linestyle='--', label='α=0.05')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des p-values')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(results['chi2_reduced'], bins=20, edgecolor='black')\n",
    "plt.axvline(1.0, color='red', linestyle='--', label='Attendu')\n",
    "plt.xlabel('Chi²/DOF')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Chi² réduit')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(results['chi2_reduced'], results['p_values'])\n",
    "plt.axhline(0.05, color='red', linestyle='--')\n",
    "plt.axvline(1.0, color='red', linestyle='--')\n",
    "plt.xlabel('Chi²/DOF')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Chi² vs P-value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9293e499fe53faca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n = 48#len(np.array(time_diff)[abs(np.array(time_diff)) > 180])+len(np.array(pos_diff)[:,1][abs(np.array(pos_diff)[:,1]) > 1])+len(np.array(pos_diff)[:,0][abs(np.array(pos_diff)[:,1]) > 1])\n",
    "p = (1-n/500)*100\n",
    "print(n,p)"
   ],
   "id": "c7e8b795deff40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(outliers)['outlier_indices'].apply(lambda x : len(x))",
   "id": "84f22c62313efad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,8 ),dpi=300)\n",
    "pos_diff = np.array(pos_diff)\n",
    "tot_pos_diff =pos_diff[:,1]\n",
    "ax1=  plt.subplot(211)\n",
    "plt.scatter(pos_events[:,1], pos_events[:,0], c=time_diff, alpha=0.85,vmin=40,vmax=-40 ,cmap='seismic')\n",
    "plt.colorbar()\n",
    "ax2= plt.subplot(223)\n",
    "plt.scatter(pos_events[:,1], pos_events[:,0],c=pos_diff[:,1], alpha=0.85, cmap='seismic',vmin=-0.51, vmax=0.51, label='lon diff')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "ax3 = plt.subplot(224,sharex=ax2,sharey=ax2)\n",
    "# make these tick labels invisible\n",
    "plt.tick_params('y', labelleft=False)\n",
    "plt.scatter(pos_events[:,1], pos_events[:,0],c=pos_diff[:,0], alpha=0.85, cmap='seismic',vmin=-0.51, vmax=0.51, label='lat diff')\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ],
   "id": "4def006927f5658f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print((np.quantile(pos_diff[:,1],0.99)))\n",
    "print(np.quantile(pos_diff[:,0],0.99))\n",
    "print(np.quantile(time_diff,0.99))"
   ],
   "id": "9b18faf0f9e653f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(pos_diff[:,0]**2 + pos_diff[:,1]**2)",
   "id": "9fad54f5408c1d1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_synthetic_detections(events_df,\n",
    "                                min_p_tissnet_primary=0.1,\n",
    "                                min_p_tissnet_secondary=0.1,\n",
    "                                merge_delta=timedelta(seconds=5)):\n",
    "    \"\"\"\n",
    "    Process synthetic detections from the RealStationDataGenerator\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    events_df : DataFrame\n",
    "        With columns ['datetime', 'station', 'probability', 'true_event']\n",
    "    min_p_tissnet_primary : float\n",
    "        Minimum probability threshold for primary detections\n",
    "    min_p_tissnet_secondary : float\n",
    "        Minimum probability threshold for secondary detections\n",
    "    merge_delta : timedelta\n",
    "        Time delta for merging close detections\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    detections : dict\n",
    "        station -> np.array([[datetime, probability], ...])\n",
    "    detections_merged : np.ndarray\n",
    "        Array of [datetime, probability, station], sorted by datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # Garder uniquement les événements qui passent le seuil secondaire\n",
    "    filtered_events = events_df[events_df['probability'] > min_p_tissnet_secondary].copy()\n",
    "\n",
    "    detections = {}\n",
    "    detections_merged_list = []\n",
    "\n",
    "    # Groupement par station\n",
    "    for station in filtered_events['station'].unique():\n",
    "        station_events = filtered_events[filtered_events['station'] == station].copy()\n",
    "        station_events = station_events.sort_values('datetime')\n",
    "\n",
    "        d = station_events[['datetime', 'probability']].values\n",
    "\n",
    "        if len(d) == 0:\n",
    "            detections[station] = np.array([]).reshape(0, 2)\n",
    "            continue\n",
    "\n",
    "        # Nettoyage des détections proches ou régulières\n",
    "        new_d = [d[0]]\n",
    "        for i in range(1, len(d)):\n",
    "            dt = (d[i, 0] - d[i - 1, 0]).total_seconds()\n",
    "            if dt > merge_delta.total_seconds():\n",
    "                if i < 3:\n",
    "                    new_d.append(d[i])\n",
    "                else:\n",
    "                    dt1 = (d[i, 0] - d[i - 1, 0]).total_seconds()\n",
    "                    dt2 = (d[i - 1, 0] - d[i - 2, 0]).total_seconds()\n",
    "\n",
    "                    condition1 = abs(dt1 - dt2) > merge_delta.total_seconds()\n",
    "\n",
    "                    if i >= 4:\n",
    "                        dt3 = (d[i - 1, 0] - d[i - 3, 0]).total_seconds()\n",
    "                        dt4 = (d[i - 2, 0] - d[i - 4, 0]).total_seconds()\n",
    "                        condition2 = abs(dt3 - dt4) > merge_delta.total_seconds()\n",
    "                    else:\n",
    "                        condition2 = True\n",
    "\n",
    "                    if condition1 and condition2:\n",
    "                        new_d.append(d[i])\n",
    "\n",
    "        d = np.array(new_d, dtype=object)\n",
    "        detections[station] = d\n",
    "\n",
    "        print(f\"Found {len(d)} detections for station {station}\")\n",
    "\n",
    "        # Ajouter à la liste globale\n",
    "        for det in d:\n",
    "            detections_merged_list.append([det[0], det[1], station])\n",
    "\n",
    "    # Création du tableau final\n",
    "    if len(detections_merged_list) == 0:\n",
    "        detections_merged = np.array([]).reshape(0, 3)\n",
    "    else:\n",
    "        detections_merged = np.array(detections_merged_list, dtype=object)\n",
    "\n",
    "        # Filtrer sur le seuil primaire\n",
    "        detections_merged = detections_merged[detections_merged[:, 1] > min_p_tissnet_primary]\n",
    "\n",
    "        # Trier par datetime\n",
    "        if len(detections_merged) > 0:\n",
    "            detections_merged = detections_merged[np.argsort(detections_merged[:, 0])]\n",
    "\n",
    "    return detections, detections_merged\n",
    "\n",
    "\n",
    "def analyze_synthetic_detections(detections, detections_merged, ground_truth_df=None):\n",
    "    \"\"\"\n",
    "    Analyze the processed synthetic detections\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "\n",
    "    # Basic statistics\n",
    "    total_detections = sum(len(det) for det in detections.values())\n",
    "    analysis['total_detections'] = total_detections\n",
    "    analysis['total_merged_detections'] = len(detections_merged)\n",
    "    analysis['num_stations_with_detections'] = len([s for s in detections if len(detections[s]) > 0])\n",
    "\n",
    "    # Per station statistics\n",
    "    station_stats = {}\n",
    "    for station, dets in detections.items():\n",
    "        station_stats[str(station)] = {\n",
    "            'num_detections': len(dets),\n",
    "            'avg_probability': np.mean(dets[:, 1]) if len(dets) > 0 else 0,\n",
    "            'std_probability': np.std(dets[:, 1]) if len(dets) > 0 else 0\n",
    "        }\n",
    "    analysis['station_stats'] = station_stats\n",
    "\n",
    "    # Time span analysis\n",
    "    if len(detections_merged) > 1:\n",
    "        start_time = detections_merged[0, 0]\n",
    "        end_time = detections_merged[-1, 0]\n",
    "        time_span_seconds = (end_time - start_time).total_seconds()\n",
    "        analysis['time_span_hours'] = time_span_seconds / 3600\n",
    "        analysis['detection_rate_per_hour'] = len(detections_merged) / (time_span_seconds / 3600)\n",
    "\n",
    "    # Ground truth comparison if available\n",
    "    if ground_truth_df is not None:\n",
    "        analysis['num_true_events'] = len(ground_truth_df)\n",
    "        analysis['detection_efficiency'] = total_detections / len(ground_truth_df) if len(ground_truth_df) > 0 else 0\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "\n",
    "def plot_detection_timeline(detections_merged, ground_truth_df=None):\n",
    "    \"\"\"\n",
    "    Plot timeline of detections and optionally ground truth events\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if len(detections_merged) == 0:\n",
    "        print(\"No detections to plot\")\n",
    "        return\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "    # Use datetime directly\n",
    "    timestamps = detections_merged[:, 0]\n",
    "    probabilities = detections_merged[:, 1].astype(float)\n",
    "\n",
    "    # Plot detections timeline\n",
    "    ax1.scatter(timestamps, probabilities, alpha=0.6, s=30)\n",
    "    ax1.set_ylabel('Detection Probability')\n",
    "    ax1.set_title('Synthetic Detections Timeline')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot detections per station\n",
    "    station_objects = detections_merged[:, 2]\n",
    "    station_strings = [str(s) for s in station_objects]\n",
    "    unique_station_strings = list(set(station_strings))\n",
    "\n",
    "    # Mapping string -> original station object\n",
    "    station_map = {}\n",
    "    for i, station_str in enumerate(station_strings):\n",
    "        if station_str not in station_map:\n",
    "            station_map[station_str] = station_objects[i]\n",
    "\n",
    "    unique_stations = [station_map[s] for s in unique_station_strings]\n",
    "    station_colors = plt.cm.tab10(np.linspace(0, 1, len(unique_stations)))\n",
    "\n",
    "    for i, station in enumerate(unique_stations):\n",
    "        station_mask = np.array([s == station for s in station_objects])\n",
    "        station_times = np.array(timestamps)[station_mask]\n",
    "        ax2.scatter(station_times, [i] * len(station_times),\n",
    "                   c=[station_colors[i]], alpha=0.7, s=50,\n",
    "                   label=str(station))\n",
    "\n",
    "    ax2.set_ylabel('Station')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_title('Detections by Station')\n",
    "    ax2.set_yticks(range(len(unique_stations)))\n",
    "    ax2.set_yticklabels([str(s) for s in unique_stations])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n",
    "\n",
    "    # Add ground truth if available\n",
    "    if ground_truth_df is not None:\n",
    "        for idx, event in ground_truth_df.iterrows():\n",
    "            ax1.axvline(event['origin_time'], color='red', alpha=0.5, linestyle='--')\n",
    "            ax2.axvline(event['origin_time'], color='red', alpha=0.5, linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Traiter les détections synthétiques\n",
    "    DETECTIONS, DETECTIONS_MERGED = process_synthetic_detections(\n",
    "        events,\n",
    "        min_p_tissnet_primary=0.0,\n",
    "        min_p_tissnet_secondary=0.0,\n",
    "        merge_delta=timedelta(seconds=0)\n",
    "    )\n",
    "    #\n",
    "    # # Analyser les résultats\n",
    "    # analysis = analyze_synthetic_detections(DETECTIONS, DETECTIONS_MERGED, ground_truth)\n",
    "    #\n",
    "    # print(\"\\nDetection Analysis:\")\n",
    "    # for key, value in analysis.items():\n",
    "    #     if key != 'station_stats':\n",
    "    #         print(f\"{key}: {value}\")\n",
    "    #\n",
    "    # # Visualiser la timeline\n",
    "    # plot_detection_timeline(DETECTIONS_MERGED, ground_truth)"
   ],
   "id": "afeecb2c36aa841d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Association",
   "id": "2207272f66c3c19b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "MIN_P_TISSNET_PRIMARY = 0.1\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}\n",
    "\n",
    "# do not keep detection entries for which the detection list is empty\n",
    "to_del = []\n",
    "for s in DETECTIONS.keys():\n",
    "    if len(DETECTIONS[s]) == 0:\n",
    "        to_del.append(s)\n",
    "for s in to_del:\n",
    "    del DETECTIONS[s]\n",
    "\n",
    "# assign an index to each detection\n",
    "idx_det = 0\n",
    "IDX_TO_DET = {}\n",
    "for idx, s in enumerate(DETECTIONS.keys()):\n",
    "    s.idx = idx  # indexes to store efficiently the associations\n",
    "    DETECTIONS[s] = list(DETECTIONS[s])\n",
    "    for i in range(len(DETECTIONS[s])):\n",
    "        DETECTIONS[s][i] = np.concatenate((DETECTIONS[s][i], [idx_det]))\n",
    "        IDX_TO_DET[idx_det] = DETECTIONS[s][i]\n",
    "        idx_det += 1\n",
    "    DETECTIONS[s] = np.array(DETECTIONS[s])\n",
    "DETECTION_IDXS = np.array(list(range(idx_det)))\n",
    "\n",
    "# only keep the stations that appear in the kept detections\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}\n",
    "\n",
    "# list that will be browsed\n",
    "DETECTIONS_MERGED = np.concatenate([[(det[0], det[1], det[2], s) for det in DETECTIONS[s]] for s in STATIONS if \"IMS\" not in s.dataset])\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[DETECTIONS_MERGED[:, 1] > MIN_P_TISSNET_PRIMARY]\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[np.argsort(DETECTIONS_MERGED[:, 1])][::-1]"
   ],
   "id": "3a8c5aada59c19ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ASSOCIATION_OUTPUT_DIR = \"../../../data\"\n",
    "DETECTIONS_DIR_NAME = \"synthetic_detections\"\n",
    "\n",
    "MIN_P_TISSNET_PRIMARY = 0.1\n",
    "MIN_P_TISSNET_SECONDARY = 0.0\n",
    "MERGE_DELTA_S = 0 # threshold below which we consider two events should be merged\n",
    "REQ_CLOSEST_STATIONS = 4  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "RUN_ASSOCIATION = True # set to False to load previous associations without processing it again\n",
    "NCPUS = 24\n",
    "SAVE_PATH_ROOT = None"
   ],
   "id": "b859ff4edbdd380c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../../../data/T-pick/grid_to_coords_ridges.pkl\", \"rb\") as f:\n",
    "    GRID_TO_COORDS = pickle.load(f)"
   ],
   "id": "8d4ceec2c6ed6159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PICK_UNCERTAINTY = 2\n",
    "SOUND_SPEED_UNCERTAINTY = 6\n",
    "MAX_CLOCK_DRIFT = 0\n",
    "\n",
    "GRID_PATH = f\"{ASSOCIATION_OUTPUT_DIR}/synthetic_detections/grids_{PICK_UNCERTAINTY}_{SOUND_SPEED_UNCERTAINTY}_{MAX_CLOCK_DRIFT}.pkl\"\n",
    "\n",
    "if not Path(GRID_PATH).exists():\n",
    "    GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTIES, travel_times, travel_time_uncertainties = compute_grids(GRID_TO_COORDS, SOUND_MODEL, STATIONS, pick_uncertainty=PICK_UNCERTAINTY, sound_speed_uncertainty=SOUND_SPEED_UNCERTAINTY, max_clock_drift=MAX_CLOCK_DRIFT)\n",
    "    with open(GRID_PATH, \"wb\") as f:\n",
    "        pickle.dump((GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTIES), f)\n",
    "else:\n",
    "    with open(GRID_PATH, \"rb\") as f:\n",
    "        GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTIES = pickle.load(f)"
   ],
   "id": "181adfd7f27ae184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(\"starting association\")\n",
    "\n",
    "MIN_ASSOCIATION_SIZE = 5\n",
    "max_reached_per_det = {det_idx:MIN_ASSOCIATION_SIZE for det_idx in DETECTION_IDXS}\n",
    "\n",
    "already_examined = set()\n",
    "associations = []\n",
    "def process_detection(arg):\n",
    "    detection, already_examined, max_reached_per_det = arg\n",
    "    local_association = []\n",
    "    date1, p1, idx_det1, s1 = detection\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1 + timedelta(days=1) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - timedelta(days=1) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([MAX_TDoA[s1][s2][date1.month - 1] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:(date1, idx_det1)}\n",
    "    candidates = compute_candidates(other_stations, current_association, DETECTIONS, MAX_TDoA, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, MAX_TDoA, MERGE_DELTA_S)\n",
    "        for idx in candidates[station]:\n",
    "            date, p, idx_det = DETECTIONS[station][idx]\n",
    "\n",
    "            if date in already_examined:\n",
    "                # the det was already browsed as main\n",
    "                continue\n",
    "            if len(other_stations) <= max_reached_per_det[idx_det]-1:\n",
    "                # the det already belongs to an association larger that what we could have here\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, TDoA, TDoA_UNCERTAINTIES)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)[:,0]\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date, idx_det)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, save_path)\n",
    "\n",
    "                if np.all([len(current_association) >= max_reached_per_det[idx]-1 for _, idx in current_association.values()]):\n",
    "                    update_results(date1, current_association, valid_points_new, local_association, TDoA, TDoA_UNCERTAINTIES)\n",
    "                    for _, idx in current_association.values():\n",
    "                        max_reached_per_det[idx] = max(max_reached_per_det[idx], len(current_association))\n",
    "\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            max_len_possible = len(current_association) + (len(other_stations) - (station_index+1))\n",
    "            if np.all([max_len_possible >= max_reached_per_det[idx]-1 for _, idx in current_association.values()]):\n",
    "                backtrack(station_index + 1, current_association, valid_grid, associations, save_path)\n",
    "        return\n",
    "\n",
    "    if len(other_stations) > max_reached_per_det[idx_det1]-1:\n",
    "        backtrack(0, current_association, None, associations, save_path=save_path)\n",
    "    return local_association\n",
    "\n",
    "\n",
    "\n",
    "def process_chunk_simple(chunk_data):\n",
    "    chunk_detections, max_reached_per_det = chunk_data\n",
    "    chunk_associations = []\n",
    "    chunk_examined = set()\n",
    "\n",
    "    for det in chunk_detections:\n",
    "        local_association = process_detection((det, chunk_examined, max_reached_per_det))\n",
    "        chunk_examined.add(det[0])\n",
    "        chunk_associations.extend(local_association)\n",
    "\n",
    "    return chunk_associations, chunk_examined\n",
    "\n",
    "# Diviser en chunks\n",
    "n_processes = cpu_count()\n",
    "chunk_size = 50 #len(DETECTIONS_MERGED) // n_processes //20\n",
    "print(chunk_size)\n",
    "chunks = [DETECTIONS_MERGED[i:i + chunk_size]\n",
    "          for i in range(0, len(DETECTIONS_MERGED), chunk_size)]\n",
    "\n",
    "# Préparer les arguments\n",
    "chunk_args = [(chunk, max_reached_per_det) for chunk in chunks]\n",
    "\n",
    "associations = []\n",
    "already_examined = set()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_processes) as executor:\n",
    "    # Soumettre toutes les tâches\n",
    "    future_to_chunk = {executor.submit(process_chunk_simple, args): i\n",
    "                      for i, args in enumerate(chunk_args)}\n",
    "\n",
    "    # Barre de progression avec informations détaillées\n",
    "    completed_chunks = 0\n",
    "    processed_detections = 0\n",
    "\n",
    "    with tqdm(total=len(DETECTIONS_MERGED),\n",
    "              desc=\"Processing detections\",\n",
    "              unit=\"det\") as pbar:\n",
    "\n",
    "        for future in as_completed(future_to_chunk):\n",
    "            chunk_id = future_to_chunk[future]\n",
    "            chunk_size_actual = len(chunks[chunk_id])\n",
    "\n",
    "            try:\n",
    "                chunk_associations, chunk_examined = future.result()\n",
    "                associations.extend(chunk_associations)\n",
    "                already_examined.update(chunk_examined)\n",
    "\n",
    "                # Mettre à jour la progression\n",
    "                completed_chunks += 1\n",
    "                pbar.update(chunk_size_actual)\n",
    "                pbar.set_postfix({\n",
    "                    'chunks': f\"{completed_chunks}/{len(chunks)}\",\n",
    "                    'associations': len(associations)\n",
    "                })\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f'Chunk {chunk_id} generated an exception: {exc}')\n",
    "\n",
    "print(f\"Traitement terminé: {len(associations)} associations trouvées\")\n",
    "\n",
    "with open(f\"{ASSOCIATION_OUTPUT_DIR}/synthetic_detections/cache/associations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(associations, f)"
   ],
   "id": "318a174826b3de2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## show results map",
   "id": "1843d75639af84e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_nb_per_coord = {n: {i:0 for i in range(len(GRID_TO_COORDS))} for n in range(3, 15)}\n",
    "for j in range(len(associations)):\n",
    "    if len(associations[j]) != 2 :\n",
    "        print(len(associations[j]))\n",
    "    detections, valid_points = associations[j]\n",
    "    for i in valid_points:\n",
    "        local_nb_per_coord[len(detections)].setdefault(i, 0)\n",
    "        local_nb_per_coord[len(detections)][i] += 1\n",
    "nb_per_coord = local_nb_per_coord\n",
    "lons = np.array(GRID_TO_COORDS)[:, 1]\n",
    "lats = np.array(GRID_TO_COORDS)[:, 0]\n"
   ],
   "id": "a0aa6815b9d5913a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# to pass a 1D grid into 2D grid with interpolation\n",
    "def squarize(coordinates, weights, lat_bounds, lon_bounds, size=1000):\n",
    "    grid_lat = np.linspace(lat_bounds[0], lat_bounds[-1], size)\n",
    "    grid_lon = np.linspace(lon_bounds[0], lon_bounds[-1], size)\n",
    "    grid_lon2d, grid_lat2d = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "    # Interpolation des poids sur une grille régulière\n",
    "    grid = griddata(\n",
    "        coordinates, weights, (grid_lat2d, grid_lon2d),\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    return grid, grid_lat, grid_lon\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "min_size_display = 3\n",
    "log = True\n",
    "weights = np.array([np.sum([nb_per_coord[n][i] for n in range(min_size_display,15)]) for i in range(len(GRID_TO_COORDS))])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width_in = 6.8\n",
    "height_in = width_in/1.11\n",
    "fig.set_size_inches(width_in, height_in)\n",
    "sq, _, _ = squarize(GRID_TO_COORDS, weights, LAT_BOUNDS, LON_BOUNDS, size=1400)\n",
    "# if log:\n",
    "#     sq[sq<1] = 1\n",
    "#     sq = np.log10(sq)\n",
    "#     sq[sq>3.5] = 3.5\n",
    "#     sq[sq<2] = 2\n",
    "# else:\n",
    "#     sq[sq>2000] = 2000\n",
    "\n",
    "im = ax.imshow(sq[::-1], cmap=\"inferno\",extent=(LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1]), vmax=50)\n",
    "xticks = np.arange(np.floor(LON_BOUNDS[0]/5)*5, np.ceil(LON_BOUNDS[-1]/5)*5 + 5, 5)\n",
    "yticks = np.arange(np.floor(LAT_BOUNDS[0]/5)*5, np.ceil(LAT_BOUNDS[-1]/5)*5 + 5, 5)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "\n",
    "for s_ in STATIONS:\n",
    "    p = s_.get_pos()\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s_.name} out of bounds\")\n",
    "        continue\n",
    "    ax.plot(p[1], p[0], 'yx', alpha=0.75, markersize=10, markeredgewidth=1)\n",
    "    ax.annotate(s_.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/30, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/50), textcoords=\"data\", color='y', alpha=0.9, weight='bold')\n",
    "cbar = plt.colorbar(im,fraction=0.0415, pad=0.04)\n",
    "cbar.set_label(f'Counts of resulting associations{\" (log)\" if log else \"\"}', rotation=270, labelpad=20)\n",
    "ax.set_title(f\"Resulting map\")\n",
    "ax.set_xlabel(\"lon (°)\")\n",
    "ax.set_ylabel(\"lat (°)\")\n",
    "\n"
   ],
   "id": "52e3eb117d1ecdfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.max(weights)",
   "id": "4b18aac397f2ce12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "for s_ in STATIONS:\n",
    "    p = s_.get_pos()\n",
    "    # if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "    #     print(f\"Station {s_.name} out of bounds\")\n",
    "    #     continue\n",
    "    ax.plot(p[1], p[0], 'yx', alpha=0.75, markersize=10, markeredgewidth=2)\n",
    "    ax.annotate(s_.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/30, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/50), textcoords=\"data\", color='y', alpha=0.9, weight='bold')\n",
    "\n",
    "\n",
    "# COAST\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, edgecolor='gray')\n",
    "ax.add_feature(cfeature.OCEAN, color='black')\n",
    "# GRIDS\n",
    "# ax.set_extent([LAT_BOUNDS[1]-1, LAT_BOUNDS[1]+1, LAT_BOUNDS[0]-1, LAT_BOUNDS[0]+1])\n",
    "gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--', x_inline=False, y_inline=False)\n",
    "gl.xlocator = mticker.FixedLocator([20, 60, 100, 140])\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "scat = ax.scatter(np.array(GRID_TO_COORDS)[:,1],np.array(GRID_TO_COORDS)[:,0],c=weights,vmax = 20, s=1, cmap=\"inferno\")\n",
    "\n",
    "cbar = plt.colorbar(scat,fraction=0.0415, pad=0.04)\n",
    "cbar.set_label(f'Counts of resulting associations{\" (log)\" if log else \"\"}', rotation=270, labelpad=20)\n",
    "ax.set_title(f\"Resulting map\")\n",
    "ax.set_xlabel(\"lon (°)\")\n",
    "ax.set_ylabel(\"lat (°)\")\n"
   ],
   "id": "7cb23897bf4a543a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Localisation",
   "id": "cca86d0b3a0855dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# del DETECTIONS_MERGED , DETECTIONS\n",
    "files = glob2.glob(f\"{ASSOCIATION_OUTPUT_DIR}/{DETECTIONS_DIR_NAME}/cache/associations.pkl\")\n",
    "with open(files[0], \"rb\") as f:\n",
    "    association = pickle.load(f)\n",
    "\n",
    "def process(i):\n",
    "    station = list(map(lambda j: STATIONS[j].get_pos(), association[i][0][:,0]))\n",
    "    if len(station) < 3:\n",
    "        return None\n",
    "    time_elapsed_seconds = 365.25*24*3600*1\n",
    "    drift = [e.other_kwargs[\"clock_drift_ppm\"] * 1e-6 * time_elapsed_seconds  if e.other_kwargs['gps_sync'] != 'ok' else 0 for e in event_extracted.station]\n",
    "    drift = np.abs(drift)\n",
    "    det = list(map(lambda j: IDX_TO_DET[j][0], association[i][0][:,1]))\n",
    "    min_date = np.argmin(det)\n",
    "    c0 = list(map(lambda j: GRID_TO_COORDS[j], association[i][1]))\n",
    "    t0 = -1 * SOUND_MODEL.get_sound_travel_time(np.mean(c0, axis =0), station[min_date], det[min_date])\n",
    "    x0 = [t0]+list(np.mean(c0, axis =0))\n",
    "    pick_uncertainties = [3]*len(station)\n",
    "    res = SOUND_MODEL.localize_with_uncertainties(\n",
    "        station, det,y_min=lon_min-6, x_min=lat_min-6,y_max=lon_max+6,x_max=lat_max+6, initial_pos=x0,pick_uncertainties = pick_uncertainties, drift_uncertainties = drift\n",
    "    )\n",
    "    return i, res\n",
    "\n",
    "# Taille du chunk (param important à ajuster selon ton CPU/RAM)\n",
    "CHUNK_SIZE = 25\n",
    "results = {}\n",
    "with mp.Pool(mp.cpu_count()) as pool, open(\"results.npy\", \"wb\") as f:\n",
    "    for r in tqdm(pool.imap(process, range(len(association)), chunksize=CHUNK_SIZE),\n",
    "                  total=len(association)):\n",
    "        if r is not None:\n",
    "            i, res = r\n",
    "            results[i] = res\n",
    "\n",
    "with open(\"results_simu.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ],
   "id": "f8d8c3af673baf15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import tzinfo\n",
    "\n",
    "datetime.fromtimestamp(results[0].x[0]).date()"
   ],
   "id": "9c87e90f01e0d055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Ajouter les fonctions d'évaluation de qualité\n",
    "def calculate_reduced_chi_square(res):\n",
    "    \"\"\"Calcule le chi-carré réduit à partir du résultat de l'optimisation.\"\"\"\n",
    "    n_observations = len(res.fun)\n",
    "    n_parameters = 2\n",
    "    degrees_of_freedom = n_observations - n_parameters\n",
    "    chi_square = np.sum(res.fun**2)\n",
    "    reduced_chi_square = chi_square / degrees_of_freedom if degrees_of_freedom > 0 else float('inf')\n",
    "    return reduced_chi_square, chi_square, degrees_of_freedom\n",
    "\n",
    "def calculate_azimuthal_gap(sensors_positions, event_position):\n",
    "    \"\"\"Calcule le gap azimutal maximal entre les stations.\"\"\"\n",
    "    azimuths = []\n",
    "    for sensor_pos in sensors_positions:\n",
    "\n",
    "        dx = sensor_pos.get_pos()[1] - event_position[1]\n",
    "        dy = sensor_pos.get_pos()[0] - event_position[0]\n",
    "        azimuth = np.degrees(np.arctan2(dx, dy)) % 360\n",
    "        azimuths.append(azimuth)\n",
    "\n",
    "    azimuths_sorted = sorted(azimuths)\n",
    "    gaps = []\n",
    "    for i in range(len(azimuths_sorted)):\n",
    "        next_idx = (i + 1) % len(azimuths_sorted)\n",
    "        gap = (azimuths_sorted[next_idx] - azimuths_sorted[i]) % 360\n",
    "        gaps.append(gap)\n",
    "\n",
    "    return max(gaps) if gaps else 360\n",
    "\n",
    "def estimate_position_uncertainty(res):\n",
    "    \"\"\"Estime l'incertitude sur la position à partir de la matrice de covariance.\"\"\"\n",
    "    try:\n",
    "        J = res.jac\n",
    "        covariance_matrix = np.linalg.inv(J.T @ J) * (2 * res.cost) / (len(res.fun) - 2)\n",
    "        x_uncertainty = np.sqrt(covariance_matrix[0, 0])\n",
    "        y_uncertainty = np.sqrt(covariance_matrix[1, 1])\n",
    "        return (x_uncertainty, y_uncertainty), covariance_matrix\n",
    "    except:\n",
    "        return (float('inf'), float('inf')), None\n",
    "\n",
    "def quality_score(res, sensors_positions, weights=None, max_chi_sq=3.0, max_gap=180.0):\n",
    "    \"\"\"Calcule un score de qualité combinant plusieurs indicateurs.\"\"\"\n",
    "    # Reduced chi-square\n",
    "    red_chi_sq, _, _ = calculate_reduced_chi_square(res)\n",
    "    chi_sq_score = max(0, 1 - (red_chi_sq / max_chi_sq))\n",
    "\n",
    "    # Gap azimutal\n",
    "    event_pos = (res.x[1], res.x[2]) if len(res.x) > 2 else (res.x[0], res.x[1])\n",
    "    gap = calculate_azimuthal_gap(sensors_positions, event_pos)\n",
    "    gap_score = max(0, 1 - (gap / max_gap))\n",
    "\n",
    "    # Résidus standardisés\n",
    "    std_residuals = res.fun\n",
    "    outlier_score = 1 - (np.sum(np.abs(std_residuals) > 3.0) / len(std_residuals))\n",
    "\n",
    "    # Score combiné (pondéré)\n",
    "    combined_score = 0.5 * chi_sq_score + 0.3 * gap_score + 0.2 * outlier_score\n",
    "    return combined_score, {\n",
    "        'red_chi_sq': red_chi_sq,\n",
    "        'azimuthal_gap': gap,\n",
    "        'chi_sq_score': chi_sq_score,\n",
    "        'gap_score': gap_score,\n",
    "        'outlier_score': outlier_score\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_cost_threshold(results_dict, cost_threshold=10000):\n",
    "    \"\"\"\n",
    "    Filtre les associations par seuil de coût.\n",
    "\n",
    "    Args:\n",
    "        results_dict: Dictionnaire des résultats d'optimisation\n",
    "        cost_threshold: Seuil de coût maximum\n",
    "\n",
    "    Returns:\n",
    "        dict: results_dict filtré par coût\n",
    "    \"\"\"\n",
    "    print(f\"Step 1: Filtering by cost threshold ({cost_threshold})...\")\n",
    "\n",
    "    filtered_results = {}\n",
    "    for i, res in results_dict.items():\n",
    "        cost = res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)\n",
    "        if cost < cost_threshold:\n",
    "            filtered_results[i] = res\n",
    "\n",
    "    print(f\"Results after cost filtering: {len(filtered_results)}\")\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "def remove_subset_associations(results_dict, association, keep_best_duplicate=True):\n",
    "    \"\"\"\n",
    "    Supprime les associations qui utilisent un sous-ensemble de détections\n",
    "    par rapport à d'autres associations.\n",
    "\n",
    "    Args:\n",
    "        results_dict: Dictionnaire des résultats d'optimisation\n",
    "        association: Structure contenant les associations de détections\n",
    "        keep_best_duplicate: Si True, garde seulement la meilleure association\n",
    "                            pour les ensembles de détections identiques\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict filtré, set des indices exclus)\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Removing subset associations...\")\n",
    "\n",
    "    # Préparer les données avec les sets de détections\n",
    "    events_data = []\n",
    "    for i in results_dict.keys():\n",
    "        detections = association[i][0][:, 1]  # Index des détections\n",
    "        det_set = frozenset(detections)\n",
    "        cost = results_dict[i].cost if hasattr(results_dict[i], 'cost') else np.sum(results_dict[i].fun**2)\n",
    "        date = datetime.fromtimestamp(results_dict[i].x[0]).date()\n",
    "        events_data.append({\n",
    "            'uid': i,\n",
    "            \"date\": date,\n",
    "            'detection_set': det_set,\n",
    "            'cost': cost,\n",
    "            'set_size': len(det_set)\n",
    "        })\n",
    "\n",
    "    # Trier par taille décroissante, puis par coût croissant\n",
    "    print(\"Sorting by detection set size (descending) and cost (ascending)...\")\n",
    "    events_data.sort(key=lambda x: (x['date'], -x['set_size'], x['cost']))\n",
    "\n",
    "    # Algorithme de suppression des subsets\n",
    "    accepted_events = defaultdict(list)\n",
    "    excluded_indices = set()\n",
    "    seen_detection_sets = set()\n",
    "\n",
    "    for current in tqdm(events_data, desc=\"Filtering subsets\"):\n",
    "        current_set = current['detection_set']\n",
    "        date = current['date']\n",
    "        is_subset = False\n",
    "\n",
    "        # Vérifier si on a déjà vu exactement le même ensemble\n",
    "        if keep_best_duplicate and current_set in seen_detection_sets:\n",
    "            excluded_indices.add(current['uid'])\n",
    "            is_subset = True\n",
    "        else:\n",
    "            # Vérifier si current est un sous-ensemble d'une association acceptée\n",
    "            for accepted in accepted_events[date-timedelta(days=-1)] + accepted_events[date]+ accepted_events[date+timedelta(days=1)]:\n",
    "                if current_set.issubset(accepted['detection_set']):\n",
    "                    # Si c'est un sous-ensemble strict OU un doublon qu'on veut éliminer\n",
    "                    if current_set != accepted['detection_set'] or keep_best_duplicate:\n",
    "                        excluded_indices.add(current['uid'])\n",
    "                        is_subset = True\n",
    "                        break\n",
    "\n",
    "        if not is_subset:\n",
    "            accepted_events[date].append(current)\n",
    "            seen_detection_sets.add(current_set)\n",
    "\n",
    "    print(f\"Events after subset removal: {len(accepted_events)}\")\n",
    "    print(f\"Excluded subset events: {len(excluded_indices)}\")\n",
    "    accepted_events = [x for xs in accepted_events.values() for x in xs]\n",
    "    # Créer le dictionnaire filtré\n",
    "    filtered_results = {event['uid']: results_dict[event['uid']] for event in accepted_events}\n",
    "\n",
    "    return filtered_results, excluded_indices\n",
    "\n",
    "\n",
    "# Version alternative avec optimisation spatiale pour grands datasets\n",
    "def remove_subset_associations_optimized(results_dict, association, keep_best_duplicate=True):\n",
    "    \"\"\"\n",
    "    Version optimisée pour les grands datasets utilisant un arbre de prefix (trie)\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Removing subset associations (optimized version)...\")\n",
    "\n",
    "    # Préparer les données\n",
    "    events_data = []\n",
    "    for i in results_dict.keys():\n",
    "        try:\n",
    "            arr = np.asarray(association[i][0])\n",
    "            detections = arr[:, 1] if arr.ndim == 2 and arr.size > 0 else np.array([], dtype=int)\n",
    "        except (IndexError, ValueError):\n",
    "            detections = np.array([], dtype=int)\n",
    "\n",
    "        det_set = frozenset(detections)\n",
    "        cost = results_dict[i].cost if hasattr(results_dict[i], 'cost') else np.sum(results_dict[i].fun**2)\n",
    "\n",
    "        events_data.append({\n",
    "            'uid': i,\n",
    "            'detection_set': det_set,\n",
    "            'set_size': len(det_set),\n",
    "            'cost': cost\n",
    "        })\n",
    "\n",
    "    # Trier par taille décroissante et coût croissant\n",
    "    events_data.sort(key=lambda x: (-x['set_size'], x['cost']))\n",
    "\n",
    "    # Utiliser un arbre de préfixe pour les ensembles\n",
    "    class DetectionSetTrie:\n",
    "        def __init__(self):\n",
    "            self.children = {}\n",
    "            self.is_end = False\n",
    "            self.event = None\n",
    "\n",
    "        def insert(self, detection_set, event):\n",
    "            \"\"\"Insère un ensemble trié\"\"\"\n",
    "            sorted_dets = sorted(detection_set)\n",
    "            node = self\n",
    "            for det in sorted_dets:\n",
    "                if det not in node.children:\n",
    "                    node.children[det] = DetectionSetTrie()\n",
    "                node = node.children[det]\n",
    "            node.is_end = True\n",
    "            node.event = event\n",
    "\n",
    "        def is_subset_of_existing(self, detection_set):\n",
    "            \"\"\"Vérifie si l'ensemble est sous-ensemble d'un ensemble existant\"\"\"\n",
    "            sorted_dets = sorted(detection_set)\n",
    "            return self._is_subset(sorted_dets, 0)\n",
    "\n",
    "        def _is_subset(self, sorted_dets, index):\n",
    "            if index >= len(sorted_dets):\n",
    "                return True\n",
    "\n",
    "            current_det = sorted_dets[index]\n",
    "            for det, child in self.children.items():\n",
    "                if det <= current_det:\n",
    "                    if det == current_det:\n",
    "                        if child._is_subset(sorted_dets, index + 1):\n",
    "                            return True\n",
    "                    else:\n",
    "                        if child._is_subset(sorted_dets, index):\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "    trie = DetectionSetTrie()\n",
    "    accepted_events = []\n",
    "    excluded_indices = set()\n",
    "\n",
    "    print(\"Processing events with prefix tree...\")\n",
    "    for event in tqdm(events_data, desc=\"Checking subsets\"):\n",
    "        current_set = event['detection_set']\n",
    "\n",
    "        if not current_set:  # Ensemble vide\n",
    "            if keep_best_duplicate:\n",
    "                excluded_indices.add(event['uid'])\n",
    "                continue\n",
    "            else:\n",
    "                accepted_events.append(event)\n",
    "                trie.insert(current_set, event)\n",
    "            continue\n",
    "\n",
    "        # Vérifier si c'est un sous-ensemble\n",
    "        if trie.is_subset_of_existing(current_set):\n",
    "            excluded_indices.add(event['uid'])\n",
    "        else:\n",
    "            # Vérifier les doublons exacts\n",
    "            is_duplicate = False\n",
    "            if keep_best_duplicate:\n",
    "                for accepted in accepted_events:\n",
    "                    if accepted['detection_set'] == current_set:\n",
    "                        # Garder celui avec le meilleur coût\n",
    "                        if event['cost'] < accepted['cost']:\n",
    "                            accepted_events.remove(accepted)\n",
    "                            excluded_indices.add(accepted['uid'])\n",
    "                            accepted_events.append(event)\n",
    "                            trie.insert(current_set, event)  # Mettre à jour le trie\n",
    "                        else:\n",
    "                            excluded_indices.add(event['uid'])\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                accepted_events.append(event)\n",
    "                trie.insert(current_set, event)\n",
    "\n",
    "    print(f\"Events after subset removal: {len(accepted_events)}\")\n",
    "    print(f\"Excluded subset events: {len(excluded_indices)}\")\n",
    "\n",
    "    filtered_results = {event['uid']: results_dict[event['uid']] for event in accepted_events}\n",
    "    return filtered_results, excluded_indices\n",
    "\n",
    "#\n",
    "# def remove_overlap(results_dict, association):\n",
    "#     print(\"Step 3: Handling overlapping associations with residual analysis...\")\n",
    "#\n",
    "#     def analyze_residuals(result):\n",
    "#         \"\"\"Analyse la qualité des résidus pour évaluer la pertinence de l'association\"\"\"\n",
    "#         residuals = result.fun\n",
    "#         num_obs = len(residuals)\n",
    "#         chi2_stat, p_value, passes, dof = SOUND_MODEL.test_chi_square(result)\n",
    "#         if num_obs == 0:\n",
    "#             print(\"error\")\n",
    "#         # Chi-carré réduit\n",
    "#         chi2_reduced = np.sum(residuals**2) / dof\n",
    "#         # Z-scroe des résidus (résidus standardisés)\n",
    "#         residual_std = np.std(residuals) if np.std(residuals) > 1e-10 else 1e-10\n",
    "#         z_scores = np.abs(residuals) / residual_std\n",
    "#         max_zscore = np.max(z_scores)\n",
    "#         outliers = SOUND_MODEL.detect_outliers(result,method='absolute')\n",
    "#         outliers_score = len(outliers['outlier_indices'])\n",
    "#         # Fraction d'outliers (|z-score| > 2.5)\n",
    "#         outlier_fraction = np.mean(z_scores >1.5)\n",
    "#\n",
    "#         # Test unitaire de variance\n",
    "#         p = 2  # nombre de paramètres ajustés (ici a et b)\n",
    "#         sigma_theorique = 3\n",
    "#         s2 = np.var(residuals, ddof=p)  # variance sans biais\n",
    "#         # print(f\"Variance empirique : {s2:.3f}\")\n",
    "#         # Test de normalité des résidus (Shapiro-Wilk si n < 5000, sinon Kolmogorov-Smirnov)\n",
    "#         if num_obs >= 3 and num_obs < 5000:\n",
    "#              _, normality_pvalue = stats.shapiro(residuals)\n",
    "#         # Score de qualité composite (plus c'est haut, mieux c'est)\n",
    "#         quality_score = 1.0 / (1.0 + chi2_reduced)  # Pénalise chi2 élevé\n",
    "#         quality_score *= (1.0 - outlier_fraction)    # Pénalise les outliers\n",
    "#         quality_score *= 1.0 / (1.0 + max_zscore/10.0)  # Pénalise les résidus extrêmes\n",
    "#         return {\n",
    "#             'chi2_reduced': float(chi2_reduced),\n",
    "#             'max_residual_zscore': float(max_zscore),\n",
    "#             'outlier_fraction': float(outlier_fraction),\n",
    "#             'normality_pvalue': float(normality_pvalue),\n",
    "#             'quality_score': float(quality_score),\n",
    "#             'cost':float(result.cost),\n",
    "#             'chi2_stat': float(chi2_stat),\n",
    "#             'p_value': float(p_value),\n",
    "#             'passes': bool(passes),\n",
    "#             \"outliers_score\" : outliers_score\n",
    "#         }\n",
    "#     # Analyse des événements avec résidus\n",
    "#     events = {}\n",
    "#     print(\"Analyzing residuals for each association...\")\n",
    "#\n",
    "#     for i in tqdm(results_dict.keys(), desc=\"Processing associations\"):\n",
    "#         try:\n",
    "#             arr = np.asarray(association[i][0])\n",
    "#             if arr.size == 0:\n",
    "#                 detections = np.array([], dtype=int)\n",
    "#             elif arr.ndim == 1:\n",
    "#                 detections = np.array([int(arr[1])]) if arr.size >= 2 else np.array([], dtype=int)\n",
    "#             else:\n",
    "#                 detections = arr[:, 1].astype(int)\n",
    "#         except Exception:\n",
    "#             detections = np.array([], dtype=int)\n",
    "#             arr = np.array([])\n",
    "#\n",
    "#         det_set = frozenset(detections.tolist())\n",
    "#         num_stations = int(arr.shape[0]) if hasattr(arr, 'shape') else 0\n",
    "#         node = results_dict[i]\n",
    "#\n",
    "#         # Calcul du coût traditionnel\n",
    "#         cost = getattr(node, 'cost', None)\n",
    "#         if cost is None:\n",
    "#             fun = getattr(node, 'fun', None)\n",
    "#             cost = float(np.sum(fun**2)) if fun is not None else float('inf')\n",
    "#\n",
    "#         # Analyse des résidus\n",
    "#         residual_analysis = analyze_residuals(node)\n",
    "#\n",
    "#         events[i] = {\n",
    "#             'detection_set': det_set,\n",
    "#             'cost': float(cost),\n",
    "#             'num_stations': int(num_stations),\n",
    "#             'residual_analysis': residual_analysis\n",
    "#         }\n",
    "#\n",
    "#     # Filtrage préliminaire basé sur la qualité des résidus\n",
    "#     quality_threshold = np.percentile([ev['residual_analysis']['quality_score']\n",
    "#                                      for ev in events.values()], 10)  # Garde les 90% meilleurs\n",
    "#\n",
    "#     print(f\"Quality threshold: {quality_threshold:.4f}\")\n",
    "#\n",
    "#\n",
    "#     det_to_events = defaultdict(set)\n",
    "#     for uid, ev in events.items():\n",
    "#         for d in ev['detection_set']:\n",
    "#             det_to_events[d].add(uid)\n",
    "#\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(events.keys())\n",
    "#     for uids in det_to_events.values():\n",
    "#         if len(uids) > 1:\n",
    "#             for a, b in combinations(sorted(uids, key=lambda x: str(x)), 2):\n",
    "#                 G.add_edge(a, b)\n",
    "#\n",
    "#     print(f\"Found {G.number_of_edges()} overlapping association pairs\")\n",
    "#\n",
    "#     clusters = list(nx.connected_components(G))\n",
    "#     overlapping_clusters = [c for c in clusters if len(c) > 1]\n",
    "#     isolated_events = [list(c)[0] for c in clusters if len(c) == 1]\n",
    "#\n",
    "#     def enhanced_association_score(uid):\n",
    "#         \"\"\"Score amélioré intégrant l'analyse des résidus\"\"\"\n",
    "#         ev = events[uid]\n",
    "#\n",
    "#         # Score traditionnel\n",
    "#         traditional_score = ev['cost'] / (ev['num_stations'] + 1e-9)\n",
    "#\n",
    "#         # Poids basé sur la qualité des résidus\n",
    "#         quality_weight = ev['residual_analysis']['quality_score']\n",
    "#\n",
    "#         # Score composite (plus petit = meilleur)\n",
    "#         composite_score = -quality_weight\n",
    "#         outliers_score = ev['residual_analysis']['outliers_score']\n",
    "#         return (outliers_score,\n",
    "#             composite_score*quality_weight, # Chi2 réduit\n",
    "#             # ev['residual_analysis']['outlier_fraction'],\n",
    "#             # ev['residual_analysis']['chi2_reduced'],\n",
    "#             # traditional_score,# Qualité (négatif pour ordre croissant\n",
    "#         )\n",
    "#\n",
    "#     final_indices = []\n",
    "#\n",
    "#     # Traitement des clusters avec overlap\n",
    "#     for cluster in tqdm(overlapping_clusters, desc=\"Processing overlapping clusters\"):\n",
    "#         cluster = set(cluster)\n",
    "#         events_dets = {uid: events[uid]['detection_set'] for uid in cluster}\n",
    "#\n",
    "#         # Sous-clustering basé sur la similarité\n",
    "#         H = nx.Graph()\n",
    "#         H.add_nodes_from(cluster)\n",
    "#         for a, b in combinations(sorted(cluster, key=lambda x: str(x)), 2):\n",
    "#             A = events_dets[a]\n",
    "#             B = events_dets[b]\n",
    "#             inter = len(A & B)\n",
    "#             union = len(A | B)\n",
    "#             jaccard = inter / union if union else 0.0\n",
    "#\n",
    "#             # Critères pour lier deux événements\n",
    "#             if inter >= 2 or jaccard >= 0.1:\n",
    "#                 H.add_edge(a, b)\n",
    "#\n",
    "#         subs = list(nx.connected_components(H))\n",
    "#\n",
    "#         for sub in subs:\n",
    "#             sub = set(sub)\n",
    "#             ordered = sorted(list(sub), key=enhanced_association_score)\n",
    "#\n",
    "#             # Sélection gloutonne dans le sous-cluster\n",
    "#             picked = []\n",
    "#             picked_dets = set()\n",
    "#\n",
    "#             for uid in ordered:\n",
    "#                 dets = events[uid]['detection_set']\n",
    "#                 quality_score = events[uid]['residual_analysis']['quality_score']\n",
    "#                 passes = events[uid]['residual_analysis']['passes']\n",
    "#                 # Vérification des critères de sélection\n",
    "#                 has_overlap = bool(dets & picked_dets)\n",
    "#                 is_high_quality = passes#quality_score >= quality_threshold * 0.5  # Seuil plus souple\n",
    "#\n",
    "#                 if not has_overlap and is_high_quality:\n",
    "#                     picked.append(uid)\n",
    "#                     picked_dets |= set(dets)\n",
    "#\n",
    "#             if not picked:\n",
    "#                 # Si aucun n'est sélectionné, prend le meilleur du sous-cluster\n",
    "#                 best = min(sub, key=enhanced_association_score)\n",
    "#                 final_indices.append(best)\n",
    "#             else:\n",
    "#                 final_indices.extend(picked)\n",
    "#\n",
    "#     final_indices.extend(isolated_events)\n",
    "#     final_indices = list(dict.fromkeys(final_indices))\n",
    "#\n",
    "#     # Sélection finale avec analyse des résidus\n",
    "#     candidates = list(dict.fromkeys(final_indices))\n",
    "#     candidates.sort(key=enhanced_association_score)\n",
    "#\n",
    "#     selected = []\n",
    "#     selected_dets = set()\n",
    "#\n",
    "#     print(\"Final selection with residual quality check...\")\n",
    "#     for uid in tqdm(candidates, desc=\"Final selection\"):\n",
    "#         dets = events[uid]['detection_set']\n",
    "#         quality_score = events[uid]['residual_analysis']['quality_score']\n",
    "#\n",
    "#         # Critères de sélection finale\n",
    "#         no_overlap = selected_dets.isdisjoint(dets)\n",
    "#         good_quality = quality_score >= quality_threshold * 0.3  # Seuil très souple pour la sélection finale\n",
    "#         reasonable_residuals = events[uid]['residual_analysis']['max_residual_zscore'] < 5.0\n",
    "#\n",
    "#         if no_overlap and good_quality and reasonable_residuals:\n",
    "#             selected.append(uid)\n",
    "#             selected_dets |= set(dets)\n",
    "#\n",
    "#     # Validation finale\n",
    "#     total_dets = sum(len(events[uid]['detection_set']) for uid in selected)\n",
    "#     if total_dets != len(selected_dets):\n",
    "#         raise RuntimeError(\"Overlap detected after final selection\")\n",
    "#\n",
    "#     # Statistiques sur la qualité\n",
    "#     if selected:\n",
    "#         quality_scores = [events[uid]['residual_analysis']['quality_score'] for uid in selected]\n",
    "#         chi2_values = [events[uid]['residual_analysis']['chi2_reduced'] for uid in selected]\n",
    "#\n",
    "#         print(f\"Selected {len(selected)} associations\")\n",
    "#         print(f\"Quality scores: mean={np.mean(quality_scores):.4f}, std={np.std(quality_scores):.4f}\")\n",
    "#         print(f\"Chi2 reduced: mean={np.mean(chi2_values):.4f}, std={np.std(chi2_values):.4f}\")\n",
    "#     else:\n",
    "#         print(\"No associations selected!\")\n",
    "#\n",
    "#     filtered_results = {i: results_dict[i] for i in selected if i in results_dict}\n",
    "#     return filtered_results\n",
    "\n",
    "def remove_overlap(results_dict, association):\n",
    "    print(\"Step 3: Advanced filtering of localization results...\")\n",
    "\n",
    "    def compute_quality_metrics(result):\n",
    "        \"\"\"Calcule des métriques de qualité robustes pour évaluer la localisation\"\"\"\n",
    "        residuals = result.fun\n",
    "        num_obs = len(residuals)\n",
    "\n",
    "        if num_obs < 3:  # Trop peu d'observations\n",
    "            return {\n",
    "                'quality_score': -np.inf,\n",
    "                'chi2_reduced': np.inf,\n",
    "                'outlier_fraction': 1.0,\n",
    "                'max_residual_zscore': np.inf,\n",
    "                'passes_chi2': False,\n",
    "                'num_obs': num_obs,\n",
    "                'cost': np.inf\n",
    "            }\n",
    "\n",
    "        # Test chi-carré\n",
    "        chi2_stat, p_value, passes_chi2, dof = SOUND_MODEL.test_chi_square(result)\n",
    "\n",
    "        # Métriques principales de qualité\n",
    "        chi2_reduced = np.sum(residuals**2) / dof if dof > 0 else np.inf\n",
    "\n",
    "        # Détection robuste des outliers avec MAD (Median Absolute Deviation)\n",
    "        median_residual = np.median(residuals)\n",
    "        mad = np.median(np.abs(residuals - median_residual))\n",
    "        robust_std = mad * 1.4826 if mad > 0 else 1e-10  # Conversion MAD → std normal\n",
    "\n",
    "        # Z-scores robustes\n",
    "        z_scores = np.abs(residuals - median_residual) / robust_std\n",
    "        max_zscore = np.max(z_scores)\n",
    "\n",
    "        # Fraction d'outliers avec seuil robuste\n",
    "        outlier_fraction = np.mean(z_scores > 2.5)  # Seuil plus conservateur\n",
    "\n",
    "        # Score de qualité composite (pondéré)\n",
    "        weights = {\n",
    "            'p_value': 0.35,\n",
    "            'chi2': 0.25,\n",
    "            'outliers': 0.25,\n",
    "            'max_residual': 0.1,\n",
    "            'num_obs': 0.15\n",
    "        }\n",
    "\n",
    "        # Composantes normalisées du score\n",
    "        chi2_component = 1.0 / (1.0 + min(chi2_reduced, 10.0))  # Pénalise chi2 élevé\n",
    "        outlier_component = 1.0 - outlier_fraction\n",
    "        residual_component = 1.0 / (1.0 + max_zscore / 6.0)  # Pénalise résidus extrêmes\n",
    "        obs_component = min(num_obs / 8.0, 1.0)  # Favorise plus d'observations\n",
    "        p_value_component = min(p_value*2, 1.0) # Normaliser p∈[0,1]\n",
    "\n",
    "        quality_score = (\n",
    "            weights['p_value']*p_value_component+\n",
    "            weights['chi2'] * chi2_component +\n",
    "            weights['outliers'] * outlier_component +\n",
    "            # weights['max_residual'] * residual_component +\n",
    "            weights['num_obs'] * obs_component\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'quality_score': float(quality_score),\n",
    "            'chi2_reduced': float(chi2_reduced),\n",
    "            'max_residual_zscore': float(max_zscore),\n",
    "            'outlier_fraction': float(outlier_fraction),\n",
    "            'passes_chi2': bool(passes_chi2),\n",
    "            'num_obs': int(num_obs),\n",
    "            'cost': float(np.sum(residuals**2)),\n",
    "            'p_value': float(p_value)\n",
    "        }\n",
    "\n",
    "    # Construction du graphe d'événements\n",
    "    print(\"Building event graph and computing quality metrics...\")\n",
    "    events = {}\n",
    "    det_to_events = defaultdict(set)\n",
    "\n",
    "    for i in tqdm(results_dict.keys(), desc=\"Processing associations\"):\n",
    "        try:\n",
    "            arr = np.asarray(association[i][0])\n",
    "            detections = arr[:, 1].astype(int) if arr.ndim == 2 and arr.size > 0 else np.array([], dtype=int)\n",
    "            num_stations = arr.shape[0] if arr.ndim == 2 else 0\n",
    "        except Exception:\n",
    "            detections = np.array([], dtype=int)\n",
    "            num_stations = 0\n",
    "\n",
    "        det_set = frozenset(detections.tolist())\n",
    "        quality_metrics = compute_quality_metrics(results_dict[i])\n",
    "\n",
    "        events[i] = {\n",
    "            'detection_set': det_set,\n",
    "            'num_stations': num_stations,\n",
    "            'quality': quality_metrics\n",
    "        }\n",
    "\n",
    "        # Mapping détection → événements\n",
    "        for det in det_set:\n",
    "            det_to_events[det].add(i)\n",
    "\n",
    "    # Construction du graphe de chevauchement\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(events.keys())\n",
    "\n",
    "    for det, event_set in det_to_events.items():\n",
    "        if len(event_set) > 1:\n",
    "            for event1, event2 in combinations(sorted(event_set), 2):\n",
    "                G.add_edge(event1, event2)\n",
    "\n",
    "    print(f\"Overlap graph: {G.number_of_nodes()} events, {G.number_of_edges()} overlaps\")\n",
    "\n",
    "    # Séparation des événements isolés et groupés\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    isolated_events = [list(c)[0] for c in clusters if len(c) == 1]\n",
    "    overlapping_clusters = [c for c in clusters if len(c) > 1]\n",
    "\n",
    "    print(f\"Found {len(isolated_events)} isolated events and {len(overlapping_clusters)} overlapping clusters\")\n",
    "\n",
    "    def event_priority_score(event_id):\n",
    "        \"\"\"Score de priorité pour la sélection (plus élevé = meilleur)\"\"\"\n",
    "        ev = events[event_id]\n",
    "        qual = ev['quality']\n",
    "\n",
    "        # Critères de rejet immédiat\n",
    "        if qual['num_obs'] < 3:  # Trop peu de données\n",
    "            return (-np.inf, 0, 0, 0)\n",
    "        if not qual['passes_chi2']:  # Échec test chi2\n",
    "            return (-np.inf, 0, 0, 0)\n",
    "        if qual['max_residual_zscore'] > 8.0:  # Résidu trop extrême\n",
    "            return (-np.inf, 0, 0, 0)\n",
    "\n",
    "        # Score principal basé sur la qualité\n",
    "        base_score = qual['quality_score']\n",
    "\n",
    "        # Bonus pour le nombre de stations (diversité des données)\n",
    "        station_bonus = min(ev['num_stations'] / 10.0, 1.0)\n",
    "\n",
    "        # Pénalité pour le chi2 élevé\n",
    "        chi2_penalty = 1.0 / (1.0 + qual['chi2_reduced'])\n",
    "\n",
    "        # Score composite\n",
    "        composite_score = (\n",
    "            base_score * 0.6 +\n",
    "            station_bonus * 0.2 +\n",
    "            chi2_penalty * 0.2\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            composite_score,  # Score principal\n",
    "            qual['p_value']\n",
    "            -qual['chi2_reduced'],  # Chi2 (plus bas = mieux)\n",
    "            -qual['outlier_fraction'],  # Fraction d'outliers (plus bas = mieux)\n",
    "            ev['num_stations']  # Nombre de stations\n",
    "        )\n",
    "\n",
    "    selected_events = set()\n",
    "    selected_detections = set()\n",
    "\n",
    "    # Traitement des clusters avec chevauchement\n",
    "    print(\"Resolving overlapping clusters...\")\n",
    "    for cluster in tqdm(overlapping_clusters, desc=\"Processing clusters\"):\n",
    "        cluster_events = list(cluster)\n",
    "\n",
    "        # Tri par qualité décroissante\n",
    "        cluster_events.sort(key=event_priority_score, reverse=True)\n",
    "\n",
    "        # Sélection gloutonne optimisée\n",
    "        cluster_selected = []\n",
    "        cluster_used_dets = set()\n",
    "\n",
    "        for event_id in cluster_events:\n",
    "            event_dets = events[event_id]['detection_set']\n",
    "\n",
    "            # Vérifier le chevauchement et la qualité\n",
    "            has_overlap = not event_dets.isdisjoint(cluster_used_dets)\n",
    "            is_high_quality = event_priority_score(event_id)[0] > -np.inf\n",
    "\n",
    "            if not has_overlap and is_high_quality:\n",
    "                cluster_selected.append(event_id)\n",
    "                cluster_used_dets.update(event_dets)\n",
    "\n",
    "        # Si conflit persistant, prendre le meilleur événement du cluster\n",
    "        if not cluster_selected and cluster_events:\n",
    "            best_event = cluster_events[0]\n",
    "            if event_priority_score(best_event)[0] > -np.inf:\n",
    "                cluster_selected = [best_event]\n",
    "                cluster_used_dets = events[best_event]['detection_set']\n",
    "\n",
    "        selected_events.update(cluster_selected)\n",
    "        selected_detections.update(cluster_used_dets)\n",
    "\n",
    "    # Ajout des événements isolés de bonne qualité\n",
    "    print(\"Adding qualified isolated events...\")\n",
    "    for event_id in isolated_events:\n",
    "        if (event_priority_score(event_id)[0] > -np.inf and\n",
    "            events[event_id]['detection_set'].isdisjoint(selected_detections)):\n",
    "            selected_events.add(event_id)\n",
    "            selected_detections.update(events[event_id]['detection_set'])\n",
    "\n",
    "    # Validation finale et statistiques\n",
    "    final_events = list(selected_events)\n",
    "\n",
    "    # Vérification de cohérence\n",
    "    total_dets_in_events = set()\n",
    "    for event_id in final_events:\n",
    "        total_dets_in_events.update(events[event_id]['detection_set'])\n",
    "\n",
    "    if total_dets_in_events != selected_detections:\n",
    "        print(\"Warning: Inconsistency in detection tracking\")\n",
    "        # Correction automatique\n",
    "        selected_detections = total_dets_in_events\n",
    "\n",
    "    # Statistiques de qualité finale\n",
    "    if final_events:\n",
    "        quality_scores = [events[eid]['quality']['quality_score'] for eid in final_events]\n",
    "        chi2_values = [events[eid]['quality']['chi2_reduced'] for eid in final_events]\n",
    "        num_stations_list = [events[eid]['num_stations'] for eid in final_events]\n",
    "\n",
    "        print(f\"\\n=== FILTERING RESULTS ===\")\n",
    "        print(f\"Selected {len(final_events)} events from {len(events)} candidates\")\n",
    "        print(f\"Quality scores: mean={np.mean(quality_scores):.3f} ± {np.std(quality_scores):.3f}\")\n",
    "        print(f\"Chi2 reduced: mean={np.mean(chi2_values):.3f} ± {np.std(chi2_values):.3f}\")\n",
    "        print(f\"Stations per event: mean={np.mean(num_stations_list):.1f} ± {np.std(num_stations_list):.1f}\")\n",
    "        print(f\"Detection reuse efficiency: {len(selected_detections)} detections for {len(final_events)} events\")\n",
    "\n",
    "        # Analyse des rejets\n",
    "        rejected = set(events.keys()) - set(final_events)\n",
    "        if rejected:\n",
    "            reject_reasons = []\n",
    "            for rid in rejected:\n",
    "                qual = events[rid]['quality']\n",
    "                if qual['num_obs'] < 3:\n",
    "                    reject_reasons.append(\"too_few_observations\")\n",
    "                elif not qual['passes_chi2']:\n",
    "                    reject_reasons.append(\"failed_chi2_test\")\n",
    "                elif qual['max_residual_zscore'] > 8.0:\n",
    "                    reject_reasons.append(\"extreme_residuals\")\n",
    "                else:\n",
    "                    reject_reasons.append(\"overlap_or_quality\")\n",
    "\n",
    "            print(f\"Rejection reasons: {dict(Counter(reject_reasons))}\")\n",
    "    else:\n",
    "        print(\"Warning: No events selected after filtering!\")\n",
    "        # Fallback: prendre les meilleurs événements non chevauchants\n",
    "        final_events = fallback_selection(events, selected_detections)\n",
    "\n",
    "    filtered_results = {eid: results_dict[eid] for eid in final_events if eid in results_dict}\n",
    "    return filtered_results\n",
    "\n",
    "def fallback_selection(events, used_detections):\n",
    "    \"\"\"Sélection de repli si la sélection principale échoue\"\"\"\n",
    "    candidates = []\n",
    "    for event_id, ev_data in events.items():\n",
    "        if (ev_data['quality']['num_obs'] >= 3 and\n",
    "            ev_data['detection_set'].isdisjoint(used_detections)):\n",
    "            candidates.append(event_id)\n",
    "\n",
    "    # Tri par qualité\n",
    "    candidates.sort(key=lambda x: events[x]['quality']['quality_score'], reverse=True)\n",
    "\n",
    "    selected = []\n",
    "    current_dets = set(used_detections)\n",
    "\n",
    "    for cand in candidates:\n",
    "        cand_dets = events[cand]['detection_set']\n",
    "        if current_dets.isdisjoint(cand_dets):\n",
    "            selected.append(cand)\n",
    "            current_dets.update(cand_dets)\n",
    "\n",
    "    return selected\n",
    "\n",
    "def filter_associations_simple(results_dict, association, cost_threshold=10000):\n",
    "    \"\"\"\n",
    "    Version améliorée du filtrage des associations avec fonctions séparées.\n",
    "\n",
    "    Args:\n",
    "        results_dict: Dictionnaire des résultats d'optimisation\n",
    "        association: Structure contenant les associations de détections\n",
    "        cost_threshold: Seuil de coût pour le filtrage initial\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire des résultats filtrés\n",
    "    \"\"\"\n",
    "    print(\"Starting association filtering...\")\n",
    "    print(f\"Initial associations count: {len(results_dict)}\")\n",
    "\n",
    "    # Étape 1: Filtrer par seuil de coût\n",
    "    filtered_by_cost = filter_by_cost_threshold(results_dict, cost_threshold)\n",
    "    # filtered_by_cost = results_dict\n",
    "    # Étape 2: Supprimer les associations subset\n",
    "    filtered_no_subsets, excluded_indices = remove_subset_associations(filtered_by_cost, association)\n",
    "    # filtered_no_subsets =results_dict\n",
    "    # excluded_indices = []\n",
    "    # Étape 3: Résoudre les overlaps\n",
    "    final_results = remove_overlap(filtered_no_subsets, association)\n",
    "    # final_results = filtered_no_subsets\n",
    "    print(\"Association filtering completed!\")\n",
    "    print(f\"Final results: {len(final_results)} associations\")\n",
    "\n",
    "    return final_results, excluded_indices\n",
    "\n",
    "def run_filtered_associations():\n",
    "    \"\"\"Pipeline complet avec filtrage - version simplifiée\"\"\"\n",
    "    CHUNK_SIZE = 25\n",
    "\n",
    "    # # Étape 1: Traiter toutes les associations avec dictionnaire\n",
    "    # print(\"Step 1: Processing all associations...\")\n",
    "    # results = {}\n",
    "    # with mp.Pool(mp.cpu_count()) as pool:\n",
    "    #     for r in tqdm(pool.imap(process, range(len(association)), chunksize=CHUNK_SIZE),\n",
    "    #                   total=len(association)):\n",
    "    #         if r is not None:\n",
    "    #             i, res = r\n",
    "    #             results[i] = res\n",
    "\n",
    "    print(f\"Raw associations processed: {len(results)}\")\n",
    "    with open(\"results_sium.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    # Étape 2: Filtrer et déduplicater\n",
    "    filtered_results, excluded_indices = filter_associations_simple(results,association, cost_threshold= 900)\n",
    "\n",
    "    # Étape 3: Sauvegarder\n",
    "    # Sauvegarder juste les résultats comme array (compatible avec votre format original)\n",
    "    results_array = [filtered_results[i] for i in sorted(filtered_results.keys())]\n",
    "    np.save(\"filtered_results.npy\", np.array(results_array))\n",
    "\n",
    "    # Optionnel: sauvegarder les métadonnées\n",
    "    metadata = {\n",
    "        'filtered_indices': list(filtered_results.keys()),\n",
    "        'excluded_indices': list(excluded_indices),\n",
    "        'original_count': len(association),\n",
    "        'processed_count': len(results),\n",
    "        'final_count': len(filtered_results)\n",
    "    }\n",
    "    np.save(\"association_metadata.npy\", metadata)\n",
    "\n",
    "    print(f\"Processing complete:\")\n",
    "    print(f\"  Original associations: {len(association)}\")\n",
    "    print(f\"  Successfully processed: {len(results)}\")\n",
    "    print(f\"  After filtering: {len(filtered_results)}\")\n",
    "    print(f\"  Excluded: {len(excluded_indices)}\")\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "# Utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = run_filtered_associations()"
   ],
   "id": "d26487041f7f6c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_association_map(results, stations_dict, title=\"Associations localisées\"):\n",
    "    \"\"\"\n",
    "    Carte montrant les positions des événements et des stations\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Extraire les positions des événements\n",
    "    event_positions = []\n",
    "    costs = []\n",
    "    is_valid = []\n",
    "\n",
    "    for i, res in results.items():\n",
    "        if hasattr(res, 'x') and len(res.x) >= 2:\n",
    "            # Supposer que res.x = [t0, lat, lon] ou [lat, lon]\n",
    "            if len(res.x) == 3:\n",
    "                lat, lon = res.x[1], res.x[2]\n",
    "            else:\n",
    "                lat, lon = res.x[0], res.x[1]\n",
    "            event_positions.append([lat, lon])\n",
    "            cost = res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)\n",
    "            costs.append(cost)\n",
    "\n",
    "    if event_positions:\n",
    "        event_positions = np.array(event_positions)\n",
    "\n",
    "        costs = np.array(costs)\n",
    "\n",
    "        # Scatter plot des événements colorés par cost\n",
    "        scatter = ax.scatter(event_positions[:, 1], event_positions[:, 0],\n",
    "                           c=costs, cmap='viridis_r', s=50, alpha=0.7,\n",
    "                           label='Événements')\n",
    "        plt.colorbar(scatter, label='Cost')\n",
    "\n",
    "        # Ajouter les stations si disponibles\n",
    "        if stations_dict:\n",
    "            station_lats = []\n",
    "            station_lons = []\n",
    "            for station in stations_dict:\n",
    "                pos = station.get_pos()\n",
    "                station_lats.append(pos[0])\n",
    "                station_lons.append(pos[1])\n",
    "\n",
    "            ax.scatter(station_lons, station_lats, c='red', marker='^',\n",
    "                      s=100, label='Stations', alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_cost_distribution(results, bins=50):\n",
    "    \"\"\"\n",
    "    Distribution des costs\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    red_chi_squares = []\n",
    "\n",
    "    for i, res in results.items():\n",
    "        cost = res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Calculer reduced chi-square\n",
    "        n_obs = len(res.fun)\n",
    "        n_params = 2\n",
    "        dof = n_obs - n_params\n",
    "        red_chi_sq = cost / dof if dof > 0 else float('inf')\n",
    "        red_chi_squares.append(red_chi_sq)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Distribution des costs\n",
    "    ax1.hist(costs, bins=bins, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Cost')\n",
    "    ax1.set_ylabel('Nombre d\\'associations')\n",
    "    ax1.set_title('Distribution des Costs')\n",
    "    ax1.axvline(np.median(costs), color='red', linestyle='--',\n",
    "                label=f'Médiane: {np.median(costs):.1f}')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Distribution des reduced chi-square\n",
    "    red_chi_squares = [x for x in red_chi_squares if x < 100]  # Filtrer les valeurs extrêmes\n",
    "    ax2.hist(red_chi_squares, bins=bins, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Reduced Chi-square')\n",
    "    ax2.set_ylabel('Nombre d\\'associations')\n",
    "    ax2.set_title('Distribution des Reduced Chi-square')\n",
    "    ax2.axvline(1.0, color='green', linestyle='--', label='Valeur idéale')\n",
    "    ax2.axvline(np.median(red_chi_squares), color='red', linestyle='--',\n",
    "                label=f'Médiane: {np.median(red_chi_squares):.2f}')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, costs, red_chi_squares\n",
    "\n",
    "def plot_residuals_analysis(results, association_data, idx_to_det, stations_dict):\n",
    "    \"\"\"\n",
    "    Analyse des résidus par station\n",
    "    \"\"\"\n",
    "    station_residuals = {}\n",
    "\n",
    "    for i, res in results.items():\n",
    "        if i in association_data:\n",
    "            stations_idx = association_data[i][0][:,0]\n",
    "            residuals = res.fun\n",
    "\n",
    "            for j, station_idx in enumerate(stations_idx):\n",
    "                if j < len(residuals):\n",
    "                    if station_idx not in station_residuals:\n",
    "                        station_residuals[station_idx] = []\n",
    "                    station_residuals[station_idx].append(residuals[j])\n",
    "\n",
    "    # Box plot des résidus par station\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    station_names = []\n",
    "    residual_data = []\n",
    "\n",
    "    for station_idx, residuals in station_residuals.items():\n",
    "        if len(residuals) >= 5:  # Au moins 5 mesures\n",
    "            station_names.append(f'Station {station_idx}')\n",
    "            residual_data.append(residuals)\n",
    "\n",
    "    if residual_data:\n",
    "        ax.boxplot(residual_data, labels=station_names)\n",
    "        ax.set_ylabel('Résidus (s)')\n",
    "        ax.set_title('Distribution des résidus par station')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_quality_metrics(results, association_data, stations_dict):\n",
    "    \"\"\"\n",
    "    Métriques de qualité des associations\n",
    "    \"\"\"\n",
    "    quality_data = []\n",
    "\n",
    "    for i, res in results.items():\n",
    "        try:\n",
    "            if True :\n",
    "                # Récupérer les positions des stations\n",
    "                stations_idx = association_data[i][0][:,0]\n",
    "                stations_pos = [stations_dict[idx].get_pos() for idx in stations_idx]\n",
    "\n",
    "                # Calculer les métriques de base\n",
    "                cost = res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)\n",
    "\n",
    "                # Reduced chi-square\n",
    "                n_obs = len(res.fun)\n",
    "                n_params = 2\n",
    "                dof = n_obs - n_params\n",
    "                red_chi_sq = cost / dof if dof > 0 else float('inf')\n",
    "\n",
    "                # Gap azimutal\n",
    "                event_pos = (res.x[1], res.x[2]) if len(res.x) > 2 else (res.x[0], res.x[1])\n",
    "\n",
    "                # Calculer azimuts\n",
    "                azimuths = []\n",
    "                for sensor_pos in stations_pos:\n",
    "                    dx = sensor_pos[1] - event_pos[1]\n",
    "                    dy = sensor_pos[0] - event_pos[0]\n",
    "                    azimuth = np.degrees(np.arctan2(dx, dy)) % 360\n",
    "                    azimuths.append(azimuth)\n",
    "\n",
    "                # Calculer gap azimutal\n",
    "                azimuths_sorted = sorted(azimuths)\n",
    "                gaps = []\n",
    "                for j in range(len(azimuths_sorted)):\n",
    "                    next_idx = (j + 1) % len(azimuths_sorted)\n",
    "                    gap = (azimuths_sorted[next_idx] - azimuths_sorted[j]) % 360\n",
    "                    gaps.append(gap)\n",
    "                az_gap = max(gaps) if gaps else 360\n",
    "\n",
    "                # Incertitude position (approximation simple)\n",
    "                try:\n",
    "                    if hasattr(res, 'jac') and res.jac is not None:\n",
    "                        J = res.jac\n",
    "                        cov_matrix = np.linalg.inv(J.T @ J) * (2 * cost) / max(dof, 1)\n",
    "                        max_uncert = np.sqrt(max(np.diag(cov_matrix)))\n",
    "                    else:\n",
    "                        max_uncert = None\n",
    "                except:\n",
    "                    max_uncert = None\n",
    "\n",
    "                quality_data.append({\n",
    "                    'association_id': i,\n",
    "                    'cost': cost,\n",
    "                    'reduced_chi_sq': red_chi_sq,\n",
    "                    'azimuthal_gap': az_gap,\n",
    "                    'max_uncertainty': max_uncert,\n",
    "                    'num_stations': len(stations_pos)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pour l'association {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(quality_data)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Aucune donnée de qualité disponible\")\n",
    "        return plt.figure(), pd.DataFrame()\n",
    "\n",
    "    # Créer une figure avec subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Cost vs Reduced Chi-square\n",
    "    if 'cost' in df.columns and 'reduced_chi_sq' in df.columns:\n",
    "        valid_data = df[(df['reduced_chi_sq'] != float('inf')) & (df['reduced_chi_sq'] < 100)]\n",
    "        if not valid_data.empty:\n",
    "            axes[0,0].scatter(valid_data['cost'], valid_data['reduced_chi_sq'], alpha=0.6)\n",
    "            axes[0,0].set_xlabel('Cost')\n",
    "            axes[0,0].set_ylabel('Reduced Chi-square')\n",
    "            axes[0,0].set_title('Cost vs Reduced Chi-square')\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Gap azimutal vs Nombre de stations\n",
    "    if 'num_stations' in df.columns and 'azimuthal_gap' in df.columns:\n",
    "        axes[0,1].scatter(df['num_stations'], df['azimuthal_gap'], alpha=0.6)\n",
    "        axes[0,1].set_xlabel('Nombre de stations')\n",
    "        axes[0,1].set_ylabel('Gap azimutal (°)')\n",
    "        axes[0,1].set_title('Gap azimutal vs Nombre de stations')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Incertitude vs Cost\n",
    "    if 'max_uncertainty' in df.columns and 'cost' in df.columns:\n",
    "        df_clean = df.dropna(subset=['max_uncertainty'])\n",
    "        df_clean = df_clean[df_clean['max_uncertainty'] != float('inf')]\n",
    "        if not df_clean.empty:\n",
    "            axes[1,0].scatter(df_clean['cost'], df_clean['max_uncertainty'], alpha=0.6)\n",
    "            axes[1,0].set_xlabel('Cost')\n",
    "            axes[1,0].set_ylabel('Incertitude max position')\n",
    "            axes[1,0].set_title('Incertitude vs Cost')\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1,0].text(0.5, 0.5, 'Pas de données d\\'incertitude disponibles',\n",
    "                          ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "\n",
    "    # Histogramme du gap azimutal\n",
    "    if 'azimuthal_gap' in df.columns:\n",
    "        axes[1,1].hist(df['azimuthal_gap'], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1,1].axvline(180, color='red', linestyle='--', label='Seuil critique (180°)')\n",
    "        axes[1,1].set_xlabel('Gap azimutal (°)')\n",
    "        axes[1,1].set_ylabel('Nombre d\\'associations')\n",
    "        axes[1,1].set_title('Distribution du gap azimutal')\n",
    "        axes[1,1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, df\n",
    "\n",
    "def create_interactive_map(results, stations_dict):\n",
    "    \"\"\"\n",
    "    Carte interactive avec Plotly\n",
    "    \"\"\"\n",
    "    # Préparer les données\n",
    "    event_data = []\n",
    "\n",
    "    for i, res in results.items():\n",
    "        if hasattr(res, 'x') and len(res.x) >= 2:\n",
    "            if len(res.x) == 3:\n",
    "                lat, lon = res.x[1], res.x[2]\n",
    "            else:\n",
    "                lat, lon = res.x[0], res.x[1]\n",
    "\n",
    "            cost = res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)\n",
    "\n",
    "            event_data.append({\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'cost': cost,\n",
    "                'association_id': i,\n",
    "                'type': 'Événement'\n",
    "            })\n",
    "\n",
    "    # Ajouter les stations\n",
    "    station_data = []\n",
    "    if stations_dict:\n",
    "        for idx, station in enumerate(stations_dict):\n",
    "            pos = station.get_pos()\n",
    "            station_data.append({\n",
    "                'lat': pos[0],\n",
    "                'lon': pos[1],\n",
    "                'station_id': idx,\n",
    "                'type': 'Station'\n",
    "            })\n",
    "\n",
    "    # Créer la figure Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ajouter les événements\n",
    "    if event_data:\n",
    "        events_df = pd.DataFrame(event_data)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=events_df['lon'],\n",
    "            y=events_df['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=events_df['cost'],\n",
    "                colorscale='Viridis_r',\n",
    "                colorbar=dict(title=\"Cost\"),\n",
    "                showscale=True\n",
    "            ),\n",
    "            text=events_df.apply(lambda row: f\"Association {row['association_id']}<br>Cost: {row['cost']:.1f}\", axis=1),\n",
    "            hovertemplate='%{text}<br>Lat: %{y:.4f}<br>Lon: %{x:.4f}<extra></extra>',\n",
    "            name='Événements'\n",
    "        ))\n",
    "\n",
    "    # Ajouter les stations\n",
    "    if station_data:\n",
    "        stations_df = pd.DataFrame(station_data)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=stations_df['lon'],\n",
    "            y=stations_df['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                color='red',\n",
    "                symbol='triangle-up'\n",
    "            ),\n",
    "            text=stations_df.apply(lambda row: f\"Station {row['station_id']}\", axis=1),\n",
    "            hovertemplate='%{text}<br>Lat: %{y:.4f}<br>Lon: %{x:.4f}<extra></extra>',\n",
    "            name='Stations'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Carte interactive des associations\",\n",
    "        xaxis_title=\"Longitude\",\n",
    "        yaxis_title=\"Latitude\",\n",
    "        showlegend=True,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def generate_summary_report(results, association_data, stations_dict):\n",
    "    \"\"\"\n",
    "    Génère un rapport de synthèse\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RAPPORT DE SYNTHÈSE DES ASSOCIATIONS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Statistiques générales\n",
    "    print(f\"\\n📊 STATISTIQUES GÉNÉRALES\")\n",
    "    print(f\"   • Nombre total d'associations: {len(results)}\")\n",
    "\n",
    "    # Analyse des costs\n",
    "    costs = [res.cost if hasattr(res, 'cost') else np.sum(res.fun**2) for res in results.values()]\n",
    "    print(f\"\\n💰 ANALYSE DES COSTS\")\n",
    "    print(f\"   • Cost moyen: {np.mean(costs):.2f}\")\n",
    "    print(f\"   • Cost médian: {np.median(costs):.2f}\")\n",
    "    print(f\"   • Cost min/max: {np.min(costs):.2f} / {np.max(costs):.2f}\")\n",
    "    print(f\"   • Écart-type: {np.std(costs):.2f}\")\n",
    "\n",
    "    # Seuils de qualité\n",
    "    good_threshold = np.percentile(costs, 75)\n",
    "    excellent_threshold = np.percentile(costs, 25)\n",
    "\n",
    "    excellent = sum(1 for c in costs if c <= excellent_threshold)\n",
    "    good = sum(1 for c in costs if excellent_threshold < c <= good_threshold)\n",
    "    poor = sum(1 for c in costs if c > good_threshold)\n",
    "\n",
    "    print(f\"\\n⭐ QUALITÉ DES ASSOCIATIONS\")\n",
    "    print(f\"   • Excellentes (Q1): {excellent} ({excellent/len(costs)*100:.1f}%)\")\n",
    "    print(f\"   • Bonnes (Q2-Q3): {good} ({good/len(costs)*100:.1f}%)\")\n",
    "    print(f\"   • Moyennes (Q4): {poor} ({poor/len(costs)*100:.1f}%)\")\n",
    "\n",
    "    # Analyse des reduced chi-square\n",
    "    red_chi_sqs = []\n",
    "    for res in results.values():\n",
    "        n_obs = len(res.fun)\n",
    "        n_params = 2\n",
    "        dof = n_obs - n_params\n",
    "        red_chi_sq = (res.cost if hasattr(res, 'cost') else np.sum(res.fun**2)) / dof if dof > 0 else float('inf')\n",
    "        if red_chi_sq != float('inf'):\n",
    "            red_chi_sqs.append(red_chi_sq)\n",
    "\n",
    "    if red_chi_sqs:\n",
    "        print(f\"\\n📈 REDUCED CHI-SQUARE\")\n",
    "        print(f\"   • Médian: {np.median(red_chi_sqs):.2f}\")\n",
    "        print(f\"   • < 1.5 (bon ajustement): {sum(1 for x in red_chi_sqs if x < 1.5)} ({sum(1 for x in red_chi_sqs if x < 1.5)/len(red_chi_sqs)*100:.1f}%)\")\n",
    "        print(f\"   • > 3.0 (mauvais ajustement): {sum(1 for x in red_chi_sqs if x > 3.0)} ({sum(1 for x in red_chi_sqs if x > 3.0)/len(red_chi_sqs)*100:.1f}%)\")\n",
    "\n",
    "# Fonction principale de visualisation\n",
    "def visualize_association_results(results, association_data, stations_dict, idx_to_det,\n",
    "                                 save_plots=True, output_dir=\"./plots/\"):\n",
    "    \"\"\"\n",
    "    Lance toutes les visualisations\n",
    "    \"\"\"\n",
    "\n",
    "    if save_plots:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Génération des visualisations...\")\n",
    "\n",
    "    # 1. Rapport de synthèse\n",
    "    generate_summary_report(results, association_data, stations_dict)\n",
    "\n",
    "    # 2. Carte des associations\n",
    "    fig1 = plot_association_map(results, stations_dict)\n",
    "    if save_plots:\n",
    "        fig1.savefig(f\"{output_dir}/association_map.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 3. Distribution des costs\n",
    "    fig2, costs, red_chi_sqs = plot_cost_distribution(results)\n",
    "    if save_plots:\n",
    "        fig2.savefig(f\"{output_dir}/cost_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 4. Analyse des résidus\n",
    "    fig3 = plot_residuals_analysis(results, association_data, idx_to_det, stations_dict)\n",
    "    if save_plots:\n",
    "        fig3.savefig(f\"{output_dir}/residuals_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 5. Métriques de qualité\n",
    "    fig4, quality_df = plot_quality_metrics(results, association_data, stations_dict)\n",
    "    if save_plots:\n",
    "        fig4.savefig(f\"{output_dir}/quality_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # # 6. Carte interactive (Plotly)\n",
    "    # fig_interactive = create_interactive_map(results, stations_dict)\n",
    "    # if True:\n",
    "    #     fig_interactive.write_html(f\"{output_dir}/interactive_map.html\")\n",
    "    #\n",
    "    # plt.show()\n",
    "\n",
    "    return {\n",
    "        'costs': costs,\n",
    "        'red_chi_squares': red_chi_sqs,\n",
    "        'quality_dataframe': quality_df,\n",
    "        # 'interactive_map': fig_interactive\n",
    "    }\n",
    "\n",
    "# Utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Supposons que vous avez vos résultats filtrés\n",
    "    # results = {...}  # Votre dictionnaire de résultats\n",
    "\n",
    "    # Lancer toutes les visualisations\n",
    "    viz_results = visualize_association_results(\n",
    "        results=final_results,\n",
    "        association_data=association,\n",
    "        stations_dict=STATIONS,\n",
    "        idx_to_det=IDX_TO_DET,\n",
    "        save_plots=False,\n",
    "        output_dir=\"./association_plots/\"\n",
    "    )"
   ],
   "id": "e3353c02c913d435",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checking results",
   "id": "30cc73dd96c6914c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_residuals(result):\n",
    "    \"\"\"Analyse la qualité des résidus pour évaluer la pertinence de l'association\"\"\"\n",
    "    residuals = result.fun\n",
    "    num_params = len(result.x) if hasattr(result, 'x') else 0\n",
    "    num_obs = len(residuals)\n",
    "    dof = max(1, num_obs - num_params-1)\n",
    "\n",
    "    if num_obs == 0:\n",
    "        print(\"error\")\n",
    "\n",
    "    # Chi-carré réduit\n",
    "    chi2_reduced = np.sum(residuals**2) / dof\n",
    "\n",
    "    # Z-score des résidus (résidus standardisés)\n",
    "    residual_std = np.std(residuals) if np.std(residuals) > 1e-10 else 1e-10\n",
    "    z_scores = np.abs(residuals) / residual_std\n",
    "    max_zscore = np.max(z_scores)\n",
    "\n",
    "    # Fraction d'outliers (|z-score| > 2.5)\n",
    "    outlier_fraction = np.mean(z_scores > 2.0)\n",
    "\n",
    "    # Test de normalité des résidus (Shapiro-Wilk si n < 5000, sinon Kolmogorov-Smirnov)\n",
    "    if num_obs >= 3 and num_obs < 5000:\n",
    "         _, normality_pvalue = stats.shapiro(residuals)\n",
    "\n",
    "    # Score de qualité composite (plus c'est haut, mieux c'est)\n",
    "    quality_score = 1.0 / (1.0 + chi2_reduced)  # Pénalise chi2 élevé\n",
    "    quality_score *= (1.0 - outlier_fraction)    # Pénalise les outliers\n",
    "    quality_score *= 1.0 / (1.0 + max_zscore/10.0)  # Pénalise les résidus extrêmes\n",
    "\n",
    "    return {\n",
    "        'chi2_reduced': float(chi2_reduced),\n",
    "        'max_residual_zscore': float(max_zscore),\n",
    "        'outlier_fraction': float(outlier_fraction),\n",
    "        'normality_pvalue': float(normality_pvalue),\n",
    "        'quality_score': float(quality_score),\n",
    "        'cost':float(res.cost)\n",
    "    }"
   ],
   "id": "7cbaf3dbf78af570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "quality = []\n",
    "rows = []\n",
    "for i in final_results:\n",
    "    res = final_results[i]\n",
    "    rows.append({\n",
    "        \"lat\": res.x[1],\n",
    "        \"lon\": res.x[2],\n",
    "        \"origin_time\": res.x[0],\n",
    "        \"cost\": res.cost\n",
    "    })\n",
    "    # quality.append(analyze_residuals(res))\n",
    "\n",
    "# quality = pd.DataFrame(quality)\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"origin_time\"] = pd.to_datetime(df[\"origin_time\"], unit=\"s\",utc=True)\n",
    "\n",
    "try :\n",
    "    ground_truth['origin_time'] = pd.to_datetime(ground_truth['origin_time'])\n",
    "    ground_truth['origin_time'] = ground_truth['origin_time'].dt.tz_localize('UTC')\n",
    "except :\n",
    "    pass\n",
    "\n",
    "matches = []\n",
    "\n",
    "time_threshold = pd.Timedelta(seconds=3*60)\n",
    "latlon_threshold = 1 # en degrés (à adapter !)\n",
    "\n",
    "for i, row_res in df.iterrows():\n",
    "    for j, row_gt in ground_truth.iterrows():\n",
    "        # condition temporelle\n",
    "        time_close = abs(row_res[\"origin_time\"] - row_gt[\"origin_time\"]) <= time_threshold\n",
    "\n",
    "        # condition spatiale (ici simple différence en degrés)\n",
    "        lat_close = abs(row_res[\"lat\"] - row_gt[\"lat\"]) <= latlon_threshold\n",
    "        lon_close = abs(row_res[\"lon\"] - row_gt[\"lon\"]) <= latlon_threshold\n",
    "\n",
    "        if time_close and lat_close and lon_close:\n",
    "            matches.append((i, j))\n",
    "\n",
    "print(len(matches))"
   ],
   "id": "ab83707585aea24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_precision_recall(df, ground_truth, matches):\n",
    "    \"\"\"\n",
    "    Évalue la précision et le rappel à partir des matches trouvés.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame des détections prédites\n",
    "        ground_truth: DataFrame de la vérité terrain\n",
    "        matches: Liste de tuples (index_pred, index_gt) des correspondances\n",
    "\n",
    "    Returns:\n",
    "        dict: Métriques d'évaluation\n",
    "    \"\"\"\n",
    "    # Convertir les matches en sets pour éviter les doublons\n",
    "    matched_pred_indices = set(i for i, j in matches)\n",
    "    matched_gt_indices = set(j for i, j in matches)\n",
    "\n",
    "    # Calcul des métriques de base\n",
    "    true_positives = len(matches)\n",
    "    false_positives = len(df) - len(matched_pred_indices)\n",
    "    false_negatives = len(ground_truth) - len(matched_gt_indices)\n",
    "\n",
    "    # Calcul de Precision et Recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    # F1-score (moyenne harmonique)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'n_predictions': len(df),\n",
    "        'n_ground_truth': len(ground_truth)\n",
    "    }\n"
   ],
   "id": "3fa3494e5a708411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_unique_matches(df, ground_truth, time_threshold=pd.Timedelta(seconds=60),\n",
    "                       latlon_threshold=1):\n",
    "    \"\"\"\n",
    "    Trouve des matches uniques (1 vérité terrain → 1 prédiction maximum)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    used_gt_indices = set()\n",
    "    used_pred_indices = set()\n",
    "\n",
    "    # Créer toutes les paires candidates\n",
    "    candidates = []\n",
    "\n",
    "    for i, row_res in df.iterrows():\n",
    "        for j, row_gt in ground_truth.iterrows():\n",
    "            time_close = abs(row_res[\"origin_time\"] - row_gt[\"origin_time\"]) <= time_threshold\n",
    "            lat_close = abs(row_res[\"lat\"] - row_gt[\"lat\"]) <= latlon_threshold\n",
    "            lon_close = abs(row_res[\"lon\"] - row_gt[\"lon\"]) <= latlon_threshold\n",
    "\n",
    "            if time_close and lat_close and lon_close:\n",
    "                # Calculer la distance composite comme score de qualité\n",
    "                time_diff = abs(row_res[\"origin_time\"] - row_gt[\"origin_time\"]).total_seconds()\n",
    "                spatial_diff = np.sqrt((row_res[\"lat\"] - row_gt[\"lat\"])**2 +\n",
    "                                     (row_res[\"lon\"] - row_gt[\"lon\"])**2)\n",
    "                score = time_diff/time_threshold.total_seconds() + spatial_diff/latlon_threshold\n",
    "\n",
    "                candidates.append((score, i, j))\n",
    "\n",
    "    # Trier par meilleur score (plus proche)\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Assigner les matches sans doublons\n",
    "    for score, pred_idx, gt_idx in candidates:\n",
    "        if pred_idx not in used_pred_indices and gt_idx not in used_gt_indices:\n",
    "            matches.append((pred_idx, gt_idx))\n",
    "            used_pred_indices.add(pred_idx)\n",
    "            used_gt_indices.add(gt_idx)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def find_missing_events(df, ground_truth, matches):\n",
    "    \"\"\"\n",
    "    Identifie tous les événements de la vérité terrain qui n'ont pas été détectés\n",
    "    \"\"\"\n",
    "    # Récupérer tous les indices GT qui ont été matchés\n",
    "    matched_gt_indices = set(j for i, j in matches)\n",
    "\n",
    "    # Trouver les indices GT qui n'ont PAS été matchés\n",
    "    missing_gt_indices = set(ground_truth.index) - matched_gt_indices\n",
    "\n",
    "    if len(missing_gt_indices) == 0:\n",
    "        print(\"Aucun événement manquant trouvé.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"ÉVÉNEMENTS RÉELS MANQUANTS: {len(missing_gt_indices)}\")\n",
    "    missing_events = []\n",
    "\n",
    "    for gt_idx in missing_gt_indices:\n",
    "        missing_event = ground_truth.loc[gt_idx]\n",
    "        missing_events.append(missing_event)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ÉVÉNEMENT MANQUANT {len(missing_events)+1}/{len(missing_gt_indices)} (Index {gt_idx})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Time: {missing_event['origin_time']}\")\n",
    "        print(f\"Latitude: {missing_event['lat']:.4f}\")\n",
    "        print(f\"Longitude: {missing_event['lon']:.4f}\")\n",
    "    missing_events = pd.DataFrame(missing_events)\n",
    "    return pd.DataFrame(missing_events)\n",
    "\n",
    "def find_false_positives(df, ground_truth, matches):\n",
    "\n",
    "    # Récupérer tous les indices df qui ont été matchés\n",
    "    matched_indices = set(i for i, j in matches)\n",
    "    # Trouver les indices df qui n'ont PAS été matchés\n",
    "    missing_indices = set(df.index) - matched_indices\n",
    "    if len(missing_indices) == 0:\n",
    "        print(\"Aucun événement manquant trouvé.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"False Positives (FP): {len(missing_indices)}\")\n",
    "    false_positives = []\n",
    "\n",
    "    for idx in missing_indices:\n",
    "        missing_event = df.loc[idx]\n",
    "        false_positives.append(missing_event)\n",
    "        # print(f\"{'='*60}\")\n",
    "        # print(f\"False Positives (FP) {len(false_positives)+1}/{len(missing_indices)} (Index {idx})\")\n",
    "        # # print(f\"{'='*60}\")\n",
    "        # print(f\"Time: {missing_event['origin_time']}\")\n",
    "        # print(f\"Latitude: {missing_event['lat']:.4f}\")\n",
    "        # print(f\"Longitude: {missing_event['lon']:.4f}\")\n",
    "    false_positives = pd.DataFrame(false_positives)\n",
    "    return false_positives\n",
    "\n",
    "# Réévaluation avec matching unique\n",
    "missing_events = find_missing_events(df, ground_truth, matches)\n",
    "find_false_positives(df, ground_truth, matches)\n",
    "unique_matches = find_unique_matches(df, ground_truth)\n",
    "metrics = evaluate_precision_recall(df, ground_truth, unique_matches)\n",
    "false_positives= find_false_positives(df, ground_truth, unique_matches)\n",
    "print(\"=== ÉVALUATION CORRIGÉE ===\")\n",
    "print(f\"Prédictions trouvées: {metrics['n_predictions']}\")\n",
    "print(f\"Événements réels: {metrics['n_ground_truth']}\")\n",
    "print(f\"True Positives (TP): {metrics['true_positives']}\")\n",
    "print(f\"False Positives (FP): {metrics['false_positives']}\")\n",
    "print(f\"False Negatives (FN): {metrics['false_negatives']}\")\n",
    "print(f\"Précision: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)\")\n",
    "print(f\"Rappel: {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.3f}\")"
   ],
   "id": "aab09781e4738cd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=== ÉVALUATION CORRIGÉE ===\")\n",
    "print(f\"Prédictions trouvées: {metrics['n_predictions']}\")\n",
    "print(f\"Événements réels: {metrics['n_ground_truth']}\")\n",
    "print(f\"True Positives (TP): {metrics['true_positives']}\")\n",
    "print(f\"False Positives (FP): {metrics['false_positives']}\")\n",
    "print(f\"False Negatives (FN): {metrics['false_negatives']}\")\n",
    "print(f\"Précision: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)\")\n",
    "print(f\"Rappel: {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.3f}\")"
   ],
   "id": "dafec3f58fe33cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carte simple des événements\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# True Positives (TP) - en bleu\n",
    "tp_indices = [i for i, j in unique_matches]\n",
    "tp_points = df.loc[tp_indices]\n",
    "plt.scatter(tp_points['lon'], tp_points['lat'],\n",
    "           c='blue', marker='o', s=50, alpha=0.7, label=f'TP ({len(tp_points)})')\n",
    "\n",
    "# False Positives (FP) - en orange\n",
    "if not false_positives.empty:\n",
    "    plt.scatter(false_positives['lon'], false_positives['lat'],\n",
    "               c='orange', marker='x', s=50, alpha=0.7, label=f'FP ({len(false_positives)})')\n",
    "\n",
    "# Ground Truth (GT) - contours verts\n",
    "plt.scatter(ground_truth['lon'], ground_truth['lat'],\n",
    "           facecolors='none', edgecolors='green', marker='s',\n",
    "           s=80, linewidth=2, label=f'GT ({len(ground_truth)})')\n",
    "\n",
    "# Missing Events (FN) - en rouge\n",
    "if not missing_events.empty:\n",
    "    plt.scatter(missing_events['lon'], missing_events['lat'],\n",
    "               c='red', marker='+', s=80, alpha=0.8, label=f'FN ({len(missing_events)})')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Carte des Événements - TP, FP, GT, FN')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Histogramme des coûts pour TP et FP\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extraire les coûts\n",
    "tp_costs = tp_points['cost'].values\n",
    "fp_costs = false_positives['cost'].values if not false_positives.empty else []\n",
    "\n",
    "# Créer l'histogramme comparatif\n",
    "if len(fp_costs) > 0:\n",
    "    # Histogramme comparatif TP vs FP\n",
    "    bins = np.linspace(min(np.min(tp_costs), np.min(fp_costs)),\n",
    "                      max(np.max(tp_costs), np.max(fp_costs)), 30)\n",
    "\n",
    "    plt.hist(tp_costs, bins=bins, alpha=0.7, color='blue', label=f'TP ({len(tp_costs)})', density=True)\n",
    "    plt.hist(fp_costs, bins=bins, alpha=0.7, color='orange', label=f'FP ({len(fp_costs)})', density=True)\n",
    "else:\n",
    "    # Seulement TP si pas de FP\n",
    "    plt.hist(tp_costs, bins=30, alpha=0.7, color='blue', label=f'TP ({len(tp_costs)})', density=True)\n",
    "\n",
    "plt.xlabel('Coût')\n",
    "plt.ylabel('Densité')\n",
    "plt.title('Distribution des Coûts - TP vs FP')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Statistiques descriptives des coûts\n",
    "print(\"\\n=== STATISTIQUES DES COÛTS ===\")\n",
    "print(f\"TP - Moyenne: {np.mean(tp_costs):.3f}, Médiane: {np.median(tp_costs):.3f}, Std: {np.std(tp_costs):.3f}\")\n",
    "\n",
    "if len(fp_costs) > 0:\n",
    "    print(f\"FP - Moyenne: {np.mean(fp_costs):.3f}, Médiane: {np.median(fp_costs):.3f}, Std: {np.std(fp_costs):.3f}\")\n",
    "\n",
    "    # Test statistique simple\n",
    "    if np.mean(fp_costs) > np.mean(tp_costs):\n",
    "        print(\"→ Les FP ont tendance à avoir des coûts plus élevés\")\n",
    "    else:\n",
    "        print(\"→ Les TP ont tendance à avoir des coûts plus élevés\")\n",
    "\n",
    "# 4. Boxplot comparatif des coûts\n",
    "if len(fp_costs) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data = [tp_costs, fp_costs]\n",
    "    labels = ['TP', 'FP']\n",
    "\n",
    "    plt.boxplot(data, labels=labels, patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "               medianprops=dict(color='red'))\n",
    "\n",
    "    plt.ylabel('Coût')\n",
    "    plt.title('Boxplot des Coûts - TP vs FP')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "37cce0655ba43129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_missing_events(df, ground_truth, matches)",
   "id": "c3213e9905931f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration minimale\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. Carte simple\n",
    "tp_indices = [i for i, j in unique_matches]\n",
    "tp_points = df.loc[tp_indices]\n",
    "\n",
    "# Points sur la carte\n",
    "ax1.scatter(tp_points['lon'], tp_points['lat'], c='blue', label=f'TP ({len(tp_points)})', alpha=0.7)\n",
    "if not false_positives.empty:\n",
    "    ax1.scatter(false_positives['lon'], false_positives['lat'], c='orange', marker='x', label=f'FP ({len(false_positives)})')\n",
    "ax1.scatter(ground_truth['lon'], ground_truth['lat'], facecolors='none', edgecolors='green', label='GT')\n",
    "if not missing_events.empty:\n",
    "    ax1.scatter(missing_events['lon'], missing_events['lat'], c='red', marker='^', label=f'FN ({len(missing_events)})')\n",
    "\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title('Carte des Événements')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histogramme des coûts\n",
    "tp_costs = tp_points['cost'].values\n",
    "fp_costs = false_positives['cost'].values if not false_positives.empty else []\n",
    "\n",
    "if len(fp_costs) > 0:\n",
    "    # ax2.hist(tp_costs, bins=20, alpha=0.7,  color='blue', label='TP')\n",
    "    ax2.hist(fp_costs, bins=20, alpha=0.7, color='orange', label='FP')\n",
    "else:\n",
    "    ax2.hist(tp_costs, bins=20, alpha=0.7, color='blue', label='TP', density=True)\n",
    "\n",
    "ax2.set_xlabel('Coût')\n",
    "ax2.set_ylabel('Densité')\n",
    "ax2.set_title('Distribution des Coûts')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des stats en une ligne\n",
    "print(f\"Coût TP: {np.mean(tp_costs):.3f} ± {np.std(tp_costs):.3f}\")\n",
    "if len(fp_costs) > 0:\n",
    "    print(f\"Coût FP: {np.mean(fp_costs):.3f} ± {np.std(fp_costs):.3f}\")"
   ],
   "id": "1f4ac358b88dba1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.hist(tp_costs, bins=3, alpha=0.7, color='orange', label='TP')",
   "id": "18beab7d2ec50796",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyse du seuil de coût optimal pour séparer TP et FP\n",
    "if len(fp_costs) > 0:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    # Préparer les données pour la courbe ROC\n",
    "    y_true = np.concatenate([np.ones(len(tp_costs)), np.zeros(len(fp_costs))])\n",
    "    y_scores = np.concatenate([tp_costs, fp_costs])\n",
    "\n",
    "    # Inverser les coûts (car généralement, coût bas = bon)\n",
    "    y_scores_inv = -y_scores  # ou utiliser y_scores_max = np.max(y_scores) - y_scores\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores_inv)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Trouver le seuil optimal (plus proche du coin en haut à gauche)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = -thresholds[optimal_idx]  # re-inverser\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Courbe ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100,\n",
    "                label=f'Seuil optimal: {optimal_threshold:.3f}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Courbe ROC basée sur le coût')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Distribution avec seuil optimal\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(tp_costs, bins=20, alpha=0.7, color='blue', label='TP', density=True)\n",
    "    plt.hist(fp_costs, bins=20, alpha=0.7, color='orange', label='FP', density=True)\n",
    "    plt.axvline(optimal_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Seuil optimal: {optimal_threshold:.3f}')\n",
    "    plt.xlabel('Coût')\n",
    "    plt.ylabel('Densité')\n",
    "    plt.title('Seuil de coût optimal')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Seuil de coût optimal: {optimal_threshold:.3f}\")\n",
    "    print(f\"TP en dessous du seuil: {np.sum(tp_costs <= optimal_threshold)}/{len(tp_costs)}\")\n",
    "    print(f\"FP au-dessus du seuil: {np.sum(fp_costs > optimal_threshold)}/{len(fp_costs)}\")"
   ],
   "id": "f6c1b0e6ff2eea73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Find best quality score",
   "id": "57dca8a4bad7e095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_residuals(result):\n",
    "    \"\"\"Analyse la qualité des résidus pour évaluer la pertinence de l'association\"\"\"\n",
    "    residuals = result.fun\n",
    "    num_obs = len(residuals)\n",
    "    chi2_stat, p_value, passes, dof = SOUND_MODEL.test_chi_square(result)\n",
    "    if num_obs == 0:\n",
    "        print(\"error\")\n",
    "    # Chi-carré réduit\n",
    "    chi2_reduced = np.sum(residuals**2) / dof\n",
    "    # Z-scroe des résidus (résidus standardisés)\n",
    "    residual_std = np.std(residuals) if np.std(residuals) > 1e-10 else 1e-10\n",
    "    z_scores = np.abs(residuals) / residual_std\n",
    "    max_zscore = np.max(z_scores)\n",
    "    outliers = SOUND_MODEL.detect_outliers(result,method='absolute')\n",
    "    outliers_score = len(outliers['outlier_indices'])\n",
    "    # Fraction d'outliers (|z-score| > 2.5)\n",
    "    outlier_fraction = np.mean(z_scores >1.5)\n",
    "\n",
    "    # Test unitaire de variance\n",
    "    p = 2  # nombre de paramètres ajustés (ici a et b)\n",
    "    sigma_theorique = 3\n",
    "    s2 = np.var(residuals, ddof=p)  # variance sans biais\n",
    "    # print(f\"Variance empirique : {s2:.3f}\")\n",
    "    # Test de normalité des résidus (Shapiro-Wilk si n < 5000, sinon Kolmogorov-Smirnov)\n",
    "    if num_obs >= 3 and num_obs < 5000:\n",
    "         _, normality_pvalue = stats.shapiro(residuals)\n",
    "    # Score de qualité composite (plus c'est haut, mieux c'est)\n",
    "    quality_score = 1.0 / (1.0 + chi2_reduced)  # Pénalise chi2 élevé\n",
    "    quality_score *= (1.0 - outlier_fraction)    # Pénalise les outliers\n",
    "    quality_score *= 1.0 / (1.0 + max_zscore/10.0)  # Pénalise les résidus extrêmes\n",
    "    return {\n",
    "        'chi2_reduced': float(chi2_reduced),\n",
    "        'max_residual_zscore': float(max_zscore),\n",
    "        'outlier_fraction': float(outlier_fraction),\n",
    "        'normality_pvalue': float(normality_pvalue),\n",
    "        'quality_score': float(quality_score),\n",
    "        'cost':float(result.cost),\n",
    "        'chi2_stat': float(chi2_stat),\n",
    "        'p_value': float(p_value),\n",
    "        'passes': bool(passes),\n",
    "        \"outliers_score\" : outliers_score\n",
    "    }"
   ],
   "id": "3f51399a6e31c75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "for i in final_results:\n",
    "    res = final_results[i]\n",
    "    metrics = analyze_residuals(res)  # ton analyse des résidus\n",
    "\n",
    "    rows.append({\n",
    "        \"lat\": res.x[1],\n",
    "        \"lon\": res.x[2],\n",
    "        \"origin_time\": res.x[0],\n",
    "        \"cost\": res.cost,\n",
    "        \"chi2_reduced\": metrics[\"chi2_reduced\"],\n",
    "        \"outlier_fraction\": metrics[\"outlier_fraction\"],\n",
    "        \"max_zscore\": metrics[\"max_residual_zscore\"],\n",
    "        \"normality_pvalue\": metrics[\"normality_pvalue\"],\n",
    "        \"quality_score\": metrics[\"quality_score\"],\n",
    "        'chi2_stat' : metrics[\"chi2_stat\"],\n",
    "        'p_value': metrics[\"p_value\"],\n",
    "        'passes': metrics[\"passes\"],\n",
    "        \"outliers_score\" : metrics[\"outliers_score\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"origin_time\"] = pd.to_datetime(df[\"origin_time\"], unit=\"s\", utc=True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"origin_time\"] = pd.to_datetime(df[\"origin_time\"], unit=\"s\",utc=True)\n",
    "\n",
    "try :\n",
    "    ground_truth['origin_time'] = pd.to_datetime(ground_truth['origin_time'])\n",
    "    ground_truth['origin_time'] = ground_truth['origin_time'].dt.tz_localize('UTC')\n",
    "except :\n",
    "        ground_truth['origin_time'] = pd.to_datetime(ground_truth['origin_time'])\n",
    "        ground_truth['origin_time'] = ground_truth['origin_time'].dt.tz_convert('UTC')\n",
    "\n",
    "matches = []\n",
    "\n",
    "time_threshold = pd.Timedelta(seconds=1*60)\n",
    "latlon_threshold = 1 # en degrés (à adapter !)\n",
    "df[\"is_match\"] = False\n",
    "for i, row_res in df.iterrows():\n",
    "    for j, row_gt in ground_truth.iterrows():\n",
    "        # condition temporelle\n",
    "        time_close = abs(row_res[\"origin_time\"] - row_gt[\"origin_time\"]) <= time_threshold\n",
    "\n",
    "        # condition spatiale (ici simple différence en degrés)\n",
    "        lat_close = abs(row_res[\"lat\"] - row_gt[\"lat\"]) <= latlon_threshold\n",
    "        lon_close = abs(row_res[\"lon\"] - row_gt[\"lon\"]) <= latlon_threshold\n",
    "\n",
    "        if time_close and lat_close and lon_close:\n",
    "            matches.append((i, j))\n",
    "            df.at[i, \"is_match\"] = True\n",
    "\n",
    "print(len(matches))"
   ],
   "id": "b2c1f5edd75487b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "['cost', 'chi2_reduced', 'outlier_fraction','max_zscore', 'normality_pvalue', 'quality_score', 'chi2_stat','p_value', 'passes', 'outliers_score', 'is_match']",
   "id": "35ddd2ee5f05551c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "benchmark = {}\n",
    "\n",
    "for col in ['cost', 'chi2_reduced', 'outlier_fraction','max_zscore', 'normality_pvalue', 'quality_score', 'chi2_stat','p_value', 'passes', 'outliers_score', 'is_match']:\n",
    "    try:\n",
    "        auc = roc_auc_score(df[\"is_match\"], -df[col] if col in [\"cost\",\"chi2_reduced\", \"outlier_fraction\", \"max_zscore\",'chi2_stat','outliers_score'] else df[col])\n",
    "        benchmark[col] = auc\n",
    "        benchmark[col] = auc\n",
    "    except ValueError as e:\n",
    "        print(\"error\", e)\n",
    "        benchmark[col] = None\n",
    "\n",
    "print(benchmark)\n"
   ],
   "id": "188a0a0600842b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for col in ['cost', 'chi2_reduced', 'outlier_fraction','max_zscore', 'normality_pvalue', 'quality_score', 'chi2_stat','p_value', 'passes', 'outliers_score']:\n",
    "    plt.figure()\n",
    "    sns.histplot(data=df, x=col, hue=\"is_match\", kde=True, stat=\"density\", common_norm=False)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ],
   "id": "ebd916132e08c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_results",
   "id": "b2b61c023cb575af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a77cd26328581575",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
