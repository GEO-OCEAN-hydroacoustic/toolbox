{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from utils.physics.sound_model.spherical_sound_model import GridEllipsoidalSoundModel\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "import os\n",
    "import utils.physics.sound_model.ISAS_grid as isg\n",
    "import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2020\"\n",
    "DETECTIONS_DIR = \"/media/rsafran/CORSAIR/detections_CTBT/\"\n",
    "lat_bounds = [-60, 5]\n",
    "lon_bounds = [35, 120]\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "grid_size = 400\n",
    "# Define start and end points\n",
    "lat1, lon1 = -31.5758,83.2423    # Example: Station MADE\n",
    "lat2, lon2 = -59.99254334995582,35.00354027003104  # Example: Station NEAMS\n",
    "depth = 1200    # Depth in meters\n",
    "\n",
    "method = 'min'\n",
    "year = '2018'\n",
    "PATH = f\"/media/rsafran/CORSAIR/ISAS/86442/field/{year}\"\n",
    "out_dir = f\"/media/rsafran/CORSAIR/ISAS/extracted/{year}/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "result = isg.load_ISAS_extracted(out_dir, 4)\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/extracted/2018\"\n",
    "arr = os.listdir(ISAS_PATH)\n",
    "file_list = [os.path.join(ISAS_PATH, fname) for fname in arr if fname.endswith('.nc')]\n",
    "SOUND_MODEL = GridEllipsoidalSoundModel(file_list)\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "if False:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    det_files = [f for f in det_files if \"2018\" in f ]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    # DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/refined_detections_merged.npy\", allow_pickle=True)\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e455940b714fb43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "def compute_latitude_line(lat_idx, lat, pts_lon, stations, sound_model, date):\n",
    "    \"\"\"\n",
    "    Calcule tous les temps de trajet pour une ligne de latitude donnée\n",
    "    Cette fonction sera exécutée en parallèle par chaque processus\n",
    "    \"\"\"\n",
    "    line_results = {}\n",
    "\n",
    "    # Pré-calcul des positions des stations pour éviter les appels répétés\n",
    "    station_positions = {s: s.get_pos() for s in stations}\n",
    "\n",
    "    for s in stations:\n",
    "        station_pos = station_positions[s]\n",
    "        line_times = np.zeros(len(pts_lon))\n",
    "\n",
    "        # Calcul vectorisé pour tous les points de longitude de cette latitude\n",
    "        for lon_idx, lon in enumerate(pts_lon):\n",
    "            line_times[lon_idx] = sound_model.get_sound_travel_time(\n",
    "                [lat, lon], station_pos, date=date\n",
    "            )\n",
    "\n",
    "        line_results[s] = line_times\n",
    "\n",
    "    return lat_idx, line_results\n",
    "\n",
    "def compute_grids_by_latitude(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                             pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None, irregular_points=None):\n",
    "    \"\"\"\n",
    "    Version optimisée qui traite ligne de latitude par ligne de latitude\n",
    "    avec multiprocessing pour une performance maximale\n",
    "    \"\"\"\n",
    "    if irregular_points is None:\n",
    "        pts_lat = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "        pts_lon = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "    else :\n",
    "        pts_lat = irregular_points[:,0]\n",
    "        pts_lon = irregular_points[:,1]\n",
    "\n",
    "    grid_max_res_time = (0.5 * np.sqrt(2) * (pts_lat[1] - pts_lat[0]) * 111_000) / (\n",
    "                sound_model.constant_velocity - sound_speed_uncertainty)\n",
    "    grid_tolerance = grid_max_res_time + pick_uncertainty\n",
    "    print(f\"Grid tolerance of {grid_tolerance:.2f}s\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = min(len(pts_lat), mp.cpu_count())\n",
    "\n",
    "    print(f\"Processing {len(pts_lat)} latitude lines using {n_workers} workers\")\n",
    "\n",
    "    # Date fixe pour tous les calculs\n",
    "    calc_date = datetime.datetime(year=2020, month=1, day=1)\n",
    "\n",
    "    # Initialisation des structures de données\n",
    "    grid_station_travel_time = {s: np.zeros((len(pts_lat), len(pts_lon))) for s in stations}\n",
    "\n",
    "    # Préparation de la fonction pour multiprocessing\n",
    "    compute_line_func = partial(\n",
    "        compute_latitude_line,\n",
    "        pts_lon=pts_lon,\n",
    "        stations=stations,\n",
    "        sound_model=sound_model,\n",
    "        date=calc_date\n",
    "    )\n",
    "\n",
    "    # Traitement parallèle ligne par ligne\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Soumettre toutes les tâches\n",
    "        futures = []\n",
    "        for lat_idx, lat in enumerate(pts_lat):\n",
    "            future = executor.submit(compute_line_func, lat_idx, lat)\n",
    "            futures.append(future)\n",
    "\n",
    "        # Récupération des résultats avec barre de progression\n",
    "        completed = 0\n",
    "        for future in futures:\n",
    "            lat_idx, line_results = future.result()\n",
    "\n",
    "            # Assemblage des résultats dans la grille finale\n",
    "            for station in stations:\n",
    "                grid_station_travel_time[station][lat_idx, :] = line_results[station]\n",
    "\n",
    "            completed += 1\n",
    "            if completed % max(1, len(pts_lat) // 10) == 0:\n",
    "                print(f\"Completed {completed}/{len(pts_lat)} latitude lines ({100*completed/len(pts_lat):.1f}%)\")\n",
    "\n",
    "    print(\"Computing station couple travel times...\")\n",
    "    # Calcul vectorisé des différences de temps de trajet\n",
    "    grid_station_couple_travel_time = {}\n",
    "    for s in stations:\n",
    "        grid_station_couple_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            grid_station_couple_travel_time[s][s2] = (\n",
    "                grid_station_travel_time[s2] - grid_station_travel_time[s]\n",
    "            )\n",
    "\n",
    "    print(\"Computing station max travel times...\")\n",
    "    # Calcul des temps de trajet maximum entre stations\n",
    "    station_max_travel_time = {}\n",
    "    for s in stations:\n",
    "        station_max_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            station_max_travel_time[s][s2] = sound_model.get_sound_travel_time(\n",
    "                s.get_pos(), s2.get_pos(), date=calc_date\n",
    "            )\n",
    "\n",
    "    return (pts_lat, pts_lon, station_max_travel_time, grid_station_travel_time,\n",
    "            grid_station_couple_travel_time, grid_tolerance)\n",
    "\n",
    "\n",
    "def compute_grids_chunked_latitude(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                                  pick_uncertainty=5, sound_speed_uncertainty=2,\n",
    "                                  n_workers=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Version avec chunking adaptatif pour optimiser l'équilibrage de charge\n",
    "    Traite plusieurs lignes de latitude par chunk\n",
    "    \"\"\"\n",
    "    pts_lat = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "    pts_lon = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "\n",
    "    grid_max_res_time = (0.5 * np.sqrt(2) * (pts_lat[1] - pts_lat[0]) * 111_000) / (\n",
    "                sound_model.constant_velocity - sound_speed_uncertainty)\n",
    "    grid_tolerance = grid_max_res_time + pick_uncertainty\n",
    "    print(f\"Grid tolerance of {grid_tolerance:.2f}s\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = mp.cpu_count()\n",
    "\n",
    "    if chunk_size is None:\n",
    "        # Chunk size adaptatif basé sur le nombre de workers\n",
    "        chunk_size =  max(1, grid_size // (n_workers // 2))\n",
    "\n",
    "    print(f\"Processing {len(pts_lat)} latitude lines in chunks of {chunk_size} using {n_workers} workers\")\n",
    "\n",
    "    calc_date = datetime.datetime(year=2020, month=1, day=1)\n",
    "\n",
    "    def compute_latitude_chunk(lat_indices_chunk):\n",
    "        \"\"\"Traite un chunk de lignes de latitude\"\"\"\n",
    "        chunk_results = {}\n",
    "        station_positions = {s: s.get_pos() for s in stations}\n",
    "\n",
    "        for lat_idx in lat_indices_chunk:\n",
    "            lat = pts_lat[lat_idx]\n",
    "            chunk_results[lat_idx] = {}\n",
    "\n",
    "            for s in stations:\n",
    "                station_pos = station_positions[s]\n",
    "                line_times = np.zeros(len(pts_lon))\n",
    "\n",
    "                for lon_idx, lon in enumerate(pts_lon):\n",
    "                    line_times[lon_idx] = sound_model.get_sound_travel_time(\n",
    "                        [lat, lon], station_pos, date=calc_date\n",
    "                    )\n",
    "\n",
    "                chunk_results[lat_idx][s] = line_times\n",
    "\n",
    "        return chunk_results\n",
    "\n",
    "    # Création des chunks\n",
    "    lat_indices = list(range(len(pts_lat)))\n",
    "    chunks = [lat_indices[i:i + chunk_size] for i in range(0, len(lat_indices), chunk_size)]\n",
    "\n",
    "    # Initialisation des structures de données\n",
    "    grid_station_travel_time = {s: np.zeros((len(pts_lat), len(pts_lon))) for s in stations}\n",
    "\n",
    "    # Traitement parallèle par chunks\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_futures = [executor.submit(compute_latitude_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "        completed_chunks = 0\n",
    "        for future in chunk_futures:\n",
    "            chunk_results = future.result()\n",
    "\n",
    "            # Assemblage des résultats\n",
    "            for lat_idx, lat_results in chunk_results.items():\n",
    "                for station, line_times in lat_results.items():\n",
    "                    grid_station_travel_time[station][lat_idx, :] = line_times\n",
    "\n",
    "            completed_chunks += 1\n",
    "            completed_lines = completed_chunks * chunk_size\n",
    "            print(f\"Completed ~{min(completed_lines, len(pts_lat))}/{len(pts_lat)} latitude lines \"\n",
    "                  f\"({100*min(completed_lines, len(pts_lat))/len(pts_lat):.1f}%)\")\n",
    "\n",
    "    # Calcul des différences et temps max (identique à la version précédente)\n",
    "    print(\"Computing station couple travel times...\")\n",
    "    grid_station_couple_travel_time = {}\n",
    "    for s in stations:\n",
    "        grid_station_couple_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            grid_station_couple_travel_time[s][s2] = (\n",
    "                grid_station_travel_time[s2] - grid_station_travel_time[s]\n",
    "            )\n",
    "\n",
    "    print(\"Computing station max travel times...\")\n",
    "    station_max_travel_time = {}\n",
    "    for s in stations:\n",
    "        station_max_travel_time[s] = {}\n",
    "        for s2 in stations:\n",
    "            station_max_travel_time[s][s2] = sound_model.get_sound_travel_time(\n",
    "                s.get_pos(), s2.get_pos(), date=calc_date\n",
    "            )\n",
    "\n",
    "    return (pts_lat, pts_lon, station_max_travel_time, grid_station_travel_time,\n",
    "            grid_station_couple_travel_time, grid_tolerance)\n",
    "\n",
    "\n",
    "# Version avec monitoring de performance\n",
    "def compute_grids_monitored(lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "                           pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None,irregular_points=None):\n",
    "    \"\"\"\n",
    "    Version avec monitoring détaillé des performances\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = compute_grids_by_latitude(\n",
    "        lat_bounds, lon_bounds, grid_size, sound_model, stations,\n",
    "        pick_uncertainty, sound_speed_uncertainty, n_workers, irregular_points=irregular_points\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_calculations = grid_size * grid_size * len(stations)\n",
    "\n",
    "    print(f\"\\n=== Performance Report ===\")\n",
    "    print(f\"Total execution time: {total_time:.2f}s\")\n",
    "    print(f\"Total calculations: {total_calculations:,}\")\n",
    "    print(f\"Calculations per second: {total_calculations/total_time:,.0f}\")\n",
    "    print(f\"Grid size: {grid_size}x{grid_size}\")\n",
    "    print(f\"Number of stations: {len(stations)}\")\n",
    "    print(f\"Workers used: {n_workers or mp.cpu_count()}\")\n",
    "\n",
    "    return result"
   ],
   "id": "2f5b8c1fd6b779d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compute_grids_monitored(lat_bounds, lon_bounds, grid_size, SOUND_MODEL, STATIONS,\n",
    "                           pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None)"
   ],
   "id": "247b45c27c231b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for s in STATIONS :\n",
    "    print(s.name, s.get_pos())"
   ],
   "id": "2b5fd16562def058",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dorsal_db = \"/home/rsafran/Documents/Database_geo/\"\n",
    "dorsal_db_f = os.listdir(dorsal_db)\n",
    "df = [pd.read_csv(dorsal_db + f, comment=\">\",sep='\\s+', names=[\"lat\",\"lon\",\"n\"])for f in dorsal_db_f]"
   ],
   "id": "e25b18039309738c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]], crs=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "for i in range(len(df)):\n",
    "    plt.plot(df[i].lat, df[i].lon, transform=ccrs.PlateCarree())\n"
   ],
   "id": "a78892b46f1e48f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:44:47.100880Z",
     "start_time": "2025-09-04T09:44:46.999530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "def load_ridge_data(dorsal_db_path):\n",
    "    \"\"\"\n",
    "    Charge les données des dorsales océaniques\n",
    "    \"\"\"\n",
    "    dorsal_files = [f for f in os.listdir(dorsal_db_path) if f.endswith('.xy')]\n",
    "    print(f\"Loading {len(dorsal_files)} ridge files: {dorsal_files}\")\n",
    "\n",
    "    ridge_data = {}\n",
    "    all_ridge_points = []\n",
    "\n",
    "    for f in dorsal_files:\n",
    "        ridge_name = f.replace('axe-', '').replace('-tout.xy', '')\n",
    "        df = pd.read_csv(os.path.join(dorsal_db_path, f),\n",
    "                        comment=\">\", sep=r'\\s+', names=[\"lon\", \"lat\", \"n\"])\n",
    "\n",
    "        # Nettoyage des données\n",
    "        # df = df.dropna()\n",
    "        ridge_points = df[['lat', 'lon']].values\n",
    "\n",
    "        ridge_data[ridge_name] = ridge_points\n",
    "        all_ridge_points.append(ridge_points)\n",
    "\n",
    "        print(f\"  {ridge_name}: {len(ridge_points)} points\")\n",
    "\n",
    "    # Combinaison de toutes les dorsales\n",
    "    all_ridge_points = np.vstack(all_ridge_points)\n",
    "    print(f\"Total ridge points: {len(all_ridge_points)}\")\n",
    "\n",
    "    return ridge_data, all_ridge_points\n",
    "\n",
    "def create_regular_grid(lat_bounds, lon_bounds, grid_size):\n",
    "    \"\"\"\n",
    "    Crée une grille régulière de base\n",
    "    \"\"\"\n",
    "    lats = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "    lons = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "\n",
    "    lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
    "    grid_points = np.column_stack([lat_grid.ravel(), lon_grid.ravel()])\n",
    "\n",
    "    return grid_points, lats, lons\n",
    "\n",
    "def filter_points_near_ridges_chunk(grid_chunk, ridge_points, max_distance_deg):\n",
    "    \"\"\"\n",
    "    Filtre un chunk de points de grille selon la distance aux dorsales\n",
    "    \"\"\"\n",
    "    if len(grid_chunk) == 0:\n",
    "        return []\n",
    "\n",
    "    # Calcul des distances (approximation euclidienne rapide)\n",
    "    distances = cdist(grid_chunk, ridge_points, metric='euclidean')\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    # Points à conserver (distance < seuil)\n",
    "    valid_mask = min_distances <= max_distance_deg\n",
    "    valid_points = grid_chunk[valid_mask]\n",
    "\n",
    "    return valid_points.tolist()\n",
    "\n",
    "def create_ridge_based_grid(lat_bounds, lon_bounds, grid_size, ridge_points,\n",
    "                           max_distance_deg=4.0, n_workers=None):\n",
    "    \"\"\"\n",
    "    Crée une grille irrégulière basée sur la proximité des dorsales océaniques\n",
    "\n",
    "    Args:\n",
    "        lat_bounds: [lat_min, lat_max]\n",
    "        lon_bounds: [lon_min, lon_max]\n",
    "        grid_size: taille de la grille régulière de base\n",
    "        ridge_points: array numpy des points de dorsales [lat, lon]\n",
    "        max_distance_deg: distance maximum en degrés des dorsales\n",
    "        n_workers: nombre de workers pour le parallélisme\n",
    "\n",
    "    Returns:\n",
    "        irregular_points: array numpy des points sélectionnés\n",
    "    \"\"\"\n",
    "    print(f\"Creating ridge-based grid:\")\n",
    "    print(f\"  Bounds: lat {lat_bounds}, lon {lon_bounds}\")\n",
    "    print(f\"  Grid size: {grid_size}x{grid_size}\")\n",
    "    print(f\"  Max distance from ridges: {max_distance_deg}°\")\n",
    "\n",
    "    # Création de la grille régulière de base\n",
    "    grid_points, _, _ = create_regular_grid(lat_bounds, lon_bounds, grid_size)\n",
    "    print(f\"  Initial regular grid: {len(grid_points)} points\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = mp.cpu_count()\n",
    "\n",
    "    # Chunking pour le parallélisme\n",
    "    chunk_size = max(1000, len(grid_points) // (n_workers * 4))\n",
    "    chunks = [grid_points[i:i + chunk_size] for i in range(0, len(grid_points), chunk_size)]\n",
    "\n",
    "    print(f\"  Processing {len(chunks)} chunks using {n_workers} workers...\")\n",
    "\n",
    "    # Filtrage parallèle\n",
    "    filter_func = partial(filter_points_near_ridges_chunk,\n",
    "                         ridge_points=ridge_points,\n",
    "                         max_distance_deg=max_distance_deg)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_results = list(executor.map(filter_func, chunks))\n",
    "\n",
    "    # Combinaison des résultats\n",
    "    irregular_points = []\n",
    "    for chunk_result in chunk_results:\n",
    "        irregular_points.extend(chunk_result)\n",
    "\n",
    "    irregular_points = np.array(irregular_points)\n",
    "\n",
    "    reduction_factor = len(irregular_points) / len(grid_points) * 100\n",
    "    print(f\"  Final irregular grid: {len(irregular_points)} points ({reduction_factor:.1f}% of original)\")\n",
    "\n",
    "    return irregular_points\n",
    "\n",
    "\n",
    "def visualize_ridge_grid(ridge_data, irregular_points, lat_bounds, lon_bounds,\n",
    "                        max_distance_deg=4.0, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Visualise la grille irrégulière par rapport aux dorsales\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "    # Plot des dorsales\n",
    "    for i, (ridge_name, ridge_points) in enumerate(ridge_data.items()):\n",
    "        plt.scatter(ridge_points[:, 1], ridge_points[:, 0],\n",
    "                   c=colors[i % len(colors)], s=1, alpha=0.7,\n",
    "                   label=f'{ridge_name} ridge')\n",
    "\n",
    "    # Plot de la grille irrégulière\n",
    "    plt.scatter(irregular_points[:, 1], irregular_points[:, 0],\n",
    "               c='black', s=0.5, alpha=0.3, label='Grid points')\n",
    "\n",
    "    plt.xlim(lon_bounds)\n",
    "    plt.ylim(lat_bounds)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'Ridge-based irregular grid (≤{max_distance_deg}° from ridges)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    return plt.gcf()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Exemple complet d'utilisation\n",
    "    \"\"\"\n",
    "    # Paramètres\n",
    "    dorsal_db = \"/home/rsafran/Documents/Database_geo/\"\n",
    "    lat_bounds = [-60, 5]\n",
    "    lon_bounds = [35, 120]\n",
    "    grid_size = 400  # Grille régulière de base\n",
    "    max_distance_deg = 4.0\n",
    "\n",
    "    # Chargement des dorsales\n",
    "    ridge_data, all_ridge_points = load_ridge_data(dorsal_db)\n",
    "\n",
    "    # Génération de la grille irrégulière\n",
    "    irregular_points = create_ridge_based_grid(\n",
    "        lat_bounds, lon_bounds, grid_size, all_ridge_points, max_distance_deg\n",
    "    )\n",
    "\n",
    "    # Visualisation (optionnel)\n",
    "    fig = visualize_ridge_grid(ridge_data, irregular_points, lat_bounds, lon_bounds, max_distance_deg)\n",
    "    plt.show()\n",
    "\n",
    "    # Calcul complet avec votre sound_model et stations\n",
    "    # result = compute_grids_monitored(lat_bounds, lon_bounds, grid_size, SOUND_MODEL, STATIONS,\n",
    "    #                        pick_uncertainty=5, sound_speed_uncertainty=2, n_workers=None, irregular_points=irregular_points)\n",
    "\n",
    "    return irregular_points, ridge_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    irregular_points, ridge_data = example_usage()"
   ],
   "id": "b803977972b16496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3 ridge files: ['axe-CIR-tout.xy', 'axe-SWIR-tout.xy', 'axe-SEIR-tout.xy']\n",
      "  CIR: 907 points\n",
      "  SWIR: 693 points\n",
      "  SEIR: 4242 points\n",
      "Total ridge points: 5842\n",
      "Creating ridge-based grid:\n",
      "  Bounds: lat [-60, 5], lon [35, 120]\n",
      "  Grid size: 400x400\n",
      "  Max distance from ridges: 4.0°\n",
      "  Initial regular grid: 160000 points\n",
      "  Processing 113 chunks using 28 workers...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 184\u001B[39m\n\u001B[32m    181\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m irregular_points, ridge_data\n\u001B[32m    183\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     irregular_points, ridge_data = \u001B[43mexample_usage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 169\u001B[39m, in \u001B[36mexample_usage\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    166\u001B[39m ridge_data, all_ridge_points = load_ridge_data(dorsal_db)\n\u001B[32m    168\u001B[39m \u001B[38;5;66;03m# Génération de la grille irrégulière\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m169\u001B[39m irregular_points = \u001B[43mcreate_ridge_based_grid\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlat_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlon_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_ridge_points\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_distance_deg\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[38;5;66;03m# Visualisation (optionnel)\u001B[39;00m\n\u001B[32m    174\u001B[39m fig = visualize_ridge_grid(ridge_data, irregular_points, lat_bounds, lon_bounds, max_distance_deg)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 109\u001B[39m, in \u001B[36mcreate_ridge_based_grid\u001B[39m\u001B[34m(lat_bounds, lon_bounds, grid_size, ridge_points, max_distance_deg, n_workers)\u001B[39m\n\u001B[32m    104\u001B[39m filter_func = partial(filter_points_near_ridges_chunk,\n\u001B[32m    105\u001B[39m                      ridge_points=ridge_points,\n\u001B[32m    106\u001B[39m                      max_distance_deg=max_distance_deg)\n\u001B[32m    108\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ProcessPoolExecutor(max_workers=n_workers) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[32m--> \u001B[39m\u001B[32m109\u001B[39m     chunk_results = \u001B[38;5;28mlist\u001B[39m(\u001B[43mexecutor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilter_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m# Combinaison des résultats\u001B[39;00m\n\u001B[32m    112\u001B[39m irregular_points = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/process.py:864\u001B[39m, in \u001B[36mProcessPoolExecutor.map\u001B[39m\u001B[34m(self, fn, timeout, chunksize, *iterables)\u001B[39m\n\u001B[32m    861\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize < \u001B[32m1\u001B[39m:\n\u001B[32m    862\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mchunksize must be >= 1.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m864\u001B[39m results = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_process_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    865\u001B[39m \u001B[43m                      \u001B[49m\u001B[43m_get_chunks\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43miterables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    866\u001B[39m \u001B[43m                      \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _chain_from_iterable_of_lists(results)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/_base.py:608\u001B[39m, in \u001B[36mExecutor.map\u001B[39m\u001B[34m(self, fn, timeout, chunksize, *iterables)\u001B[39m\n\u001B[32m    605\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    606\u001B[39m     end_time = timeout + time.monotonic()\n\u001B[32m--> \u001B[39m\u001B[32m608\u001B[39m fs = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(*iterables)]\n\u001B[32m    610\u001B[39m \u001B[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001B[39;00m\n\u001B[32m    611\u001B[39m \u001B[38;5;66;03m# before the first iterator value is required.\u001B[39;00m\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresult_iterator\u001B[39m():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/process.py:836\u001B[39m, in \u001B[36mProcessPoolExecutor.submit\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    834\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._safe_to_dynamically_spawn_children:\n\u001B[32m    835\u001B[39m     \u001B[38;5;28mself\u001B[39m._adjust_process_count()\n\u001B[32m--> \u001B[39m\u001B[32m836\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_start_executor_manager_thread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    837\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m f\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/process.py:775\u001B[39m, in \u001B[36mProcessPoolExecutor._start_executor_manager_thread\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._executor_manager_thread \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    773\u001B[39m     \u001B[38;5;66;03m# Start the processes so that their sentinels are known.\u001B[39;00m\n\u001B[32m    774\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._safe_to_dynamically_spawn_children:  \u001B[38;5;66;03m# ie, using fork.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m775\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_launch_processes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    776\u001B[39m     \u001B[38;5;28mself\u001B[39m._executor_manager_thread = _ExecutorManagerThread(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    777\u001B[39m     \u001B[38;5;28mself\u001B[39m._executor_manager_thread.start()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/process.py:802\u001B[39m, in \u001B[36mProcessPoolExecutor._launch_processes\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    798\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._executor_manager_thread, (\n\u001B[32m    799\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mProcesses cannot be fork()ed after the thread has started, \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    800\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mdeadlock in the child processes could result.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._processes), \u001B[38;5;28mself\u001B[39m._max_workers):\n\u001B[32m--> \u001B[39m\u001B[32m802\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_spawn_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/concurrent/futures/process.py:812\u001B[39m, in \u001B[36mProcessPoolExecutor._spawn_process\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    804\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_spawn_process\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    805\u001B[39m     p = \u001B[38;5;28mself\u001B[39m._mp_context.Process(\n\u001B[32m    806\u001B[39m         target=_process_worker,\n\u001B[32m    807\u001B[39m         args=(\u001B[38;5;28mself\u001B[39m._call_queue,\n\u001B[32m   (...)\u001B[39m\u001B[32m    810\u001B[39m               \u001B[38;5;28mself\u001B[39m._initargs,\n\u001B[32m    811\u001B[39m               \u001B[38;5;28mself\u001B[39m._max_tasks_per_child))\n\u001B[32m--> \u001B[39m\u001B[32m812\u001B[39m     \u001B[43mp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    813\u001B[39m     \u001B[38;5;28mself\u001B[39m._processes[p.pid] = p\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/multiprocessing/process.py:121\u001B[39m, in \u001B[36mBaseProcess.start\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process._config.get(\u001B[33m'\u001B[39m\u001B[33mdaemon\u001B[39m\u001B[33m'\u001B[39m), \\\n\u001B[32m    119\u001B[39m        \u001B[33m'\u001B[39m\u001B[33mdaemonic processes are not allowed to have children\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    120\u001B[39m _cleanup()\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m \u001B[38;5;28mself\u001B[39m._popen = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[38;5;28mself\u001B[39m._sentinel = \u001B[38;5;28mself\u001B[39m._popen.sentinel\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/multiprocessing/context.py:282\u001B[39m, in \u001B[36mForkProcess._Popen\u001B[39m\u001B[34m(process_obj)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    280\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_Popen\u001B[39m(process_obj):\n\u001B[32m    281\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpopen_fork\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/multiprocessing/popen_fork.py:19\u001B[39m, in \u001B[36mPopen.__init__\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mself\u001B[39m.returncode = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;28mself\u001B[39m.finalizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/multiprocessing/popen_fork.py:66\u001B[39m, in \u001B[36mPopen._launch\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     64\u001B[39m parent_r, child_w = os.pipe()\n\u001B[32m     65\u001B[39m child_r, parent_w = os.pipe()\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m \u001B[38;5;28mself\u001B[39m.pid = \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfork\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pid == \u001B[32m0\u001B[39m:\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mOSError\u001B[39m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "irregular_points",
   "id": "b4264cb00d658020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f87161a5e48b5f0d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
