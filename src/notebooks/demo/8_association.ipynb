{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook aims at associating detections to compute possible source positions on a grid.",
   "id": "f32399650858950a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.detection.association import load_detections\n",
    "from utils.detection.association import compute_grids\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association import compute_candidates, association_is_new, update_valid_grid, update_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# paths\n",
    "CATALOG_PATH = \"../../../data/demo\"\n",
    "DETECTIONS_DIR = \"../../../data/detection/TiSSNet/demo\"\n",
    "ASSOCIATION_OUTPUT_DIR = \"../../../data/detection/association\"\n",
    "\n",
    "# Detections loading parameters\n",
    "RELOAD_DETECTIONS = True # if False, load files called \"detections.npy\" and \"detections_merged.npy\" containing everything instead of the raw detection output. Leave at True by default\n",
    "MIN_P_TISSNET_PRIMARY = 0.5  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.1  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 5 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "REQ_CLOSEST_STATIONS = 0  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "\n",
    "# sound model definition\n",
    "SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)\n",
    "\n",
    "# association running parameters\n",
    "RUN_ASSOCIATION = True # set to False to load previous associations without processing it again\n",
    "SAVE_PATH_ROOT = None  # change this to save the grids as figures, leave at None by default\n",
    "NCPUS = 6  # nb of CPUs used"
   ],
   "id": "dd5df7fb2321b1b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Detections",
   "id": "4b94ddbe1b466510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "if RELOAD_DETECTIONS:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "id": "539e47a3426aaa13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compute grids",
   "id": "e05c8fbff4b619c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LAT_BOUNDS = [-13.4, -12.4]\n",
    "LON_BOUNDS = [45.2, 46.2]\n",
    "GRID_SIZE = 50  # number of points along each axis\n",
    "\n",
    "(PTS_LAT, PTS_LON, STATION_MAX_TRAVEL_TIME, GRID_STATION_TRAVEL_TIME,\n",
    " GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE) = compute_grids(LAT_BOUNDS, LON_BOUNDS, GRID_SIZE, SOUND_MODEL, STATIONS, pick_uncertainty=5, sound_speed_uncertainty=2)"
   ],
   "id": "e292ad978c79cfa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now associate",
   "id": "5cc85beda7820d0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"starting association\")\n",
    "\n",
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "\n",
    "association_hashlist = set()\n",
    "associations = {}\n",
    "\n",
    "def process_detection(arg):\n",
    "    detection, local_association_hashlist = arg\n",
    "    local_association = {}\n",
    "    date1, p1, s1 = detection\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1 + datetime.timedelta(seconds=4*GRID_TOLERANCE) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - datetime.timedelta(seconds=4*GRID_TOLERANCE) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([STATION_MAX_TRAVEL_TIME[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:date1}\n",
    "    candidates =  compute_candidates(other_stations, current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    if len(other_stations) < 2:\n",
    "        return local_association, local_association_hashlist\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "        for idx in candidates[station]:\n",
    "            date, p = DETECTIONS[station][idx]\n",
    "            if not association_is_new(current_association, date, local_association_hashlist):\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE, save_path, LON_BOUNDS, LAT_BOUNDS)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date)\n",
    "\n",
    "                if len(current_association) > 2:\n",
    "                    update_results(date1, current_association, valid_points_new, local_association, GRID_STATION_COUPLE_TRAVEL_TIME)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, save_path)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations, save_path)\n",
    "        return\n",
    "    backtrack(0, current_association, None, associations, save_path=save_path)\n",
    "    return local_association, local_association_hashlist\n",
    "\n",
    "# main part\n",
    "if RUN_ASSOCIATION:\n",
    "    try:\n",
    "        with ProcessPoolExecutor(NCPUS) as executor:\n",
    "            futures = {executor.submit(process_detection, (det, association_hashlist)): det for det in DETECTIONS_MERGED}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                local_association, local_association_hashlist = future.result()\n",
    "                association_hashlist = association_hashlist.union(local_association_hashlist)\n",
    "                associations = associations | local_association\n",
    "    finally:\n",
    "        # save the associations no matter if the execution stopped properly\n",
    "        np.save(OUT_FILE, associations)"
   ],
   "id": "7c2e8523d7786cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Take a look at the results",
   "id": "5f518aeb1a216c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 3\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) < MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1])\n",
    "im = plt.imshow(valid[::-1], aspect=1, cmap=\"inferno\", extent=extent, interpolation=None)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "for s in STATIONS:\n",
    "    p = s.get_pos()\n",
    "\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    plt.plot(p[1], p[0], 'wx', alpha=0.75)\n",
    "    plt.annotate(s.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/15, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/100), textcoords=\"data\", color='w', alpha=0.9)"
   ],
   "id": "e5e541884cf46538",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
