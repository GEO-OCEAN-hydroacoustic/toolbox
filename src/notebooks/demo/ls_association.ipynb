{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ls_association_with_clock_drift.py\n",
    "Seismic event localization with clock drift estimation.\n",
    "Handles stations with unknown clock drift by initially assigning larger uncertainties\n",
    "and then estimating the drift parameters after initial localization.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Geod\n",
    "from joblib import Parallel, delayed, Memory\n",
    "import time\n",
    "import psutil\n",
    "import pickle\n",
    "import warnings\n",
    "from utils.physics.sound_model.ellipsoidal_sound_model import GridEllipsoidalSoundModel\n",
    "# === CONFIGURATION ===\n",
    "ASSO_FILE = \"/media/rsafran/CORSAIR/Association/2018/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "OUTPUT_DIR = \"/media/rsafran/CORSAIR/Association/validated\"\n",
    "OUTPUT_BASENAME = \"s_-60-5,35-120,350,0.8,0.6_with_drift\"\n",
    "CHUNK_SIZE = 25  # checkpoint every N dates\n",
    "N_JOBS = max(1, 0*os.cpu_count() - 1)  # leave one core free\n",
    "GRID_LAT_BOUNDS = [-60, 5]\n",
    "GRID_LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/extracted/2018\"\n",
    "BATCH_SIZE = 5000\n",
    "#SOUND_MODEL\n",
    "arr = os.listdir(ISAS_PATH)\n",
    "file_list = [os.path.join(ISAS_PATH, fname) for fname in arr if fname.endswith('.nc')]\n",
    "SOUND_MODEL = GridEllipsoidalSoundModel(file_list)\n",
    "\n",
    "# === PERFORMANCE MONITORING ===\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def log_progress(message):\n",
    "    elapsed = time.time() - start_time\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    print(f\"[{elapsed:.1f}s | {memory_usage:.1f}MB] {message}\")\n",
    "\n",
    "\n",
    "# === INITIALIZATION ===\n",
    "\n",
    "# Precompute grid lat/lon\n",
    "PTS_LAT = np.linspace(*GRID_LAT_BOUNDS, GRID_SIZE)\n",
    "PTS_LON = np.linspace(*GRID_LON_BOUNDS, GRID_SIZE)\n",
    "\n",
    "# Geod instance for geodesic calculations\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def grid_index_to_coord(indices):\n",
    "    \"\"\"Convert grid indices to geographic coordinates\"\"\"\n",
    "    i, j = indices\n",
    "    return [PTS_LAT[i], PTS_LON[j]]\n",
    "\n",
    "def process_date(date, associations_list):\n",
    "    res = []\n",
    "    # Create simplified associations list to avoid serialization issues\n",
    "    simplified_associations = []\n",
    "    for detections, valid_points in associations_list:\n",
    "        simple_detections = []\n",
    "        for station_obj, det_time in detections:\n",
    "            # Extract only necessary data from station_obj\n",
    "            lat, lon = station_obj.get_pos()\n",
    "            drift = station_obj.get_clock_error(det_time) if \"not_ok\" in station_obj.other_kwargs.values() else 0\n",
    "            station_name = station_obj.name  # Get station name\n",
    "            simple_detections.append(((lat, lon), det_time, drift, station_name))\n",
    "        simplified_associations.append((simple_detections, valid_points))\n",
    "\n",
    "    for detections, valid_points in simplified_associations:\n",
    "        # Skip tiny clusters\n",
    "        if len(detections) < 7:\n",
    "            continue\n",
    "\n",
    "        # Build refined detections & station positions\n",
    "        station_positions = [pos for pos, _,_, _ in detections]\n",
    "        detection_times = [t for _, t,_, _ in detections]\n",
    "        drifts = [d for _,_,d, _ in detections]\n",
    "        print(np.array(valid_points))\n",
    "        c0 = np.mean(np.array(valid_points), axis=0)\n",
    "        print(c0)\n",
    "        r, _, _ = SOUND_MODEL.localize_with_uncertainties(\n",
    "            station_positions, detection_times, drift_uncertainties=drifts, initial_pos=c0\n",
    "        )\n",
    "        res.append(r)\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Main execution function\"\"\"\n",
    "log_progress(f\"Starting with {N_JOBS} workers\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load input\n",
    "log_progress(f\"Loading associations from {ASSO_FILE}\")\n",
    "associations = np.load(ASSO_FILE, allow_pickle=True).item()\n",
    "items = list(associations.items())\n",
    "total_items = len(items)\n",
    "log_progress(f\"Found {total_items} date entries to process\")\n",
    "\n",
    "# Process in batches with checkpoints\n",
    "validated_associations = {}\n",
    "\n",
    "for batch_start in range(0, total_items, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_items)\n",
    "    batch = items[batch_start:batch_end]\n",
    "\n",
    "    log_progress(f\"Processing batch {batch_start + 1}-{batch_end} of {total_items}\")\n",
    "\n",
    "    # Process batch in parallel\n",
    "    # Note: we use a smaller chunk_size when jobs > 1 for better load balancing\n",
    "    effective_chunk = 1 if N_JOBS > 1 else CHUNK_SIZE\n",
    "\n",
    "    results = Parallel(n_jobs=1, verbose=5, batch_size=effective_chunk)(\n",
    "        delayed(process_date)(date, lst) for date, lst in batch\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    i= 0\n",
    "    for res in results:\n",
    "        if res:  # Only store if we have validated results\n",
    "            validated_associations[i]\n",
    "            i+=1\n",
    "\n",
    "    # Checkpoint\n",
    "    chkpt_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        f\"{OUTPUT_BASENAME}_partial_{batch_end}.npy\"\n",
    "    )\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        np.save(chkpt_path, validated_associations)\n",
    "\n",
    "    log_progress(f\"Checkpoint saved: {chkpt_path}\")\n",
    "\n",
    "    # Memory optimization: save and reload larger checkpoints\n",
    "    if len(validated_associations) > 5000:\n",
    "        log_progress(\"Checkpointing and refreshing memory...\")\n",
    "        pickle_path = os.path.join(OUTPUT_DIR, \"temp_checkpoint.pkl\")\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(validated_associations, f)\n",
    "\n",
    "        # Clear and reload\n",
    "        validated_associations.clear()\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            validated_associations = pickle.load(f)\n",
    "\n",
    "        # Force garbage collection\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# Final save\n",
    "log_progress(\"Saving final results\")\n",
    "final_path = os.path.join(OUTPUT_DIR, f\"{OUTPUT_BASENAME}_final.npy\")\n",
    "np.save(final_path, validated_associations)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "log_progress(f\"All done! Final results saved to {final_path}\")\n",
    "log_progress(f\"Total execution time: {elapsed:.1f} seconds ({elapsed / 60:.1f} minutes)\")\n"
   ],
   "id": "d5715403d4cebf39",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
