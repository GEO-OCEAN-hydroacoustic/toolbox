{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initilaization",
   "id": "e846eb5a6b9675f9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# from notebooks.demo.Open_ISAS_Grid import travel_time\n",
    "#\n",
    "# from notebooks.demo.dev_asso_manual import model\n",
    "from utils.detection.association import load_detections\n",
    "from utils.detection.association import compute_grids\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association import compute_candidates, association_is_new, update_valid_grid, update_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# paths\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "# DETECTIONS_DIR = \"/media/rsafran/CORSAIR/temp/2018\"\n",
    "DETECTIONS_DIR = \"/home/rsafran/Bureau/tissnet/2018\"\n",
    "ASSOCIATION_OUTPUT_DIR = \"../../../data/detection/association\"\n",
    "\n",
    "# Detections loading parameters\n",
    "RELOAD_DETECTIONS = False # if False, load files called \"detections.npy\" and \"detections_merged.npy\" containing everything instead of the raw detection output. Leave at True by default\n",
    "MIN_P_TISSNET_PRIMARY = 0.8  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.6  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 10 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "REQ_CLOSEST_STATIONS = 0  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "\n",
    "# sound model definition\n",
    "SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)\n",
    "\n",
    "# association running parameters\n",
    "RUN_ASSOCIATION = True # set to False to load previous associations without processing it again\n",
    "SAVE_PATH_ROOT = None  # change this to save the grids as figures, leave at None by default\n",
    "NCPUS = 20  # nb of CPUs used"
   ],
   "id": "14941421b34ecb51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "if RELOAD_DETECTIONS:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "id": "6f1c99b9c852b9ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(vars(STATIONS[0]))\n",
    "print(STATIONS[0].depth)\n",
    "print(STATIONS[0].name)\n",
    "print(STATIONS[0].get_pos(include_depth=True))\n"
   ],
   "id": "f886cb92bff74eb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350  # number of points along each axis\n",
    "\n",
    "(PTS_LAT, PTS_LON, STATION_MAX_TRAVEL_TIME, GRID_STATION_TRAVEL_TIME,\n",
    " GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE) = compute_grids(LAT_BOUNDS, LON_BOUNDS, GRID_SIZE, SOUND_MODEL, STATIONS, pick_uncertainty=2, sound_speed_uncertainty=1)"
   ],
   "id": "6181d3b2e22452fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Association",
   "id": "5d38dc3a4e18eb74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"starting association\")\n",
    "\n",
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "\n",
    "association_hashlist = set()\n",
    "associations = {}\n",
    "\n",
    "def process_detection(arg):\n",
    "    detection, local_association_hashlist = arg\n",
    "    local_association = {}\n",
    "    date1, p1, s1 = detection\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1 + datetime.timedelta(seconds=4*GRID_TOLERANCE) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - datetime.timedelta(seconds=4*GRID_TOLERANCE) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([STATION_MAX_TRAVEL_TIME[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:date1}\n",
    "    candidates =  compute_candidates(other_stations, current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    if len(other_stations) < 2:\n",
    "        return local_association, local_association_hashlist\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "        for idx in candidates[station]:\n",
    "            date, p = DETECTIONS[station][idx]\n",
    "            if not association_is_new(current_association, date, local_association_hashlist):\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE, save_path, LON_BOUNDS, LAT_BOUNDS)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date)\n",
    "\n",
    "                if len(current_association) > 2:\n",
    "                    update_results(date1, current_association, valid_points_new, local_association, GRID_STATION_COUPLE_TRAVEL_TIME)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, save_path)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations, save_path)\n",
    "        return\n",
    "    backtrack(0, current_association, None, associations, save_path=save_path)\n",
    "    return local_association, local_association_hashlist\n",
    "\n",
    "# main part\n",
    "if RUN_ASSOCIATION:\n",
    "    try:\n",
    "        with ProcessPoolExecutor(NCPUS) as executor:\n",
    "            futures = {executor.submit(process_detection, (det, association_hashlist)): det for det in DETECTIONS_MERGED}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                local_association, local_association_hashlist = future.result()\n",
    "                association_hashlist = association_hashlist.union(local_association_hashlist)\n",
    "                associations = associations | local_association\n",
    "    finally:\n",
    "        # save the associations no matter if the execution stopped properly\n",
    "        np.save(OUT_FILE, associations)"
   ],
   "id": "cd1998dcdda36f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## plotting density map",
   "id": "631e4ced2608005b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 7\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) > MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1])\n",
    "im = plt.imshow(valid[::-1], aspect=1, cmap=\"inferno\", extent=extent, interpolation=None, vmax=250)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "for s in STATIONS:\n",
    "    p = s.get_pos()\n",
    "\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    plt.plot(p[1], p[0], 'wx', alpha=0.75)\n",
    "    plt.annotate(s.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/15, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/100), textcoords=\"data\", color='w', alpha=0.9)"
   ],
   "id": "3c08dea4155fcf3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1])\n",
    "im = plt.imshow(valid[::-1], aspect=1, cmap=\"inferno\", extent=extent, interpolation=None, vmax=800)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "for s in STATIONS:\n",
    "    p = s.get_pos()\n",
    "\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    plt.plot(p[1], p[0], 'wx', alpha=0.75)\n",
    "    plt.annotate(s.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/15, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/100), textcoords=\"data\", color='w', alpha=0.9)"
   ],
   "id": "2b3de97fb99b9f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE =6\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) < MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "# Create a figure with cartopy's PlateCarree projection\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "# Set the extent of the map (min_lon, max_lon, min_lat, max_lat)\n",
    "ax.set_extent([LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1]], crs=projection)\n",
    "\n",
    "# Add natural features: land, ocean, and coastlines.\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "# ax.add_feature(cfeature.OCEAN, facecolor='lightblue', zorder=2)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "\n",
    "# Plot the georeferenced image.\n",
    "# Set a lower zorder (e.g., 1) so that the map features drawn with higher zorders remain visible.\n",
    "# Adjust alpha to add a bit of transparency if desired.\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1])\n",
    "im = ax.imshow(valid[::-1],\n",
    "               cmap=\"winter\",\n",
    "               extent=extent,\n",
    "               interpolation=\"nearest\",\n",
    "               origin='upper',\n",
    "               transform=projection,\n",
    "               zorder=1,\n",
    "               alpha=1, vmax=10)\n",
    "\n",
    "# Add a colorbar for the image.\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.05)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "# Plot station markers and add annotations using the axes methods.\n",
    "for s in STATIONS:\n",
    "    lat, lon = s.get_pos()\n",
    "    if lat > LAT_BOUNDS[1] or lat < LAT_BOUNDS[0] or lon > LON_BOUNDS[1] or lon < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    # Plot a marker with a higher zorder so it's on top of the image\n",
    "    ax.plot(lon, lat, 'wx', alpha=0.75, markersize=8, transform=projection, zorder=4)\n",
    "    ax.text(lon - (LON_BOUNDS[1] - LON_BOUNDS[0]) / 15,\n",
    "            lat + (LAT_BOUNDS[1] - LAT_BOUNDS[0]) / 100,\n",
    "            s.name,\n",
    "            color='white',\n",
    "            alpha=0.9,\n",
    "            transform=projection,\n",
    "            zorder=4)\n",
    "\n",
    "plt.title(\"Association Data with Land and Sea\")\n",
    "plt.show()\n"
   ],
   "id": "fe57088139aca4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "associations = np.load(path_asso, allow_pickle=True).item()"
   ],
   "id": "c41d7f16931e67dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for date, associations_ in associations.items():\n",
    "    print(date)\n",
    "    for (detections, valid_points) in associations_:\n",
    "        detection = detections\n",
    "        valid_point = valid_points\n",
    "        print(detections)\n",
    "        break\n",
    "    break\n",
    "detections[0,0] #station object\n",
    "detection[0,0].name #station name\n",
    "detection[0,0].get_pos() #station position (lat, lon)\n",
    "detections[0,1] #datetime object of detection at corresponding station\n",
    "valid_points #points on the grid that are candidates for the group of detections"
   ],
   "id": "3c9c62c57614140c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## To Dataframe",
   "id": "b3c31922b025b430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved associations file.\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "\n",
    "# Load associations (allow_pickle=True because the file was saved with pickled Python objects).\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "# Flatten associations:\n",
    "# For each association key and each candidate association (tuple) in its list,\n",
    "# we build a row with the association key, candidate number, detection info, and grid info.\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert the key (datetime) to a string, if needed.\n",
    "    assoc_key_str = str(assoc_key)\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        # candidate is expected to be a tuple: (detections_array, grids_array)\n",
    "        detections_array, grids_array = candidate\n",
    "\n",
    "        # Convert detections array into a list of tuples\n",
    "        # or a string representation if you want a summary.\n",
    "        # For instance, each row of 'detections_array' is [station, datetime]\n",
    "        detection_list = []\n",
    "        for row in detections_array:\n",
    "            # row[0] is typically the station identifier and row[1] is the detection datetime.\n",
    "            detection_list.append((row[0], row[1]))\n",
    "\n",
    "        # Similarly, convert the grid indices array to a list of lists (or string)\n",
    "        grid_list = grids_array.tolist()\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"association_key\": assoc_key_str,\n",
    "            \"candidate_index\": i,\n",
    "            \"detections\": detection_list,\n",
    "            \"grids\": grid_list\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the flattened data.\n",
    "df_associations = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame.\n",
    "# print(df_associations.head())\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file.\n",
    "df_associations.to_csv(\"associations_dataframe.csv\", index=False)\n"
   ],
   "id": "bf3048def0e6b039",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered  = df_associations[df_associations['detections'].apply(len) > 6]",
   "id": "8ad4f1af89493b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_associations",
   "id": "6b12c53ca05aac22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 5\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for grid in df_filtered['grids']:\n",
    "    for valid_points in grid:\n",
    "        valid[valid_points[0], valid_points[1]] += 1\n",
    "\n",
    "# Create a figure with cartopy's PlateCarree projection\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "# Set the extent of the map (min_lon, max_lon, min_lat, max_lat)\n",
    "ax.set_extent([LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1]], crs=projection)\n",
    "\n",
    "# Add natural features: land, ocean, and coastlines.\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "# ax.add_feature(cfeature.OCEAN, facecolor='lightblue', zorder=2)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "\n",
    "# Plot the georeferenced image.\n",
    "# Set a lower zorder (e.g., 1) so that the map features drawn with higher zorders remain visible.\n",
    "# Adjust alpha to add a bit of transparency if desired.\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1])\n",
    "im = ax.imshow(valid[::-1],\n",
    "               cmap=\"tab20c\",\n",
    "               extent=extent,\n",
    "               interpolation=\"nearest\",\n",
    "               origin='upper',\n",
    "               transform=projection,\n",
    "               zorder=1,\n",
    "               alpha=1, vmax=5)\n",
    "\n",
    "# Add a colorbar for the image.\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.05)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "# Plot station markers and add annotations using the axes methods.\n",
    "for s in STATIONS:\n",
    "    lat, lon = s.get_pos()\n",
    "    if lat > LAT_BOUNDS[1] or lat < LAT_BOUNDS[0] or lon > LON_BOUNDS[1] or lon < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    # Plot a marker with a higher zorder so it's on top of the image\n",
    "    ax.plot(lon, lat, 'wx', alpha=0.75, markersize=8, transform=projection, zorder=4)\n",
    "    ax.text(lon - (LON_BOUNDS[1] - LON_BOUNDS[0]) / 15,\n",
    "            lat + (LAT_BOUNDS[1] - LAT_BOUNDS[0]) / 100,\n",
    "            s.name,\n",
    "            color='white',\n",
    "            alpha=0.9,\n",
    "            transform=projection,\n",
    "            zorder=4)\n",
    "\n",
    "plt.title(\"Association Data min {}\".format(MIN_SIZE))\n",
    "plt.show()\n"
   ],
   "id": "c79a040cc7ac51ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration matching your original association parameters\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "\n",
    "# Calculate grid coordinates (replicating your original grid setup)\n",
    "PTS_LAT = np.linspace(LAT_BOUNDS[0], LAT_BOUNDS[1], GRID_SIZE)\n",
    "PTS_LON = np.linspace(LON_BOUNDS[0], LON_BOUNDS[1], GRID_SIZE)\n",
    "\n",
    "def grid_index_to_coords(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    return [(PTS_LAT[i], PTS_LON[j]) for i, j in indices]\n",
    "\n",
    "# Load associations\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert numpy datetime64 to Python datetime\n",
    "    event_time = pd.to_datetime(assoc_key).to_pydatetime()\n",
    "\n",
    "    for candidate_id, (detections_array, grids_array) in enumerate(candidates):\n",
    "        # Process detections\n",
    "        detections = []\n",
    "        for station, dt_np64 in detections_array:\n",
    "            dt = pd.to_datetime(dt_np64).to_pydatetime()\n",
    "            detections.append({\n",
    "                \"station\": station,\n",
    "                \"detection_time\": dt,\n",
    "                \"travel_time\": (event_time - dt).total_seconds()\n",
    "            })\n",
    "\n",
    "        # Process grid points\n",
    "        grid_points = grid_index_to_coords(grids_array)\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"event_time\": event_time,\n",
    "            \"candidate_id\": candidate_id,\n",
    "            \"num_stations\": len(detections),\n",
    "            \"stations\": [d[\"station\"] for d in detections],\n",
    "            \"detection_times\": [d[\"detection_time\"] for d in detections],\n",
    "            \"travel_times\": [d[\"travel_time\"] for d in detections],\n",
    "            \"grid_points\": grid_points,\n",
    "            \"num_grid_points\": len(grid_points),\n",
    "            \"median_lat\": np.median([p[0] for p in grid_points]),\n",
    "            \"median_lon\": np.median([p[1] for p in grid_points])\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Add duration statistics\n",
    "df[\"detection_time_span\"] = df[\"detection_times\"].apply(\n",
    "    lambda x: (max(x) - min(x)).total_seconds()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(df)} associations with:\")\n",
    "print(f\"- Median stations per event: {df['num_stations'].median()}\")\n",
    "print(f\"- Median grid points per event: {df['num_grid_points'].median()}\")\n",
    "\n",
    "# Save to CSV with timestamp\n",
    "output_csv = f\"associations_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nSaved to {output_csv}\")"
   ],
   "id": "f3c2fff2b6d68922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter high-confidence events\n",
    "strong_events = df[(df.num_grid_points >= 1 )& (df.num_stations >6)]\n",
    "\n",
    "# Plot geographic distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(strong_events.median_lon, strong_events.median_lat, s=strong_events.num_stations, alpha=0.5)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze temporal distribution\n",
    "df[\"event_hour\"] = strong_events.event_time.dt.hour\n",
    "df.event_hour.hist(bins=200)"
   ],
   "id": "5b1ee78976b6f4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "strong_events.detection_times.iloc[0]",
   "id": "e55790537e20aaad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from ast import literal_eval\n",
    "# Configuration\n",
    "DATA_ROOT = \"/media/rsafran/CORSAIR/OHASISBIO/\"\n",
    "OUTPUT_DIR = \"./event_analysis\"\n",
    "BUFFER = timedelta(minutes=2)\n",
    "MIN_STATIONS = 7\n",
    "\n",
    "\n",
    "def safe_convert_list(val):\n",
    "    \"\"\"Safely convert string representation of list to actual list\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [x.strip(\" '\") for x in val.strip('[]').split(',')]\n",
    "\n",
    "def safe_convert_datetime(val):\n",
    "    \"\"\"Convert string representation of datetime list\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return pd.to_datetime(literal_eval(val))\n",
    "    except:\n",
    "        return [pd.to_datetime(x.strip(\" '\")) for x in val.strip('[]').split(',')]\n",
    "\n",
    "def safe_convert_coords(val):\n",
    "    \"\"\"Convert grid points string to list of tuples\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return [tuple(map(float, pair.strip('()').split(',')))\n",
    "                for pair in val.strip('[]').split('), ')]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Custom converters for each column type\n",
    "converters = {\n",
    "    'stations': safe_convert_list,\n",
    "    'travel_times': lambda x: [float(v) for v in safe_convert_list(x)],\n",
    "    'detection_time': safe_convert_datetime,\n",
    "    'grid_points': safe_convert_coords\n",
    "}\n",
    "\n",
    "# Load dataframe with proper type conversion\n",
    "df = pd.read_csv(\n",
    "    \"associations_20250414.csv\",\n",
    "    parse_dates=['event_time'],\n",
    "    converters=converters\n",
    ")\n"
   ],
   "id": "8933bd3ba6cc8b40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Signal Check",
   "id": "dd83ecaf4daf838a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs  # Missing import\n",
    "from datetime import timedelta\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "# Configuration matching your original association parameters\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "\n",
    "# Calculate grid coordinates (replicating your original grid setup)\n",
    "PTS_LAT = np.linspace(LAT_BOUNDS[0], LAT_BOUNDS[1], GRID_SIZE)\n",
    "PTS_LON = np.linspace(LON_BOUNDS[0], LON_BOUNDS[1], GRID_SIZE)\n",
    "\n",
    "def grid_index_to_coords(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    return [(PTS_LAT[i], PTS_LON[j]) for i, j in indices]\n",
    "\n",
    "\n",
    "def plot_single_event(event_index, df, data_root, stations_list, lat_bounds, lon_bounds, raw=True):\n",
    "    \"\"\"Interactive visualization for one seismic event\"\"\"\n",
    "    try:\n",
    "        event = df.iloc[event_index]\n",
    "    except IndexError:\n",
    "        print(f\"Error: Event index {event_index} out of range (0-{len(df)-1})\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nEvent {event_index}: {event['event_time']}\")\n",
    "    print(f\"Stations: {', '.join(event['stations'])}\")\n",
    "    print(f\"Median location: ({event['median_lat']:.2f}°, {event['median_lon']:.2f}°)\")\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 10), layout='constrained')\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[3, 2])\n",
    "    ax_signals = fig.add_subplot(gs[0])\n",
    "    ax_map = fig.add_subplot(gs[1], projection=ccrs.PlateCarree())\n",
    "\n",
    "    # Initialize map elements first\n",
    "    ax_map.coastlines()\n",
    "    ax_map.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]])\n",
    "\n",
    "    # Plot all background stations\n",
    "    for s in stations_list:\n",
    "        try:\n",
    "            p = s.get_pos()\n",
    "            if not (lat_bounds[0] <= p[0] <= lat_bounds[1]) or \\\n",
    "               not (lon_bounds[0] <= p[1] <= lon_bounds[1]):\n",
    "                continue\n",
    "            ax_map.plot(p[1], p[0], 'bx', alpha=0.5, markersize=5)\n",
    "            ax_map.annotate(s.name,\n",
    "                xy=(p[1], p[0]),\n",
    "                xytext=(p[1] - (lon_bounds[1]-lon_bounds[0])/15,\n",
    "                        p[0] + (lat_bounds[1]-lat_bounds[0])/100),\n",
    "                textcoords=\"data\",\n",
    "                color='gray',\n",
    "                alpha=0.5,\n",
    "                fontsize=8\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping station {s.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Plot event location\n",
    "    ax_map.plot(event['median_lon'], event['median_lat'], 'r*', markersize=15, zorder=10)\n",
    "\n",
    "    # Load and plot signals\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(event['stations'])))\n",
    "\n",
    "    for i, (full_station_name, det_time, tt) in enumerate(zip(\n",
    "        event['stations'],\n",
    "        event['detection_times'],\n",
    "        event['travel_times'])):\n",
    "        try:\n",
    "            # Split station name\n",
    "            dataset, station = full_station_name.split('_', 1)\n",
    "            center_time = det_time\n",
    "            start = center_time - timedelta(minutes=3)\n",
    "            end = center_time + timedelta(minutes=3)\n",
    "            # Get data manager\n",
    "            managers = DatFilesManager(f\"{data_root}/{dataset}/{station}\")\n",
    "            # Load data\n",
    "            data = managers.get_segment(start, end)\n",
    "            if data is None:\n",
    "                print(f\"No data for {station}\")\n",
    "                continue\n",
    "\n",
    "            # Plot signal\n",
    "            # In the signal loading/plotting section:\n",
    "            SAMPLING_RATE = 240  # Hz\n",
    "            SAMPLE_INTERVAL = 1/SAMPLING_RATE  # seconds\n",
    "\n",
    "            # Create accurate time axis\n",
    "            times = pd.to_datetime(start) + pd.to_timedelta(np.arange(len(data)) * SAMPLE_INTERVAL, unit='s')\n",
    "                        # Compute signal energy over a sliding window\n",
    "            if not raw : #Energy plot\n",
    "                SAMPLING_RATE = 240  # Hz\n",
    "                SAMPLE_INTERVAL = 1 / SAMPLING_RATE  # seconds\n",
    "\n",
    "                # Create time axis\n",
    "                times = pd.to_datetime(start) + pd.to_timedelta(np.arange(len(data)) * SAMPLE_INTERVAL, unit='s')\n",
    "                relative_times = (times - det_time).total_seconds()\n",
    "                # Compute energy envelope\n",
    "                window_sec = 5.0\n",
    "                window_samples = int(window_sec * SAMPLING_RATE)\n",
    "                energy = np.convolve(data**2, np.ones(window_samples)/window_samples, mode='same')\n",
    "\n",
    "                # Normalize energy\n",
    "                if np.max(energy) > 0:\n",
    "                    energy /= np.max(energy)\n",
    "\n",
    "                # Vertically shift for visual separation\n",
    "                offset = i * 1.5  # You can tune this value if traces are too close/far\n",
    "                energy_shifted = energy + offset\n",
    "\n",
    "                # Plot energy\n",
    "                ax_signals.plot(\n",
    "                    relative_times, energy_shifted,\n",
    "                    label=f\"{station} (ΔT={tt:.1f}s)\",\n",
    "                    color=colors[i],\n",
    "                    alpha=0.8\n",
    "                )\n",
    "\n",
    "                # Mark detection time\n",
    "                # ax_signals.axvline(relative_times, color=colors[i], linestyle='--', alpha=0.5)\n",
    "\n",
    "            if raw : #raw signal\n",
    "\n",
    "                # Normalize signal\n",
    "                data /= np.max(abs(data))\n",
    "\n",
    "                # Vertically shift for visual separation\n",
    "                offset = i * 1.5  # You can tune this value if traces are too close/far\n",
    "                data_shifted = data + offset\n",
    "                relative_times = (times - det_time).total_seconds()\n",
    "                ax_signals.plot(\n",
    "                    relative_times, data_shifted,\n",
    "                    label=f\"{station} (ΔT={tt:.1f}s)\",\n",
    "                    color=colors[i],\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                ax_signals.axvline(0, color=colors[i], linestyle='--', alpha=0.5)\n",
    "\n",
    "            # Plot detection station on map\n",
    "            dataset, station = full_station_name.split('_', 1)\n",
    "            s = next((s for s in stations_list if s.name == station), None)\n",
    "            if s:\n",
    "                p = s.get_pos()\n",
    "                ax_map.plot(\n",
    "                        p[1], p[0],\n",
    "                        marker='^',\n",
    "                        markersize=8,\n",
    "                        color=colors[i],\n",
    "                        markeredgewidth=1,\n",
    "                        zorder=9)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_station_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Format plots after all data is plotted\n",
    "    ax_signals.set_title(f\"Event {event['event_time']}\\n{len(event['stations'])} stations detected\")\n",
    "    # if raw :\n",
    "    #     ax_signals.xaxis.set_major_formatter(DateFormatter(\"%H:%M:%S\"))\n",
    "    #     ax_signals.set_xlabel(\"UTC Time\")\n",
    "    ax_signals.set_ylabel(\"Amplitude\")\n",
    "    ax_signals.legend(loc='upper right')\n",
    "\n",
    "    ax_map.set_title(\"Event Location (red star) and Detection Stations (colored triangles)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # df = pd.read_csv(\"associations_20250414.csv.csv\", parse_dates=['event_time'])  # Add your converters\n",
    "    df = df[df.num_stations > 7]\n",
    "    DATA_ROOT = \"/media/rsafran/CORSAIR/OHASISBIO\"\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            max_idx = len(df) - 1\n",
    "            prompt = f\"\\nEnter event index (0-{max_idx}) or -1 to quit: \"\n",
    "            event_idx = int(input(prompt))\n",
    "\n",
    "            if event_idx == -1:\n",
    "                break\n",
    "            plot_single_event(event_idx, df, DATA_ROOT, STATIONS, LAT_BOUNDS, LON_BOUNDS)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number\")"
   ],
   "id": "671f101b845c7613",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Least square",
   "id": "bc961ba2f0988617"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading",
   "id": "bf78fbf143dc072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved associations file.\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "\n",
    "# Load associations (allow_pickle=True because the file was saved with pickled Python objects).\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "# Flatten associations:\n",
    "# For each association key and each candidate association (tuple) in its list,\n",
    "# we build a row with the association key, candidate number, detection info, and grid info.\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert the key (datetime) to a string, if needed.\n",
    "    assoc_key_str = str(assoc_key)\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        # candidate is expected to be a tuple: (detections_array, grids_array)\n",
    "        detections_array, grids_array = candidate\n",
    "\n",
    "        # Convert detections array into a list of tuples\n",
    "        # or a string representation if you want a summary.\n",
    "        # For instance, each row of 'detections_array' is [station, datetime]\n",
    "        detection_list = []\n",
    "        for row in detections_array:\n",
    "            # row[0] is typically the station identifier and row[1] is the detection datetime.\n",
    "            detection_list.append((row[0], row[1]))\n",
    "\n",
    "        # Similarly, convert the grid indices array to a list of lists (or string)\n",
    "        grid_list = grids_array.tolist()\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"association_key\": assoc_key_str,\n",
    "            \"candidate_index\": i,\n",
    "            \"detections\": detection_list,\n",
    "            \"grids\": grid_list\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the flattened data.\n",
    "df_associations = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame.\n",
    "df_associations"
   ],
   "id": "6d6077bb43a651f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eval single value",
   "id": "371d8f5841fbd931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_associations = df_associations[df_associations['detections'].apply(len) > 7]\n",
    "df_associations.reset_index(inplace=True)"
   ],
   "id": "469e96795ea86e66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "row = df_associations.iloc[555]\n",
    "detections = row[\"detections\"]\n",
    "grids = row[\"grids\"]\n",
    "month = pd.to_datetime(row['association_key']).month\n",
    "# print(month)\n",
    "def grid_index_to_coord(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    i, j = indices\n",
    "    return [PTS_LAT[i], PTS_LON[j]]\n",
    "\n",
    "\n",
    "for [station, d_time] in detections:\n",
    "    print((station, d_time))\n",
    "    print(d_time.timestamp())  #d_time is a datetime object\n",
    "    print(station.get_pos())\n",
    "    print(station.name) #station has a method get_pos() and .name\n",
    "\n",
    "print(grid_index_to_coord(grids[0])) #initial guess event lat and long\n",
    "\n"
   ],
   "id": "b0d1e8824044c694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "lat_bounds = LAT_BOUNDS\n",
    "lon_bounds = LON_BOUNDS\n",
    "PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "\n",
    "\n",
    "ds = isg.load_ISAS_TS(PATH, month,lat_bounds,lon_bounds, fast=False)\n",
    "# Convert datetime to seconds since some origin (e.g., min time)\n",
    "arrival_times = [d_time for _, d_time in detections]\n",
    "origin_time = min(arrival_times)\n",
    "arrival_times_seconds = np.array([(t - origin_time).total_seconds() for t in arrival_times])\n",
    "station_positions = [station.get_pos() for station, _ in detections]\n",
    "grid_init_lat = [grid_index_to_coord(grid)[0] for grid in grids]\n",
    "grid_init_lon = [grid_index_to_coord(grid)[1] for grid in grids]\n",
    "def residuals(params, station_positions, arrival_times, ds):\n",
    "    \"\"\"\n",
    "    Improved residuals function that accounts for travel time uncertainty.\n",
    "\n",
    "    params: (lat, lon, t0) where t0 is in seconds\n",
    "    station_positions: list/array of (lat, lon) station coordinates\n",
    "    arrival_times: array of arrival times (in seconds)\n",
    "    ds: dataset with sound velocity data\n",
    "\n",
    "    Returns:\n",
    "    - weighted residuals: difference between modeled and observed arrival times,\n",
    "      weighted by uncertainty when available\n",
    "    \"\"\"\n",
    "    lat, lon, t0 = params\n",
    "    modeled_times = []\n",
    "    uncertainties = []\n",
    "\n",
    "    max_depth = 1130  # Or appropriate depth for your dataset\n",
    "\n",
    "    for i, (slat, slon) in enumerate(station_positions):\n",
    "        try:\n",
    "            travel_time, sig_v, _ = isg.compute_travel_time(\n",
    "                slat, slon, lat, lon, depth=max_depth,\n",
    "                ds=ds, resolution=30, verbose=False,\n",
    "                interpolate_missing=True\n",
    "            )\n",
    "            modeled_times.append(t0 + travel_time)\n",
    "            uncertainties.append(sig_v if sig_v > 0 else 1.0)  # Default to 1.0 if uncertainty is zero\n",
    "        except Exception as e:\n",
    "            print(f\"Error at station {i}: {slat},{slon} to {lat},{lon}: {e}\")\n",
    "            # Add a placeholder with high uncertainty\n",
    "            modeled_times.append(arrival_times[i])  # Use observed time as placeholder\n",
    "            uncertainties.append(1000.0)  # Very high uncertainty\n",
    "\n",
    "    modeled_times = np.array(modeled_times)\n",
    "    uncertainties = np.array(uncertainties)\n",
    "\n",
    "    # Calculate weighted residuals - divide by uncertainty to weight more certain measurements higher\n",
    "    weighted_residuals = (modeled_times - arrival_times) / 2+uncertainties\n",
    "\n",
    "    return weighted_residuals"
   ],
   "id": "418631251c0990d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Better initialization with bounds and handling\n",
    "from scipy.optimize import least_squares\n",
    "import numpy as np\n",
    "from pyproj import Geod\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Define reasonable geographic bounds based on your area of interest\n",
    "lat_min, lat_max = min(LAT_BOUNDS), max(LAT_BOUNDS)\n",
    "lon_min, lon_max = min(LON_BOUNDS), max(LON_BOUNDS)\n",
    "\n",
    "# More conservative initial guess (center of first few stations)\n",
    "lat0 = np.mean(grid_init_lat)\n",
    "lon0 = np.mean(grid_init_lon)\n",
    "\n",
    "\n",
    "# Vectorized distance calculation\n",
    "def calc_distances(lon0, lat0, positions):\n",
    "    return np.array([geod.inv(lon0, lat0, pos[1], pos[0])[2] for pos in positions])\n",
    "\n",
    "\n",
    "# Estimate travel time to nearest station for t0 guess\n",
    "distances = calc_distances(lon0, lat0, station_positions)\n",
    "nearest_idx = np.argmin(distances)\n",
    "nearest_lat, nearest_lon = station_positions[nearest_idx]\n",
    "_, _, distance_m = geod.inv(lon0, lat0, nearest_lon, nearest_lat)\n",
    "\n",
    "# Use a more conservative sound speed estimate \n",
    "sound_speed = 1480  # m/s\n",
    "t0_guess = -distance_m / sound_speed\n",
    "\n",
    "# Initial guess\n",
    "x0 = [lat0, lon0, t0_guess]\n",
    "\n",
    "# Add bounds to keep solution in reasonable area\n",
    "bounds = ([lat0-10, lon0-10, -np.inf], [lat0+10, lon0+10, 1])\n",
    "\n",
    "# Use jac='3-point' for faster Jacobian approximation\n",
    "result = least_squares(\n",
    "    residuals, x0,\n",
    "    args=(station_positions, arrival_times_seconds, ds),\n",
    "    bounds=bounds,\n",
    "    loss='soft_l1',      # or 'huber'      # tuning parameter\n",
    "    method='trf',\n",
    "    ftol=1e-12, xtol=1e-12, gtol=1e-12,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ],
   "id": "43475591f012dbab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Results\n",
    "estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "print(f\"Initial guess: {lat0, lon0, t0_guess}\")\n",
    "print(f\"Estimated source location: ({estimated_lat:.4f}, {estimated_lon:.4f})\")\n",
    "print(f\"Estimated origin time offset: {estimated_t0:.2f} seconds\")"
   ],
   "id": "a1387549f68ef49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats\n",
    "# Your results\n",
    "estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "initial_lat, initial_lon = lat0, lon0\n",
    "\n",
    "# Optional: List of receiver stations (if available)\n",
    "# receivers = [(lat1, lon1), (lat2, lon2), ...]\n",
    "\n",
    "# Create figure and map projection\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# Plot estimated source location\n",
    "ax.plot(estimated_lon, estimated_lat, marker='*', color='red', markersize=15, label='Estimated Source')\n",
    "\n",
    "# Plot initial guess (optional)\n",
    "ax.plot(initial_lon, initial_lat, marker='x', color='blue', markersize=10, label='Initial Guess')\n",
    "\n",
    "if hasattr(result, 'jac'):\n",
    "    # Get the Jacobian for the lat-lon parameters (exclude time parameter)\n",
    "    J = result.jac[:, :2]\n",
    "\n",
    "    # Calculate the covariance matrix (approximation)\n",
    "    residual_variance = np.sum(result.fun**2) / (len(result.fun) - len(result.x))\n",
    "    covariance_matrix = residual_variance * np.linalg.inv(J.T @ J)\n",
    "\n",
    "    # Eigenvalue decomposition to get ellipse parameters\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Calculate ellipse parameters\n",
    "    # For 95% confidence, use 2.4477 (chi-square with 2 DOF)\n",
    "    confidence = 0.95\n",
    "    chi2_val = stats.chi2.ppf(confidence, 2)\n",
    "\n",
    "    # Semi-major and semi-minor axes\n",
    "    a = np.sqrt(eigenvalues[1] * chi2_val)\n",
    "    b = np.sqrt(eigenvalues[0] * chi2_val)\n",
    "\n",
    "    # Angle in degrees\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n",
    "\n",
    "    # Create and add the ellipse\n",
    "    fac = 1\n",
    "    ellipse = Ellipse(xy=(estimated_lon, estimated_lat),\n",
    "                      width=fac*2 * a, height=fac*2 * b,\n",
    "                      angle=angle,\n",
    "                      edgecolor='red', facecolor='red', alpha=0.8,\n",
    "                      transform=ccrs.PlateCarree(),\n",
    "                      label=f'{int(confidence*100)}% Confidence')\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "# Plot receiver stations (optional)\n",
    "# for lat, lon in receivers:\n",
    "#     ax.plot(lon, lat, marker='^', color='green', markersize=8)\n",
    "\n",
    "# Set extent if needed (min_lon, max_lon, min_lat, max_lat)\n",
    "offset = 5\n",
    "ax.set_extent([estimated_lon - offset, estimated_lon + offset, estimated_lat - offset, estimated_lat + offset])\n",
    "\n",
    "# Add legend and title\n",
    "plt.legend()\n",
    "plt.title('Estimated Source Location on Map')\n",
    "plt.show()\n"
   ],
   "id": "ad4fde7b04091398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats\n",
    "\n",
    "def visualize_localization_result(result, station_positions, ds, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Visualize source localization result with error ellipse.\n",
    "\n",
    "    Parameters:\n",
    "    - result: The optimization result object from least_squares\n",
    "    - station_positions: List of (lat, lon) tuples for station positions\n",
    "    - ds: The ISAS dataset (for context)\n",
    "    - lat_bounds, lon_bounds: The geographic boundaries for the map\n",
    "    \"\"\"\n",
    "    # Extract the best fit parameters and stations\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "    station_lats = [pos[0] for pos in station_positions]\n",
    "    station_lons = [pos[1] for pos in station_positions]\n",
    "\n",
    "    # Set up the map projection\n",
    "    projection = ccrs.PlateCarree()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': projection})\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines(resolution='50m')\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "    # Set map boundaries with some padding\n",
    "    pad = 2  # degrees\n",
    "    ax.set_extent([lon_bounds[0]-pad, lon_bounds[1]+pad,\n",
    "                  lat_bounds[0]-pad, lat_bounds[1]+pad])\n",
    "\n",
    "    # Plot stations\n",
    "    ax.scatter(station_lons, station_lats, color='blue', s=100, marker='^',\n",
    "               transform=ccrs.PlateCarree(), label='Stations', zorder=3)\n",
    "\n",
    "    # Plot the estimated source location\n",
    "    ax.scatter([estimated_lon], [estimated_lat], color='red', s=150, marker='+',\n",
    "               transform=ccrs.PlateCarree(), label='Estimated Source', zorder=4)\n",
    "\n",
    "    # Calculate the error ellipse using the covariance matrix\n",
    "    # This requires the Jacobian from the optimization result\n",
    "    if hasattr(result, 'jac'):\n",
    "        # Get the Jacobian for the lat-lon parameters (exclude time parameter)\n",
    "        J = result.jac[:, :2]\n",
    "\n",
    "        # Calculate the covariance matrix (approximation)\n",
    "        residual_variance = np.sum(result.fun**2) / (len(result.fun) - len(result.x))\n",
    "        covariance_matrix = residual_variance * np.linalg.inv(J.T @ J)\n",
    "\n",
    "        # Eigenvalue decomposition to get ellipse parameters\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "        # Calculate ellipse parameters\n",
    "        # For 95% confidence, use 2.4477 (chi-square with 2 DOF)\n",
    "        confidence = 0.95\n",
    "        chi2_val = stats.chi2.ppf(confidence, 2)\n",
    "\n",
    "        # Semi-major and semi-minor axes\n",
    "        a = np.sqrt(eigenvalues[1] * chi2_val)\n",
    "        b = np.sqrt(eigenvalues[0] * chi2_val)\n",
    "\n",
    "        # Angle in degrees\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n",
    "\n",
    "        # Create and add the ellipse\n",
    "        fac = 1\n",
    "        ellipse = Ellipse(xy=(estimated_lon, estimated_lat),\n",
    "                          width=fac*2 * a, height=fac*2 * b,\n",
    "                          angle=angle,\n",
    "                          edgecolor='red', facecolor='red', alpha=0.8,\n",
    "                          transform=ccrs.PlateCarree(),\n",
    "                          label=f'{int(confidence*100)}% Confidence')\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "        # Add error statistics to title\n",
    "        plt.title(f'Source Localization Result\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)\\n'\n",
    "                 f'Time Offset: {estimated_t0:.2f} s, RMS Error: {np.sqrt(np.mean(result.fun**2)):.2f} s')\n",
    "    else:\n",
    "        # If no Jacobian, just add basic title\n",
    "        plt.title(f'Source Localization Result\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)')\n",
    "\n",
    "    # Add labels and legend\n",
    "    for i, (lon, lat) in enumerate(zip(station_lons, station_lats)):\n",
    "        ax.text(lon + 0.1, lat + 0.1, f'Station {i+1}', transform=ccrs.PlateCarree(),\n",
    "                fontsize=9, horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    # Add a scale bar if desired\n",
    "    # ax.gridlines(draw_labels=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Call this function with your result\n",
    "fig, ax = visualize_localization_result(result, station_positions, ds, LAT_BOUNDS, LON_BOUNDS)\n",
    "\n",
    "plt.savefig('source_localization_result.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "b8fd7d14aef0d4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_residuals(result, station_positions, arrival_times, ds):\n",
    "    \"\"\"\n",
    "    Analyze residuals to identify potential outliers in the data\n",
    "\n",
    "    Parameters:\n",
    "    - result: optimization result from least_squares\n",
    "    - station_positions: list of (lat, lon) tuples for stations\n",
    "    - arrival_times: observed arrival times in seconds\n",
    "    - ds: ISAS dataset\n",
    "\n",
    "    Returns:\n",
    "    - residuals: array of time residuals for each station\n",
    "    - is_outlier: boolean array indicating potential outliers\n",
    "    \"\"\"\n",
    "    # Extract estimated parameters\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "\n",
    "    # Calculate modeled arrival times for each station\n",
    "    modeled_times = []\n",
    "    station_ids = []\n",
    "\n",
    "    for i, (slat, slon) in enumerate(station_positions):\n",
    "        try:\n",
    "            travel_time, _, _ = isg.compute_travel_time(\n",
    "                slat, slon, estimated_lat, estimated_lon,\n",
    "                depth=1500, ds=ds, resolution=30,\n",
    "                verbose=False, interpolate_missing=True\n",
    "            )\n",
    "            modeled_times.append(estimated_t0 + travel_time)\n",
    "            station_ids.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating travel time for station {i}: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    modeled_times = np.array(modeled_times)\n",
    "    observed_times = np.array([arrival_times[i] for i in station_ids])\n",
    "\n",
    "    # Calculate residuals (modeled - observed)\n",
    "    time_residuals = modeled_times - observed_times\n",
    "\n",
    "    # Identify outliers using Modified Z-score method\n",
    "    # This is more robust than standard Z-score for small sample sizes\n",
    "    median_residual = np.median(time_residuals)\n",
    "    mad = np.median(np.abs(time_residuals - median_residual))  # Median Absolute Deviation\n",
    "\n",
    "    # MAD needs to be scaled to be comparable to standard deviation\n",
    "    mad_constant = 1.4826  # for normal distribution\n",
    "    modified_z_scores = mad_constant * (time_residuals - median_residual) / (mad + 1e-8)\n",
    "\n",
    "    # Define outlier threshold (typically 3.5 or higher)\n",
    "    outlier_threshold = 3.5\n",
    "    is_outlier = np.abs(modified_z_scores) > outlier_threshold\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nResidual Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Station':8} {'Residual (s)':12} {'Modified Z':10} {'Outlier':8}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, station_id in enumerate(station_ids):\n",
    "        outlier_mark = \"YES\" if is_outlier[i] else \"NO\"\n",
    "        print(f\"{station_id:8d} {time_residuals[i]:12.2f} {modified_z_scores[i]:10.2f} {outlier_mark:8}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"RMS Error: {np.sqrt(np.mean(time_residuals**2)):.2f} seconds\")\n",
    "    print(f\"Median Absolute Deviation: {mad:.2f} seconds\")\n",
    "\n",
    "    return time_residuals, is_outlier, station_ids\n",
    "\n",
    "def visualize_residuals(result, station_positions, arrival_times, ds, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Visualize residuals and identify outliers on a map\n",
    "    \"\"\"\n",
    "    # Calculate residuals and identify outliers\n",
    "    residuals, is_outlier, included_station_ids = analyze_residuals(\n",
    "        result, station_positions, arrival_times, ds\n",
    "    )\n",
    "\n",
    "    # Extract estimated source location\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "\n",
    "    # Set up the map\n",
    "    projection = ccrs.PlateCarree()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8),\n",
    "                                subplot_kw={'projection': projection})\n",
    "\n",
    "    # --- First subplot: Map with stations and source ---\n",
    "    ax1.coastlines(resolution='50m')\n",
    "    ax1.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax1.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax1.gridlines(draw_labels=True)\n",
    "\n",
    "    # Set map boundaries with padding\n",
    "    pad = 2  # degrees\n",
    "    ax1.set_extent([lon_bounds[0]-pad, lon_bounds[1]+pad,\n",
    "                   lat_bounds[0]-pad, lat_bounds[1]+pad])\n",
    "\n",
    "    # Plot stations with color based on residual\n",
    "    # Convert residuals to colormap values\n",
    "    norm = plt.Normalize(vmin=-np.max(np.abs(residuals)), vmax=np.max(np.abs(residuals)))\n",
    "\n",
    "    # Plot stations\n",
    "    for i, station_id in enumerate(included_station_ids):\n",
    "        slat, slon = station_positions[station_id]\n",
    "\n",
    "        # Marker properties based on whether it's an outlier\n",
    "        marker_size = 150 if is_outlier[i] else 100\n",
    "        marker_edge = 'black' if is_outlier[i] else None\n",
    "        marker_width = 2 if is_outlier[i] else 0\n",
    "\n",
    "        # Color based on residual value\n",
    "        color = plt.cm.RdBu_r(norm(residuals[i]))\n",
    "\n",
    "        # Add station marker\n",
    "        ax1.scatter([slon], [slat], color=color, s=marker_size, marker='^',\n",
    "                   edgecolor=marker_edge, linewidth=marker_width,\n",
    "                   transform=ccrs.PlateCarree())\n",
    "\n",
    "        # Add station label\n",
    "        ax1.text(slon + 0.1, slat + 0.1, f'Station {station_id}',\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                fontsize=8, horizontalalignment='left')\n",
    "\n",
    "    # Plot the estimated source\n",
    "    ax1.scatter([estimated_lon], [estimated_lat], color='red', s=150, marker='*',\n",
    "               transform=ccrs.PlateCarree(), label='Estimated Source')\n",
    "\n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap='RdBu_r', norm=norm)\n",
    "    cbar = plt.colorbar(sm, ax=ax1, pad=0.05, orientation='horizontal')\n",
    "    cbar.set_label('Residual (seconds)')\n",
    "\n",
    "    ax1.set_title('Source Localization with Residuals\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)')\n",
    "\n",
    "    # --- Second subplot: Residual plot ---\n",
    "    # Create a simple plot to show residuals by station\n",
    "    ax2.set_aspect('auto')  # Reset aspect ratio for this plot\n",
    "\n",
    "    # Create bar plot of residuals\n",
    "    bar_positions = np.arange(len(included_station_ids))\n",
    "    colors = [plt.cm.RdBu_r(norm(res)) for res in residuals]\n",
    "\n",
    "    # Plot bars\n",
    "    bars = ax2.bar(bar_positions, residuals, color=colors, edgecolor='black')\n",
    "\n",
    "    # Highlight outliers with pattern\n",
    "    for i, is_out in enumerate(is_outlier):\n",
    "        if is_out:\n",
    "            bars[i].set_hatch('///')\n",
    "\n",
    "    # Add zero line\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # Add threshold lines\n",
    "    mad = np.median(np.abs(residuals - np.median(residuals)))\n",
    "    threshold = 3.5 * 1.4826 * mad\n",
    "    ax2.axhline(y=threshold, color='red', linestyle='--', alpha=0.5, label=f'Outlier Threshold (±{threshold:.2f}s)')\n",
    "    ax2.axhline(y=-threshold, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Set labels\n",
    "    ax2.set_xlabel('Station ID')\n",
    "    ax2.set_ylabel('Residual (seconds)')\n",
    "    ax2.set_title('Arrival Time Residuals by Station')\n",
    "    ax2.set_xticks(bar_positions)\n",
    "    ax2.set_xticklabels([f'Station {sid}' for sid in included_station_ids], rotation=45)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return residuals, is_outlier\n",
    "\n",
    "# Call the function with your results\n",
    "residuals, outliers = visualize_residuals(result, station_positions, arrival_times_seconds, ds, LAT_BOUNDS, LON_BOUNDS)"
   ],
   "id": "2235be57beb5c43d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `result` from least_squares and `station_positions`, `arrival_times_seconds`, and `ds` are in scope\n",
    "\n",
    "# Extract residuals and Jacobian\n",
    "residuals = result.fun\n",
    "J = result.jac\n",
    "\n",
    "# Number of observations and parameters\n",
    "n_obs = residuals.size\n",
    "n_params = result.x.size\n",
    "\n",
    "# Compute reduced chi-squared\n",
    "SSR = np.sum(residuals**2)\n",
    "reduced_chi2 = SSR / (n_obs - n_params)\n",
    "\n",
    "# Parameter covariance and standard errors\n",
    "cov_params = np.linalg.inv(J.T @ J) * reduced_chi2\n",
    "param_std = np.sqrt(np.diag(cov_params))\n",
    "\n",
    "# Parameter names\n",
    "param_names = ['latitude', 'longitude', 't0']\n",
    "\n",
    "# Print summary table\n",
    "print(f\"Reduced χ²: {reduced_chi2:.3f}\\n\")\n",
    "print(\"Parameter estimates with 1σ uncertainties:\")\n",
    "for name, val, err in zip(param_names, result.x, param_std):\n",
    "    print(f\"  {name:<10} = {val:.6f} ± {err:.6f}\")\n",
    "\n",
    "# Plot histogram of weighted residuals\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=10, edgecolor='black')\n",
    "plt.xlabel('Weighted residuals')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Weighted Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d65c4ef243b6935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## On array",
   "id": "f82deddf50e39ffb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from scipy.optimize import least_squares, minimize\n",
    "from pyproj import Geod\n",
    "import xarray as xr\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "\n",
    "# Load the association data\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "# path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/refined_-60--12.4,35-100,350,0.8,0.5.npy\" #repointed\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "DATA_ROOT = \"/media/rsafran/CORSAIR/OHASISBIO\"\n",
    "\n",
    "# Setup for geospatial calculations\n",
    "geod = Geod(ellps=\"WGS84\")  # Use WGS84 ellipsoid for distance calculations\n",
    "\n",
    "lat_bounds = [-60,5]\n",
    "lon_bounds = [35,120]\n",
    "grid_size = 350\n",
    "\n",
    "PTS_LAT = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "PTS_LON = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "LAT_BOUNDS = lat_bounds\n",
    "LON_BOUNDS = lon_bounds\n",
    "# Constants\n",
    "SAMPLING_RATE = 240  # Hz\n",
    "SAMPLE_INTERVAL = 1 / SAMPLING_RATE  # seconds\n",
    "WINDOW_SEC = 5.0\n",
    "WINDOW_SAMPLES = int(WINDOW_SEC * SAMPLING_RATE)\n",
    "\n",
    "\n",
    "# Load the ISAS grid data for sound velocity\n",
    "def load_isas_data(month, lat_bounds, lon_bounds):\n",
    "    PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "    return isg.load_ISAS_TS(PATH, month, lat_bounds, lon_bounds, fast=False)\n",
    "\n",
    "# Results container\n",
    "validated_associations = {}\n",
    "\n",
    "velocity_grid = {}\n",
    "for month in range(1, 13):\n",
    "    print(f\"Loading month {month}...\")\n",
    "    ds = load_isas_data(month, LAT_BOUNDS, LON_BOUNDS)\n",
    "    velocity_grid[month] = ds\n",
    "\n",
    "\n",
    "# Geographic parameters\n",
    "geod = Geod(ellps=\"WGS84\")  # For accurate distance calculations\n",
    "\n",
    "# Define reasonable geographic bounds\n",
    "\n",
    "\n",
    "def grid_index_to_coord(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    i, j = indices\n",
    "    return [PTS_LAT[i], PTS_LON[j]]"
   ],
   "id": "df89925a2df0e3a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Modified validation container to include the full result object\n",
    "validated_associations = {}\n",
    "from itertools import islice\n",
    "# for date, associations_list in islice(associations.items(), 50,500):\n",
    "for date, associations_list in associations.items():\n",
    "    # Extract month from date for ISAS data selection\n",
    "    month = pd.to_datetime(date).month\n",
    "\n",
    "    # Load ISAS data for this month\n",
    "    ds = velocity_grid[month]\n",
    "    validated_associations[date] = []\n",
    "\n",
    "    # for (detections, valid_points) in associations_list:\n",
    "    #     if len(detections) <7 :\n",
    "    #         continue\n",
    "        # Store refined detection times and station positions\n",
    "        refined_detections = []\n",
    "        station_positions = []\n",
    "\n",
    "        # For each station detection\n",
    "        for i in range(len(detections)):\n",
    "            station = detections[i, 0]\n",
    "            precise_time = detections[i, 1]\n",
    "\n",
    "            # Get station position\n",
    "            station_pos = station.get_pos()  # (lat, lon)\n",
    "            station_positions.append(station_pos)\n",
    "\n",
    "            refined_detections.append((station, precise_time))\n",
    "\n",
    "        # Convert station_positions to numpy array for easier handling\n",
    "        station_positions = np.array(station_positions)\n",
    "\n",
    "        # 2) Build dynamic bounds around valid_points (±margin °)\n",
    "        # -------------------------------------------------------\n",
    "        margin = 0.5  # degrees (~50 km)\n",
    "        if valid_points.any():\n",
    "            # convert grid‐indices → lat/lon\n",
    "            coords = np.array([grid_index_to_coord(p) for p in valid_points])\n",
    "            lat_min = coords[:,0].min() - margin\n",
    "            lat_max = coords[:,0].max() + margin\n",
    "            lon_min = coords[:,1].min() - margin\n",
    "            lon_max = coords[:,1].max() + margin\n",
    "            # initial guess is simply the centroid\n",
    "            grid_init_lat, grid_init_lon = coords.mean(axis=0)\n",
    "        else:\n",
    "            # fallback to station centroid\n",
    "            lat_min, lat_max = np.min(station_positions[:,0]), np.max(station_positions[:,0])\n",
    "            lon_min, lon_max = np.min(station_positions[:,1]), np.max(station_positions[:,1])\n",
    "            grid_init_lat = station_positions[:,0].mean()\n",
    "            grid_init_lon = station_positions[:,1].mean()\n",
    "\n",
    "        # 3) Set up bounds and initial vector\n",
    "        # -----------------------------------\n",
    "        lon0, lat0 = grid_init_lon, grid_init_lat\n",
    "        distances = np.array([\n",
    "            geod.inv(lon0, lat0, lat, lon)[2]\n",
    "            for lat, lon in station_positions\n",
    "        ])\n",
    "        nearest_idx = distances.argmin()\n",
    "        nearest_time = refined_detections[nearest_idx][1]\n",
    "        distance_m = distances[nearest_idx]\n",
    "        sound_speed = 1480  # m/s (2000 m depth)\n",
    "        t0_guess = - (distance_m / sound_speed)\n",
    "        x0 = [grid_init_lat, grid_init_lon, 0]\n",
    "        bounds = (\n",
    "            [lat_min, lon_min, -3600],  # [lat, lon, t0_offset]\n",
    "            [lat_max, lon_max,  +1]\n",
    "        )\n",
    "\n",
    "        # Reference detection time (e.g., earliest detection)\n",
    "        ref_time = min([det_time for station, det_time in refined_detections])\n",
    "\n",
    "\n",
    "        # Build detection seconds array\n",
    "        detection_seconds = []\n",
    "        for station, detection_time in refined_detections:\n",
    "            seconds = (detection_time - ref_time).total_seconds()\n",
    "            detection_seconds.append((station, seconds))\n",
    "\n",
    "        # print(detection_seconds)\n",
    "        # Define the residual function for least squares\n",
    "        def residual(params):\n",
    "            lat, lon, t0_offset = params  # t0_offset is in seconds\n",
    "\n",
    "            errors = []\n",
    "            weights = []  # For weighted least squares based on error estimates\n",
    "            for (station, detection_sec) in detection_seconds:\n",
    "                station_lat, station_lon = station.get_pos()\n",
    "                # Using fixed depth of 2000m - could be refined if source depth is known\n",
    "                depth = 1250  # meters, assuming source is at this depth (Here we use mean of stations depths)\n",
    "\n",
    "                try:\n",
    "                    # Use 2D propagation at constant depth for simplicity\n",
    "                    travel_time_sec, tt_error, _ = isg.compute_travel_time(\n",
    "                        lat, lon, station_lat, station_lon,\n",
    "                        depth, ds,\n",
    "                        resolution=20,  # km between points\n",
    "                        verbose=False,\n",
    "                        interpolate_missing=True)\n",
    "\n",
    "                except ValueError:\n",
    "                    # If sound velocity data is unavailable, use simple estimate\n",
    "                    print(\"error\")\n",
    "                    _, _, distance_m = geod.inv(lon, lat, station_lon, station_lat)\n",
    "                    travel_time_sec = distance_m / sound_speed\n",
    "                    tt_error = travel_time_sec * 0.1  # 10% error estimate\n",
    "                # Predicted detection time (in seconds)\n",
    "                predicted_sec = t0_offset + travel_time_sec\n",
    "                # Residual: (observed - predicted)\n",
    "                errors.append(detection_sec - predicted_sec)\n",
    "                weights.append(1.0 / (tt_error + 1))\n",
    "            return np.array(errors) * np.array(weights)\n",
    "\n",
    "        # 4) Single least_squares solve\n",
    "        # -----------------------------\n",
    "        try:\n",
    "            best_result = least_squares(\n",
    "                residual,\n",
    "                x0,\n",
    "                bounds=bounds,\n",
    "                method='trf',\n",
    "                loss='soft_l1',\n",
    "                verbose=1,  # To monitor optimization progress\n",
    "                gtol=1e-4,  # Increase tolerance to avoid overfitting\n",
    "                xtol=1e-4   # Increase tolerance to avoid overfitting\n",
    "            )\n",
    "\n",
    "            min_cost = best_result.cost\n",
    "        except Exception as e:\n",
    "            print(f\"least_squares failure: {e}\")\n",
    "            continue  # skip this cluster on failure\n",
    "\n",
    "        # 5) Post‐process and threshold\n",
    "        # -----------------------------\n",
    "        final_lat, final_lon, t0_offset = best_result.x\n",
    "        final_t0 = ref_time + timedelta(seconds=t0_offset)\n",
    "        rms_error = np.sqrt(min_cost / len(refined_detections))\n",
    "        # your threshold\n",
    "        validated_associations[date].append({\n",
    "            'detections':       refined_detections,\n",
    "            'source_point':     (final_lat, final_lon),\n",
    "            'origin_time':      final_t0,\n",
    "            'rms_error':        rms_error,\n",
    "            'num_stations':     len(refined_detections),\n",
    "            'optimization_result': best_result\n",
    "        })\n",
    "# Save validated associations\n",
    "output_path = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/validated/2018/s_-60--12.4,35-100,350,0.8,0.5.npy\"\n",
    "np.save(output_path, validated_associations)\n",
    "print(f\"Saved validated associations to {output_path}\")"
   ],
   "id": "975c88196bd65ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot results",
   "id": "fc4be815eeae471b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validated_associations = np.load(output_path, allow_pickle=True).item()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optionally add this if you want real coastlines\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# for nice maps:\n",
    "# fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(12, 10))\n",
    "# ax.coastlines()  # If using Cartopy\n",
    "# ax.add_feature(cfeature.LAND)\n",
    "\n",
    "# Plot all station positions\n",
    "for date, clusters in validated_associations.items():\n",
    "    for cluster in clusters:\n",
    "        for station, _ in cluster['detections']:\n",
    "            lat, lon = station.get_pos()\n",
    "            ax.plot(lon, lat, 'bo', markersize=4, label='Station' if 'Station' not in ax.get_legend_handles_labels()[1] else \"\")  # blue dots\n",
    "\n",
    "# Plot all event source points\n",
    "for date, clusters in validated_associations.items():\n",
    "    for cluster in clusters:\n",
    "        lat, lon = cluster['source_point']\n",
    "        ax.plot(lon, lat, 'r*', markersize=12, label='Event' if 'Event' not in ax.get_legend_handles_labels()[1] else \"\")  # red stars\n",
    "\n",
    "# Beautify\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Validated Associations: Stations and Event Sources\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ],
   "id": "bf6575b8c656c2dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "validated_associations",
   "id": "4a236bb6d6b56ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rms_errors = []\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for date, clusters in validated_associations.items():\n",
    "    for cluster in clusters:\n",
    "        lat, lon = cluster['source_point']\n",
    "        lats.append(lat)\n",
    "        lons.append(lon)\n",
    "        rms_errors.append(cluster['rms_error'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sc = ax.scatter(lons, lats, c=rms_errors, cmap='viridis', s=100, edgecolors='k')\n",
    "plt.colorbar(sc, label=\"RMS Error (s)\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Event Locations Colored by RMS Error\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "3998c01075014ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "records = []\n",
    "\n",
    "\n",
    "for _ ,associations in validated_associations.items():\n",
    "    for association in associations:\n",
    "        lat, lon = association['source_point']\n",
    "        origin_time = association.get('origin_time', None)\n",
    "\n",
    "        if lat is not None and lon is not None and origin_time is not None:\n",
    "            records.append({\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon,\n",
    "                'OriginTime': origin_time.isoformat() if hasattr(origin_time, 'isoformat') else origin_time\n",
    "            })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Export to CSV\n",
    "csv_output_path = \"validated_associations.csv\"\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"Exported {len(df)} associations to {csv_output_path}\")\n"
   ],
   "id": "acab139cc528e0a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loop over each cluster (one per date)\n",
    "for date, clusters in validated_associations.items():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_title(f\"Cluster on {pd.to_datetime(date).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Track which stations we've already plotted\n",
    "    plotted_stations = set()\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        rms_error = cluster['rms_error']\n",
    "        cost = cluster['optimization_result'].cost  # Cost from least_squares\n",
    "        chi_square = cost / len(cluster['detections'])  # Approximate Chi-Square (normalized by number of detections)\n",
    "\n",
    "        # Plot stations\n",
    "        for station, detection_time in cluster['detections']:\n",
    "            station_id = id(station)\n",
    "            lat, lon = station.get_pos()\n",
    "            if station_id not in plotted_stations:\n",
    "                ax.plot(lon, lat, 'bo', markersize=6, label='Station' if 'Station' not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "                plotted_stations.add(station_id)\n",
    "\n",
    "        # Plot event with marker size or color depending on error metrics\n",
    "        event_lat, event_lon = cluster['source_point']\n",
    "        if rms_error < 2.0 and chi_square < 2.0:\n",
    "            color = 'g'  # Green = very good fit\n",
    "        elif rms_error < 5.0 and chi_square < 5.0:\n",
    "            color = 'y'  # Yellow = acceptable fit\n",
    "        else:\n",
    "            color = 'r'  # Red = high error\n",
    "\n",
    "        ax.plot(event_lon, event_lat, marker='*', color=color, markersize=12,\n",
    "                label='Event' if 'Event' not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "        # Annotate RMS error, Chi-Square, and Cost near the event\n",
    "        ax.text(event_lon + 0.02, event_lat + 0.02,\n",
    "                f\"RMS={rms_error:.2f}s\\nChi2={chi_square:.2f}\\nCost={cost:.2f}\",\n",
    "                fontsize=8, color=color)\n",
    "\n",
    "        # Optionally, draw dashed lines from stations to event\n",
    "        for station, _ in cluster['detections']:\n",
    "            station_lat, station_lon = station.get_pos()\n",
    "            ax.plot([event_lon, station_lon], [event_lat, station_lat], 'k--', linewidth=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ],
   "id": "b4f202f8199566bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = []\n",
    "\n",
    "for date, clusters in validated_associations.items():\n",
    "    for cluster in clusters:\n",
    "        rms_error = cluster['rms_error']\n",
    "        cost = cluster['optimization_result'].cost\n",
    "        chi_square = cost / len(cluster['detections'])  # Approximate Chi-Square\n",
    "        event_lat, event_lon = cluster['source_point']\n",
    "        origin_time = cluster['origin_time']\n",
    "\n",
    "        summary.append({\n",
    "            'Date': date,\n",
    "            'Latitude': event_lat,\n",
    "            'Longitude': event_lon,\n",
    "            'Origin Time': origin_time,\n",
    "            'RMS Error': rms_error,\n",
    "            'Chi-Square': chi_square,\n",
    "            'Cost': cost,\n",
    "            'Num Stations': cluster['num_stations']\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easy inspection\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df)\n"
   ],
   "id": "2284f381c97ac327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### optimized",
   "id": "4efc6b82ec3bcc49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "validate_associations_parallel_improved.py\n",
    "\n",
    "Parallel validation of seismic detection associations with periodic checkpointing.\n",
    "Includes enhanced error model with:\n",
    "- Clock drift error term\n",
    "- Picking error model\n",
    "- Distance-based weighting\n",
    "- Ray path deviation consideration\n",
    "- More robust weighted least squares implementation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from scipy.optimize import least_squares\n",
    "from pyproj import Geod\n",
    "from joblib import Parallel, delayed\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "ASSO_FILE = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "OUTPUT_DIR = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/validated/2018\"\n",
    "OUTPUT_BASENAME = \"s_-60-5,35-120,350,0.8,0.6\"\n",
    "BATCH_SIZE = 500            # checkpoint every N dates\n",
    "N_JOBS = os.cpu_count() - 1  # leave one core free\n",
    "GRID_LAT_BOUNDS = [-60, 5]\n",
    "GRID_LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "DEPTH = 1250               # meters\n",
    "SOUND_SPEED = 1480         # m/s, rough constant\n",
    "GTOL = 1e-4\n",
    "XTOL = 1e-4\n",
    "ISAS_PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "\n",
    "# Enhanced error model parameters\n",
    "PICKING_ERROR_BASE = 2      # Base picking error in seconds\n",
    "CLOCK_DRIFT_RATE = 1e-6         # Clock drift rate (ppm)\n",
    "DIST_WEIGHT_FACTOR = 0.8        # How much distance affects weighting (0-1)\n",
    "PATH_DEVIATION_FACTOR = 0.02    # How much potential ray path deviation increases with distance\n",
    "\n",
    "# Precompute grid lat/lon\n",
    "PTS_LAT = np.linspace(*GRID_LAT_BOUNDS, GRID_SIZE)\n",
    "PTS_LON = np.linspace(*GRID_LON_BOUNDS, GRID_SIZE)\n",
    "\n",
    "# Geod instance\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def grid_index_to_coord(indices):\n",
    "    i, j = indices\n",
    "    return [PTS_LAT[i], PTS_LON[j]]\n",
    "\n",
    "def compute_enhanced_travel_time(lat, lon, station_lat, station_lon, a=1.0, b=0.0, c=0.0):\n",
    "    \"\"\"Enhanced travel time calculation with improved error model:\n",
    "    TT = a * (distance/sound_speed) + b + c*distance\n",
    "\n",
    "    Parameters:\n",
    "    - a: scaling factor for sound speed (default 1.0)\n",
    "    - b: constant bias in seconds (default 0.0)\n",
    "    - c: distance-dependent bias (s/m) to account for ray path deviations\n",
    "\n",
    "    Returns:\n",
    "    - tt: estimated travel time\n",
    "    - err: estimated error in travel time\n",
    "    - dist_m: distance in meters\n",
    "    \"\"\"\n",
    "    _, _, dist_m = geod.inv(lon, lat, station_lon, station_lat)\n",
    "\n",
    "    # Base travel time calculation\n",
    "    base_tt = dist_m / SOUND_SPEED\n",
    "\n",
    "    # Apply model parameters\n",
    "    tt = a * base_tt + b + c * (dist_m/1000)  # c parameter is applied per km\n",
    "\n",
    "    # Error modeling with multiple components\n",
    "    picking_err = PICKING_ERROR_BASE\n",
    "    deviation_err = PATH_DEVIATION_FACTOR * (dist_m/1000)  # Increases with distance\n",
    "    clock_err = CLOCK_DRIFT_RATE * base_tt  # Clock drift increases with time\n",
    "\n",
    "    # Combined error (add in quadrature for independent error sources)\n",
    "    total_err = np.sqrt(picking_err**2 + deviation_err**2 + clock_err**2)\n",
    "\n",
    "    return tt, total_err, dist_m\n",
    "\n",
    "def calculate_station_weights(distances):\n",
    "    \"\"\"Calculate weights for stations based on their distances.\n",
    "    Closer stations get higher weights, but maintains influence from distant stations.\n",
    "\n",
    "    Uses a soft distance-weighting approach to avoid extreme down-weighting.\n",
    "    \"\"\"\n",
    "    if len(distances) == 0:\n",
    "        return []\n",
    "\n",
    "    # Normalize distances to [0,1] range\n",
    "    max_dist = max(distances)\n",
    "    if max_dist == 0:\n",
    "        return [1.0] * len(distances)\n",
    "\n",
    "    norm_distances = np.array(distances) / max_dist\n",
    "\n",
    "    # Apply distance weight factor (higher means more aggressive distance weighting)\n",
    "    weights = 1.0 / (DIST_WEIGHT_FACTOR * norm_distances + (1.0 - DIST_WEIGHT_FACTOR))\n",
    "\n",
    "    # Normalize weights to sum to number of stations for proper scaling\n",
    "    weights = weights * (len(weights) / weights.sum())\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def process_date(date, associations_list):\n",
    "    \"\"\"Process one date (cluster): refine source location & origin time with improved error model.\"\"\"\n",
    "    month = pd.to_datetime(date).month\n",
    "    validated = []\n",
    "\n",
    "    # Create simplified associations list to avoid serialization issues\n",
    "    simplified_associations = []\n",
    "    for detections, valid_points in associations_list:\n",
    "        simple_detections = []\n",
    "        for station_obj, det_time in detections:\n",
    "            # Extract only necessary data from station_obj\n",
    "            lat, lon = station_obj.get_pos()\n",
    "            simple_detections.append(((lat, lon), det_time))\n",
    "        simplified_associations.append((simple_detections, valid_points))\n",
    "\n",
    "    for detections, valid_points in simplified_associations:\n",
    "        # Skip tiny clusters\n",
    "        if len(detections) < 6:\n",
    "            continue\n",
    "\n",
    "        # Build refined detections & station positions\n",
    "        station_positions = [pos for pos, _ in detections]\n",
    "        detection_times = [t for _, t in detections]\n",
    "        ref_time = min(detection_times)\n",
    "        detection_secs = [(pos, (t - ref_time).total_seconds()) for pos, t in detections]\n",
    "\n",
    "        # Initial guess & bounds from valid_points\n",
    "        if len(valid_points) > 0:\n",
    "            coords = np.array([grid_index_to_coord(tuple(p)) for p in valid_points])\n",
    "            lat0, lon0 = coords.mean(axis=0)\n",
    "            margin = 0.5\n",
    "            lat_min, lat_max = coords[:,0].min()-margin, coords[:,0].max()+margin\n",
    "            lon_min, lon_max = coords[:,1].min()-margin, coords[:,1].max()+margin\n",
    "        else:\n",
    "            arr = np.array(station_positions)\n",
    "            lat0, lon0 = arr[:,0].mean(), arr[:,1].mean()\n",
    "            lat_min, lat_max = arr[:,0].min()-0.5, arr[:,0].max()+0.5\n",
    "            lon_min, lon_max = arr[:,1].min()-0.5, arr[:,1].max()+0.5\n",
    "\n",
    "        # Nearest station gives t0 guess\n",
    "        dists = np.array([geod.inv(lon0, lat0, lat, lon)[2] for lat, lon in station_positions])\n",
    "        nearest_idx = dists.argmin()\n",
    "        t0_offset_guess = - (dists[nearest_idx] / SOUND_SPEED)\n",
    "\n",
    "        # Adding parameter for enhanced model: a, b, c\n",
    "        # a: sound speed scaling factor\n",
    "        # b: constant time bias (clock synchronization)\n",
    "        # c: distance-dependent bias (for ray path deviations)\n",
    "        x0 = [lat0, lon0, t0_offset_guess, 0.0, 0.0]\n",
    "        bounds = (\n",
    "            [lat_min, lon_min, t0_offset_guess-1200, -50, -0.05],  # Lower bounds\n",
    "            [lat_max, lon_max, t0_offset_guess+1200, 50, 0.05]     # Upper bounds\n",
    "        )\n",
    "\n",
    "        # Pre-calculate station weights based on geometry\n",
    "        station_weights = calculate_station_weights(dists.tolist())\n",
    "\n",
    "        # Enhanced residual function with improved error model and weighting\n",
    "        def residual(params):\n",
    "            lat, lon, t0_off, b, c = params\n",
    "            errs = []\n",
    "            weights = []\n",
    "\n",
    "            for idx, (station_pos, sec) in enumerate(detection_secs):\n",
    "                slat, slon = station_pos\n",
    "                tt, terr, dist = compute_enhanced_travel_time(\n",
    "                    round(lat,3), round(lon,3),\n",
    "                    round(slat,3), round(slon,3),\n",
    "                    b=b, c=c  # Use enhanced model parameters\n",
    "                )\n",
    "\n",
    "                # Predicted arrival time\n",
    "                pred = t0_off + tt\n",
    "\n",
    "                # Raw error\n",
    "                raw_err = sec - pred\n",
    "\n",
    "                # Combined weight (error-based and station-based)\n",
    "                # Scale by station_weights to account for geometry\n",
    "                combined_weight = station_weights[idx] / (terr + 0.1)\n",
    "\n",
    "                errs.append(raw_err)\n",
    "                weights.append(combined_weight)\n",
    "\n",
    "            # Convert to arrays\n",
    "            errs = np.array(errs)\n",
    "            weights = np.array(weights)\n",
    "\n",
    "            # Normalize weights for stability\n",
    "            weights = weights / weights.mean()\n",
    "\n",
    "            # Return weighted residuals\n",
    "            return errs * weights\n",
    "\n",
    "        # Solve with robust method\n",
    "        try:\n",
    "            res = least_squares(\n",
    "                residual, x0, bounds=bounds,\n",
    "                method='trf', loss='huber',  # Huber loss is more robust to outliers\n",
    "                f_scale=1.0,                 # Scale parameter for loss function\n",
    "                gtol=GTOL, xtol=XTOL, verbose=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed for date {date}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        lat_f, lon_f, t0_off_f, b_f, c_f = res.x\n",
    "        origin_time = ref_time + timedelta(seconds=t0_off_f)\n",
    "\n",
    "        # Calculate detailed residuals for analysis\n",
    "        detailed_residuals = []\n",
    "        for idx, (station_pos, sec) in enumerate(detection_secs):\n",
    "            slat, slon = station_pos\n",
    "            tt, terr, dist = compute_enhanced_travel_time(\n",
    "                round(lat_f,3), round(lon_f,3),\n",
    "                round(slat,3), round(slon,3),\n",
    "                b=b_f, c=c_f\n",
    "            )\n",
    "            pred = t0_off_f + tt\n",
    "            raw_err = sec - pred\n",
    "            detailed_residuals.append({\n",
    "                'station_pos': station_pos,\n",
    "                'observed_time': sec,\n",
    "                'predicted_time': pred,\n",
    "                'residual': raw_err,\n",
    "                'distance': dist,\n",
    "                'travel_time': tt,\n",
    "                'estimated_error': terr,\n",
    "                'station_weight': station_weights[idx]\n",
    "            })\n",
    "\n",
    "        # Calculate weighted RMS error\n",
    "        squared_errs = np.array([r['residual']**2 for r in detailed_residuals])\n",
    "        weights = np.array([r['station_weight'] for r in detailed_residuals])\n",
    "        norm_weights = weights / weights.sum()\n",
    "        weighted_rms = np.sqrt(np.sum(squared_errs * norm_weights))\n",
    "\n",
    "        # Calculate standard RMS for comparison\n",
    "        standard_rms = np.sqrt(np.mean(squared_errs))\n",
    "\n",
    "        # Store detailed optimization results for statistical analysis\n",
    "        optimization_details = {\n",
    "            'x': res.x.tolist(),            # Final parameters\n",
    "            'cost': res.cost,               # Final cost function value\n",
    "            'fun': res.fun.tolist(),        # Final residuals\n",
    "            'nfev': res.nfev,               # Number of function evaluations\n",
    "            'njev': res.njev,               # Number of Jacobian evaluations\n",
    "            'status': res.status,           # Termination status\n",
    "            'message': res.message,         # Termination message\n",
    "            'success': res.success          # Boolean flag indicating success\n",
    "        }\n",
    "\n",
    "        validated.append({\n",
    "            'station_positions': station_positions,\n",
    "            'detection_times': detection_times,\n",
    "            'source_point': (lat_f, lon_f),\n",
    "            'origin_time': origin_time,\n",
    "            'model_params': {'b': b_f, 'c': c_f},  # Save enhanced model parameters\n",
    "            'weighted_rms_error': weighted_rms,\n",
    "            'standard_rms_error': standard_rms,\n",
    "            'num_stations': len(detection_secs),\n",
    "            'optimization_success': res.success,\n",
    "            'optimization_details': optimization_details,\n",
    "            'detailed_residuals': detailed_residuals  # Add per-station residuals\n",
    "        })\n",
    "\n",
    "    return date, validated\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load input\n",
    "    associations = np.load(ASSO_FILE, allow_pickle=True).item()\n",
    "    items = list(associations.items())\n",
    "    validated_associations = {}\n",
    "\n",
    "    # Process in batches with checkpoints\n",
    "    for i in range(0, len(items), BATCH_SIZE):\n",
    "        batch = items[i:i+BATCH_SIZE]\n",
    "        results = Parallel(n_jobs=N_JOBS, verbose=5)(\n",
    "            delayed(process_date)(date, lst) for date, lst in batch\n",
    "        )\n",
    "        for date, val in results:\n",
    "            validated_associations[date] = val\n",
    "\n",
    "        # Checkpoint\n",
    "        chkpt_path = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            f\"{OUTPUT_BASENAME}_partial_{i+BATCH_SIZE}.npy\"\n",
    "        )\n",
    "        np.save(chkpt_path, validated_associations)\n",
    "        print(f\"✔ Checkpoint saved for first {i+BATCH_SIZE} dates → {chkpt_path}\")\n",
    "\n",
    "    # Final save\n",
    "    final_path = os.path.join(OUTPUT_DIR, f\"{OUTPUT_BASENAME}_final.npy\")\n",
    "    np.save(final_path, validated_associations)\n",
    "    print(f\"🎯 All done! Final results saved to {final_path}\")\n",
    "\n",
    "    # Save a separate file with just the statistical analysis data\n",
    "    stats_data = {}\n",
    "    for date, events in validated_associations.items():\n",
    "        stats_data[date] = [{\n",
    "            'source_point': event['source_point'],\n",
    "            'model_params': event['model_params'],\n",
    "            'weighted_rms_error': event['weighted_rms_error'],\n",
    "            'standard_rms_error': event['standard_rms_error'],\n",
    "            'num_stations': event['num_stations'],\n",
    "            'optimization_details': event['optimization_details'],\n",
    "            'detailed_residuals': event['detailed_residuals']\n",
    "        } for event in events]\n",
    "\n",
    "    stats_path = os.path.join(OUTPUT_DIR, f\"{OUTPUT_BASENAME}_stats.npy\")\n",
    "    np.save(stats_path, stats_data)\n",
    "    print(f\"📊 Statistical analysis data saved to {stats_path}\")\n",
    "\n",
    "    # Generate residual analysis visualizations\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "        pdf_path = os.path.join(OUTPUT_DIR, f\"{OUTPUT_BASENAME}_residual_analysis.pdf\")\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            # Get a sample of events for analysis (up to 50)\n",
    "            all_events = []\n",
    "            for date_events in validated_associations.values():\n",
    "                all_events.extend(date_events)\n",
    "\n",
    "            sample_size = min(50, len(all_events))\n",
    "            sample_events = all_events[:sample_size] if sample_size > 0 else []\n",
    "\n",
    "            if sample_events:\n",
    "                # 1. Plot residuals vs distance\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                for idx, event in enumerate(sample_events):\n",
    "                    distances = [r['distance']/1000 for r in event['detailed_residuals']]  # km\n",
    "                    residuals = [r['residual'] for r in event['detailed_residuals']]\n",
    "                    plt.scatter(distances, residuals, alpha=0.5, label=f\"Event {idx}\" if idx < 5 else None)\n",
    "\n",
    "                plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "                plt.xlabel('Distance (km)')\n",
    "                plt.ylabel('Residual (s)')\n",
    "                plt.title('Residuals vs Distance')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                if len(sample_events) <= 5:\n",
    "                    plt.legend()\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "\n",
    "                # 2. Plot RMS vs number of stations\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                num_stations = [event['num_stations'] for event in sample_events]\n",
    "                weighted_rms = [event['weighted_rms_error'] for event in sample_events]\n",
    "                standard_rms = [event['standard_rms_error'] for event in sample_events]\n",
    "\n",
    "                plt.scatter(num_stations, weighted_rms, label='Weighted RMS', alpha=0.7)\n",
    "                plt.scatter(num_stations, standard_rms, label='Standard RMS', alpha=0.5, marker='x')\n",
    "                plt.xlabel('Number of Stations')\n",
    "                plt.ylabel('RMS Error (s)')\n",
    "                plt.title('RMS Error vs Number of Stations')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.legend()\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "\n",
    "                # 3. Plot model parameter distributions\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "                a_values = [event['model_params']['a'] for event in sample_events]\n",
    "                b_values = [event['model_params']['b'] for event in sample_events]\n",
    "                c_values = [event['model_params']['c'] for event in sample_events]\n",
    "\n",
    "                axs[0].hist(a_values, bins=20, alpha=0.7)\n",
    "                axs[0].set_title('Sound Speed Factor (a)')\n",
    "                axs[0].axvline(x=1.0, color='r', linestyle='--')\n",
    "\n",
    "                axs[1].hist(b_values, bins=20, alpha=0.7)\n",
    "                axs[1].set_title('Time Bias (b) in seconds')\n",
    "                axs[1].axvline(x=0.0, color='r', linestyle='--')\n",
    "\n",
    "                axs[2].hist(c_values, bins=20, alpha=0.7)\n",
    "                axs[2].set_title('Distance-dependent Factor (c) in s/km')\n",
    "                axs[2].axvline(x=0.0, color='r', linestyle='--')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "\n",
    "        print(f\"📈 Residual analysis visualizations saved to {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't generate visualization: {str(e)}\")"
   ],
   "id": "eecdaf38911880a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/validated/2018/s_-60--12.4,35-100,350,0.8,0.5_partial_50.npy\"\n",
    "associations = np.load(path_asso, allow_pickle=True).item()"
   ],
   "id": "9872e009ba71c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "visualize_results.py\n",
    "\n",
    "Script pour visualiser et analyser les résultats des associations validées\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Chemin vers le fichier résultat\n",
    "RESULT_FILE = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/validated/2018/s_-60-5,35-120,350,0.8,0.6_final.npy\"\n",
    "# Pour visualiser un checkpoint partiel:\n",
    "# RESULT_FILE = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/validated/2018/s_-60--12.4,35-100,350,0.8,0.5_partial_50.npy\"\n",
    "\n",
    "def load_and_summarize():\n",
    "    \"\"\"Charge et résume les données\"\"\"\n",
    "    data = np.load(RESULT_FILE, allow_pickle=True).item()\n",
    "\n",
    "    print(f\"Loaded data with {len(data)} dates\")\n",
    "\n",
    "    # Statistiques générales\n",
    "    total_events = sum(len(events) for events in data.values())\n",
    "    print(f\"Total validated events: {total_events}\")\n",
    "\n",
    "    # Résumé par date\n",
    "    date_summary = {date: len(events) for date, events in data.items()}\n",
    "    dates = sorted(date_summary.keys())\n",
    "\n",
    "    return data, date_summary, dates\n",
    "\n",
    "def plot_events_by_date(date_summary, dates):\n",
    "    \"\"\"Affiche le nombre d'événements par date\"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(dates, [date_summary[d] for d in dates])\n",
    "    plt.title(\"Nombre d'événements validés par date\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Nombre d'événements\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"events_by_date.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_event_locations(data):\n",
    "    \"\"\"Affiche la distribution spatiale des événements\"\"\"\n",
    "    # Extraire les coordonnées de tous les événements\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "\n",
    "    for date, events in data.items():\n",
    "        for event in events:\n",
    "            lat, lon = event['source_point']\n",
    "            all_lats.append(lat)\n",
    "            all_lons.append(lon)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(all_lons, all_lats, s=10, alpha=0.6)\n",
    "    plt.title(\"Distribution spatiale des événements\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.grid(True)\n",
    "    plt.colorbar(label=\"Nombre d'événements\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"event_locations.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_rms_error_histogram(data):\n",
    "    \"\"\"Affiche l'histogramme des erreurs RMS\"\"\"\n",
    "    all_errors = []\n",
    "\n",
    "    for date, events in data.items():\n",
    "        for event in events:\n",
    "            all_errors.append(event['weighted_rms_error'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(all_errors, bins=50)\n",
    "    plt.title(\"Distribution des erreurs RMS\")\n",
    "    plt.xlabel(\"Erreur RMS\")\n",
    "    plt.ylabel(\"Nombre d'événements\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rms_error_hist.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Statistiques des erreurs RMS:\")\n",
    "    print(f\"  Min: {min(all_errors):.4f}\")\n",
    "    print(f\"  Max: {max(all_errors):.4f}\")\n",
    "    print(f\"  Moyenne: {sum(all_errors)/len(all_errors):.4f}\")\n",
    "    print(f\"  Médiane: {sorted(all_errors)[len(all_errors)//2]:.4f}\")\n",
    "\n",
    "def plot_stations_per_event(data):\n",
    "    \"\"\"Affiche le nombre de stations par événement\"\"\"\n",
    "    stations_count = []\n",
    "\n",
    "    for date, events in data.items():\n",
    "        for event in events:\n",
    "            stations_count.append(event['num_stations'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(stations_count, bins=range(min(stations_count), max(stations_count)+2), align='left')\n",
    "    plt.title(\"Nombre de stations par événement\")\n",
    "    plt.xlabel(\"Nombre de stations\")\n",
    "    plt.ylabel(\"Nombre d'événements\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"stations_per_event.png\")\n",
    "    plt.show()\n",
    "\n",
    "def export_to_csv(data):\n",
    "    \"\"\"Exporte les résultats dans un fichier CSV\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for date, events in data.items():\n",
    "        for idx, event in enumerate(events):\n",
    "            lat, lon = event['source_point']\n",
    "            rows.append({\n",
    "                'date': date,\n",
    "                'event_index': idx,\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'origin_time': event['origin_time'],\n",
    "                'rms_error': event['weighted_rms_error'],\n",
    "                'num_stations': event['num_stations']\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.splitext(RESULT_FILE)[0] + '.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results exported to {csv_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data, date_summary, dates = load_and_summarize()\n",
    "\n",
    "    # Générer les visualisations\n",
    "    plot_events_by_date(date_summary, dates)\n",
    "    plot_event_locations(data)\n",
    "    plot_rms_error_histogram(data)\n",
    "    plot_stations_per_event(data)\n",
    "\n",
    "    # Exporter vers CSV\n",
    "    df = export_to_csv(data)\n",
    "\n",
    "    # Afficher les 10 premiers événements\n",
    "    print(\"\\nAperçu des 10 premiers événements:\")\n",
    "    print(df.head(10))"
   ],
   "id": "c68e77de93c761ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
