{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initilaization",
   "id": "e846eb5a6b9675f9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T14:14:03.769888Z",
     "start_time": "2025-04-18T14:14:03.115973Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# from notebooks.demo.Open_ISAS_Grid import travel_time\n",
    "#\n",
    "# from notebooks.demo.dev_asso_manual import model\n",
    "from utils.detection.association import load_detections\n",
    "from utils.detection.association import compute_grids\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association import compute_candidates, association_is_new, update_valid_grid, update_results"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:14:03.819257Z",
     "start_time": "2025-04-18T14:14:03.816979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# paths\n",
    "CATALOG_PATH = \"/media/rsafran/CORSAIR/OHASISBIO/recensement_stations_OHASISBIO_RS.csv\"\n",
    "# DETECTIONS_DIR = \"/media/rsafran/CORSAIR/temp/2018\"\n",
    "DETECTIONS_DIR = \"/home/rsafran/Bureau/tissnet/2018\"\n",
    "ASSOCIATION_OUTPUT_DIR = \"../../../data/detection/association\"\n",
    "\n",
    "# Detections loading parameters\n",
    "RELOAD_DETECTIONS = False # if False, load files called \"detections.npy\" and \"detections_merged.npy\" containing everything instead of the raw detection output. Leave at True by default\n",
    "MIN_P_TISSNET_PRIMARY = 0.8  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.6  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 10 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "REQ_CLOSEST_STATIONS = 0  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "\n",
    "# sound model definition\n",
    "SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)\n",
    "\n",
    "# association running parameters\n",
    "RUN_ASSOCIATION = True # set to False to load previous associations without processing it again\n",
    "SAVE_PATH_ROOT = None  # change this to save the grids as figures, leave at None by default\n",
    "NCPUS = 20  # nb of CPUs used"
   ],
   "id": "14941421b34ecb51",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:14:03.958350Z",
     "start_time": "2025-04-18T14:14:03.866008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "if RELOAD_DETECTIONS:\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*\") if Path(f).is_file()]\n",
    "    DETECTIONS, DETECTIONS_MERGED = load_detections(det_files, STATIONS, DETECTIONS_DIR, MIN_P_TISSNET_PRIMARY, MIN_P_TISSNET_SECONDARY, MERGE_DELTA)\n",
    "else:\n",
    "    DETECTIONS = np.load(f\"{DETECTIONS_DIR}/cache/detections.npy\", allow_pickle=True).item()\n",
    "    DETECTIONS_MERGED = np.load(f\"{DETECTIONS_DIR}/cache/detections_merged.npy\", allow_pickle=True)\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}"
   ],
   "id": "6f1c99b9c852b9ce",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:14:03.967694Z",
     "start_time": "2025-04-18T14:14:03.966052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(vars(STATIONS[0]))\n",
    "print(STATIONS[0].depth)\n",
    "print(STATIONS[0].name)\n",
    "print(STATIONS[0].get_pos(include_depth=True))\n"
   ],
   "id": "f886cb92bff74eb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'manager': None, 'path': '/media/rsafran/CORSAIR/OHASISBIO/2018/ELAN', 'name': 'ELAN', 'date_start': datetime.datetime(2018, 1, 16, 17, 34, 34), 'date_end': datetime.datetime(2019, 1, 23, 7, 17, 40), 'lat': -56.4602, 'lon': 62.976, 'dataset': '2018', 'other_kwargs': {'dataset': 2018, 'station_name': 'ELAN', 'date_start': Timestamp('2018-01-16 17:34:34'), 'date_end': Timestamp('2019-01-23 07:17:40'), 'lat': -56.4602, 'lon': 62.976, 'hydrophone_serial': 580014.0, 'sensitivity': -163.8, 'depth': 1020, 'clock_drift_ppm': -0.0222, 'gps_sync ': 'ok', 'batterie': nan, 'path': '/media/rsafran/CORSAIR/OHASISBIO/2018/ELAN'}}\n",
      "0.0\n",
      "ELAN\n",
      "[-56.4602, 62.976, 0.0]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:14:09.856921Z",
     "start_time": "2025-04-18T14:14:04.017554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350  # number of points along each axis\n",
    "\n",
    "(PTS_LAT, PTS_LON, STATION_MAX_TRAVEL_TIME, GRID_STATION_TRAVEL_TIME,\n",
    " GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE) = compute_grids(LAT_BOUNDS, LON_BOUNDS, GRID_SIZE, SOUND_MODEL, STATIONS, pick_uncertainty=2, sound_speed_uncertainty=1)"
   ],
   "id": "6181d3b2e22452fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid tolerance of 11.85s\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Association",
   "id": "5d38dc3a4e18eb74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"starting association\")\n",
    "\n",
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "\n",
    "association_hashlist = set()\n",
    "associations = {}\n",
    "\n",
    "def process_detection(arg):\n",
    "    detection, local_association_hashlist = arg\n",
    "    local_association = {}\n",
    "    date1, p1, s1 = detection\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1 + datetime.timedelta(seconds=4*GRID_TOLERANCE) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - datetime.timedelta(seconds=4*GRID_TOLERANCE) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([STATION_MAX_TRAVEL_TIME[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:date1}\n",
    "    candidates =  compute_candidates(other_stations, current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    if len(other_stations) < 2:\n",
    "        return local_association, local_association_hashlist\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, STATION_MAX_TRAVEL_TIME, MERGE_DELTA_S)\n",
    "        for idx in candidates[station]:\n",
    "            date, p = DETECTIONS[station][idx]\n",
    "            if not association_is_new(current_association, date, local_association_hashlist):\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE, save_path, LON_BOUNDS, LAT_BOUNDS)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date)\n",
    "\n",
    "                if len(current_association) > 2:\n",
    "                    update_results(date1, current_association, valid_points_new, local_association, GRID_STATION_COUPLE_TRAVEL_TIME)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, save_path)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations, save_path)\n",
    "        return\n",
    "    backtrack(0, current_association, None, associations, save_path=save_path)\n",
    "    return local_association, local_association_hashlist\n",
    "\n",
    "# main part\n",
    "if RUN_ASSOCIATION:\n",
    "    try:\n",
    "        with ProcessPoolExecutor(NCPUS) as executor:\n",
    "            futures = {executor.submit(process_detection, (det, association_hashlist)): det for det in DETECTIONS_MERGED}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                local_association, local_association_hashlist = future.result()\n",
    "                association_hashlist = association_hashlist.union(local_association_hashlist)\n",
    "                associations = associations | local_association\n",
    "    finally:\n",
    "        # save the associations no matter if the execution stopped properly\n",
    "        np.save(OUT_FILE, associations)"
   ],
   "id": "cd1998dcdda36f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## plotting density map",
   "id": "631e4ced2608005b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OUT_DIR = f\"{ASSOCIATION_OUTPUT_DIR}/grids/{DETECTIONS_DIR_NAME}\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "OUT_FILE = f\"{OUT_DIR}/s_{LAT_BOUNDS[0]}-{LAT_BOUNDS[1]},{LON_BOUNDS[0]}-{LON_BOUNDS[1]},{GRID_SIZE},{MIN_P_TISSNET_PRIMARY},{MIN_P_TISSNET_SECONDARY}.npy\".replace(\" \",\"\")\n",
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 4\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) < MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[-1], LAT_BOUNDS[0], LAT_BOUNDS[-1])\n",
    "im = plt.imshow(valid[::-1], aspect=1, cmap=\"inferno\", extent=extent, interpolation=None)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "for s in STATIONS:\n",
    "    p = s.get_pos()\n",
    "\n",
    "    if p[0] > LAT_BOUNDS[1] or p[0] < LAT_BOUNDS[0] or p[1] > LON_BOUNDS[1] or p[1] < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    plt.plot(p[1], p[0], 'wx', alpha=0.75)\n",
    "    plt.annotate(s.name, xy=(p[1], p[0]), xytext=(p[1]-(LON_BOUNDS[1]-LON_BOUNDS[0])/15, p[0]+(LAT_BOUNDS[1]-LAT_BOUNDS[0])/100), textcoords=\"data\", color='w', alpha=0.9)"
   ],
   "id": "3c08dea4155fcf3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 4\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for f in tqdm(glob2.glob(f\"{OUT_FILE[:-4]}*.npy\")):\n",
    "    associations = np.load(f, allow_pickle=True).item()\n",
    "    for date, associations_ in associations.items():\n",
    "        for (detections, valid_points) in associations_:\n",
    "            if len(detections) < MIN_SIZE:\n",
    "                continue\n",
    "            for i, j in valid_points:\n",
    "                valid[i,j] += 1\n",
    "\n",
    "# Create a figure with cartopy's PlateCarree projection\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "# Set the extent of the map (min_lon, max_lon, min_lat, max_lat)\n",
    "ax.set_extent([LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1]], crs=projection)\n",
    "\n",
    "# Add natural features: land, ocean, and coastlines.\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "# ax.add_feature(cfeature.OCEAN, facecolor='lightblue', zorder=2)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "\n",
    "# Plot the georeferenced image.\n",
    "# Set a lower zorder (e.g., 1) so that the map features drawn with higher zorders remain visible.\n",
    "# Adjust alpha to add a bit of transparency if desired.\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1])\n",
    "im = ax.imshow(valid[::-1],\n",
    "               cmap=\"tab20c\",\n",
    "               extent=extent,\n",
    "               interpolation=\"nearest\",\n",
    "               origin='upper',\n",
    "               transform=projection,\n",
    "               zorder=1,\n",
    "               alpha=1, vmax=500)\n",
    "\n",
    "# Add a colorbar for the image.\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.05)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "# Plot station markers and add annotations using the axes methods.\n",
    "for s in STATIONS:\n",
    "    lat, lon = s.get_pos()\n",
    "    if lat > LAT_BOUNDS[1] or lat < LAT_BOUNDS[0] or lon > LON_BOUNDS[1] or lon < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    # Plot a marker with a higher zorder so it's on top of the image\n",
    "    ax.plot(lon, lat, 'wx', alpha=0.75, markersize=8, transform=projection, zorder=4)\n",
    "    ax.text(lon - (LON_BOUNDS[1] - LON_BOUNDS[0]) / 15,\n",
    "            lat + (LAT_BOUNDS[1] - LAT_BOUNDS[0]) / 100,\n",
    "            s.name,\n",
    "            color='white',\n",
    "            alpha=0.9,\n",
    "            transform=projection,\n",
    "            zorder=4)\n",
    "\n",
    "plt.title(\"Association Data with Land and Sea\")\n",
    "plt.show()\n"
   ],
   "id": "fe57088139aca4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60--12.4,35-100,350,0.8,0.5.npy\"\n",
    "associations = np.load(path_asso, allow_pickle=True).item()"
   ],
   "id": "c41d7f16931e67dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## To Dataframe",
   "id": "b3c31922b025b430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved associations file.\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "\n",
    "# Load associations (allow_pickle=True because the file was saved with pickled Python objects).\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "# Flatten associations:\n",
    "# For each association key and each candidate association (tuple) in its list,\n",
    "# we build a row with the association key, candidate number, detection info, and grid info.\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert the key (datetime) to a string, if needed.\n",
    "    assoc_key_str = str(assoc_key)\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        # candidate is expected to be a tuple: (detections_array, grids_array)\n",
    "        detections_array, grids_array = candidate\n",
    "\n",
    "        # Convert detections array into a list of tuples\n",
    "        # or a string representation if you want a summary.\n",
    "        # For instance, each row of 'detections_array' is [station, datetime]\n",
    "        detection_list = []\n",
    "        for row in detections_array:\n",
    "            # row[0] is typically the station identifier and row[1] is the detection datetime.\n",
    "            detection_list.append((row[0], row[1]))\n",
    "\n",
    "        # Similarly, convert the grid indices array to a list of lists (or string)\n",
    "        grid_list = grids_array.tolist()\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"association_key\": assoc_key_str,\n",
    "            \"candidate_index\": i,\n",
    "            \"detections\": detection_list,\n",
    "            \"grids\": grid_list\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the flattened data.\n",
    "df_associations = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame.\n",
    "# print(df_associations.head())\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file.\n",
    "df_associations.to_csv(\"associations_dataframe.csv\", index=False)\n"
   ],
   "id": "bf3048def0e6b039",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered  = df_associations[df_associations['detections'].apply(len) > 6]",
   "id": "8ad4f1af89493b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_associations",
   "id": "6b12c53ca05aac22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid = np.zeros((GRID_SIZE,GRID_SIZE))\n",
    "\n",
    "MIN_SIZE = 5\n",
    "\n",
    "# load every npy file in the output directory and create a grid containing associations with cardinal >= 4\n",
    "for grid in df_filtered['grids']:\n",
    "    for valid_points in grid:\n",
    "        valid[valid_points[0], valid_points[1]] += 1\n",
    "\n",
    "# Create a figure with cartopy's PlateCarree projection\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': projection})\n",
    "\n",
    "# Set the extent of the map (min_lon, max_lon, min_lat, max_lat)\n",
    "ax.set_extent([LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1]], crs=projection)\n",
    "\n",
    "# Add natural features: land, ocean, and coastlines.\n",
    "# These features will be drawn on top if the image is behind.\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', zorder=2)\n",
    "# ax.add_feature(cfeature.OCEAN, facecolor='lightblue', zorder=2)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1, zorder=3)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='black', zorder=3)\n",
    "\n",
    "# Plot the georeferenced image.\n",
    "# Set a lower zorder (e.g., 1) so that the map features drawn with higher zorders remain visible.\n",
    "# Adjust alpha to add a bit of transparency if desired.\n",
    "extent = (LON_BOUNDS[0], LON_BOUNDS[1], LAT_BOUNDS[0], LAT_BOUNDS[1])\n",
    "im = ax.imshow(valid[::-1],\n",
    "               cmap=\"tab20c\",\n",
    "               extent=extent,\n",
    "               interpolation=\"nearest\",\n",
    "               origin='upper',\n",
    "               transform=projection,\n",
    "               zorder=1,\n",
    "               alpha=1, vmax=5)\n",
    "\n",
    "# Add a colorbar for the image.\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical', pad=0.05)\n",
    "cbar.set_label('Nb of associations')\n",
    "\n",
    "# Plot station markers and add annotations using the axes methods.\n",
    "for s in STATIONS:\n",
    "    lat, lon = s.get_pos()\n",
    "    if lat > LAT_BOUNDS[1] or lat < LAT_BOUNDS[0] or lon > LON_BOUNDS[1] or lon < LON_BOUNDS[0]:\n",
    "        print(f\"Station {s.name} out of bounds\")\n",
    "        continue\n",
    "    # Plot a marker with a higher zorder so it's on top of the image\n",
    "    ax.plot(lon, lat, 'wx', alpha=0.75, markersize=8, transform=projection, zorder=4)\n",
    "    ax.text(lon - (LON_BOUNDS[1] - LON_BOUNDS[0]) / 15,\n",
    "            lat + (LAT_BOUNDS[1] - LAT_BOUNDS[0]) / 100,\n",
    "            s.name,\n",
    "            color='white',\n",
    "            alpha=0.9,\n",
    "            transform=projection,\n",
    "            zorder=4)\n",
    "\n",
    "plt.title(\"Association Data min {}\".format(MIN_SIZE))\n",
    "plt.show()\n"
   ],
   "id": "c79a040cc7ac51ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration matching your original association parameters\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "\n",
    "# Calculate grid coordinates (replicating your original grid setup)\n",
    "PTS_LAT = np.linspace(LAT_BOUNDS[0], LAT_BOUNDS[1], GRID_SIZE)\n",
    "PTS_LON = np.linspace(LON_BOUNDS[0], LON_BOUNDS[1], GRID_SIZE)\n",
    "\n",
    "def grid_index_to_coords(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    return [(PTS_LAT[i], PTS_LON[j]) for i, j in indices]\n",
    "\n",
    "# Load associations\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert numpy datetime64 to Python datetime\n",
    "    event_time = pd.to_datetime(assoc_key).to_pydatetime()\n",
    "\n",
    "    for candidate_id, (detections_array, grids_array) in enumerate(candidates):\n",
    "        # Process detections\n",
    "        detections = []\n",
    "        for station, dt_np64 in detections_array:\n",
    "            dt = pd.to_datetime(dt_np64).to_pydatetime()\n",
    "            detections.append({\n",
    "                \"station\": station,\n",
    "                \"detection_time\": dt,\n",
    "                \"travel_time\": (event_time - dt).total_seconds()\n",
    "            })\n",
    "\n",
    "        # Process grid points\n",
    "        grid_points = grid_index_to_coords(grids_array)\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"event_time\": event_time,\n",
    "            \"candidate_id\": candidate_id,\n",
    "            \"num_stations\": len(detections),\n",
    "            \"stations\": [d[\"station\"] for d in detections],\n",
    "            \"detection_times\": [d[\"detection_time\"] for d in detections],\n",
    "            \"travel_times\": [d[\"travel_time\"] for d in detections],\n",
    "            \"grid_points\": grid_points,\n",
    "            \"num_grid_points\": len(grid_points),\n",
    "            \"median_lat\": np.median([p[0] for p in grid_points]),\n",
    "            \"median_lon\": np.median([p[1] for p in grid_points])\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Add duration statistics\n",
    "df[\"detection_time_span\"] = df[\"detection_times\"].apply(\n",
    "    lambda x: (max(x) - min(x)).total_seconds()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(df)} associations with:\")\n",
    "print(f\"- Median stations per event: {df['num_stations'].median()}\")\n",
    "print(f\"- Median grid points per event: {df['num_grid_points'].median()}\")\n",
    "\n",
    "# Save to CSV with timestamp\n",
    "output_csv = f\"associations_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nSaved to {output_csv}\")"
   ],
   "id": "f3c2fff2b6d68922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter high-confidence events\n",
    "strong_events = df[(df.num_grid_points >= 1 )& (df.num_stations >6)]\n",
    "\n",
    "# Plot geographic distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(strong_events.median_lon, strong_events.median_lat, s=strong_events.num_stations, alpha=0.5)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze temporal distribution\n",
    "df[\"event_hour\"] = strong_events.event_time.dt.hour\n",
    "df.event_hour.hist(bins=200)"
   ],
   "id": "5b1ee78976b6f4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "strong_events.detection_times.iloc[0]",
   "id": "e55790537e20aaad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from ast import literal_eval\n",
    "# Configuration\n",
    "DATA_ROOT = \"/media/rsafran/CORSAIR/OHASISBIO/\"\n",
    "OUTPUT_DIR = \"./event_analysis\"\n",
    "BUFFER = timedelta(minutes=2)\n",
    "MIN_STATIONS = 7\n",
    "\n",
    "\n",
    "def safe_convert_list(val):\n",
    "    \"\"\"Safely convert string representation of list to actual list\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [x.strip(\" '\") for x in val.strip('[]').split(',')]\n",
    "\n",
    "def safe_convert_datetime(val):\n",
    "    \"\"\"Convert string representation of datetime list\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return pd.to_datetime(literal_eval(val))\n",
    "    except:\n",
    "        return [pd.to_datetime(x.strip(\" '\")) for x in val.strip('[]').split(',')]\n",
    "\n",
    "def safe_convert_coords(val):\n",
    "    \"\"\"Convert grid points string to list of tuples\"\"\"\n",
    "    if pd.isna(val) or val == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return [tuple(map(float, pair.strip('()').split(',')))\n",
    "                for pair in val.strip('[]').split('), ')]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Custom converters for each column type\n",
    "converters = {\n",
    "    'stations': safe_convert_list,\n",
    "    'travel_times': lambda x: [float(v) for v in safe_convert_list(x)],\n",
    "    'detection_time': safe_convert_datetime,\n",
    "    'grid_points': safe_convert_coords\n",
    "}\n",
    "\n",
    "# Load dataframe with proper type conversion\n",
    "df = pd.read_csv(\n",
    "    \"associations_20250414.csv\",\n",
    "    parse_dates=['event_time'],\n",
    "    converters=converters\n",
    ")\n"
   ],
   "id": "8933bd3ba6cc8b40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Signal Check",
   "id": "dd83ecaf4daf838a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs  # Missing import\n",
    "from datetime import timedelta\n",
    "from src.utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "# Configuration matching your original association parameters\n",
    "LAT_BOUNDS = [-60, 5]\n",
    "LON_BOUNDS = [35, 120]\n",
    "GRID_SIZE = 350\n",
    "\n",
    "# Calculate grid coordinates (replicating your original grid setup)\n",
    "PTS_LAT = np.linspace(LAT_BOUNDS[0], LAT_BOUNDS[1], GRID_SIZE)\n",
    "PTS_LON = np.linspace(LON_BOUNDS[0], LON_BOUNDS[1], GRID_SIZE)\n",
    "\n",
    "def grid_index_to_coords(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    return [(PTS_LAT[i], PTS_LON[j]) for i, j in indices]\n",
    "\n",
    "\n",
    "def plot_single_event(event_index, df, data_root, stations_list, lat_bounds, lon_bounds, raw=True):\n",
    "    \"\"\"Interactive visualization for one seismic event\"\"\"\n",
    "    try:\n",
    "        event = df.iloc[event_index]\n",
    "    except IndexError:\n",
    "        print(f\"Error: Event index {event_index} out of range (0-{len(df)-1})\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nEvent {event_index}: {event['event_time']}\")\n",
    "    print(f\"Stations: {', '.join(event['stations'])}\")\n",
    "    print(f\"Median location: ({event['median_lat']:.2f}°, {event['median_lon']:.2f}°)\")\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 10), layout='constrained')\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[3, 2])\n",
    "    ax_signals = fig.add_subplot(gs[0])\n",
    "    ax_map = fig.add_subplot(gs[1], projection=ccrs.PlateCarree())\n",
    "\n",
    "    # Initialize map elements first\n",
    "    ax_map.coastlines()\n",
    "    ax_map.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]])\n",
    "\n",
    "    # Plot all background stations\n",
    "    for s in stations_list:\n",
    "        try:\n",
    "            p = s.get_pos()\n",
    "            if not (lat_bounds[0] <= p[0] <= lat_bounds[1]) or \\\n",
    "               not (lon_bounds[0] <= p[1] <= lon_bounds[1]):\n",
    "                continue\n",
    "            ax_map.plot(p[1], p[0], 'bx', alpha=0.5, markersize=5)\n",
    "            ax_map.annotate(s.name,\n",
    "                xy=(p[1], p[0]),\n",
    "                xytext=(p[1] - (lon_bounds[1]-lon_bounds[0])/15,\n",
    "                        p[0] + (lat_bounds[1]-lat_bounds[0])/100),\n",
    "                textcoords=\"data\",\n",
    "                color='gray',\n",
    "                alpha=0.5,\n",
    "                fontsize=8\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping station {s.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Plot event location\n",
    "    ax_map.plot(event['median_lon'], event['median_lat'], 'r*', markersize=15, zorder=10)\n",
    "\n",
    "    # Load and plot signals\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(event['stations'])))\n",
    "\n",
    "    for i, (full_station_name, det_time, tt) in enumerate(zip(\n",
    "        event['stations'],\n",
    "        event['detection_times'],\n",
    "        event['travel_times'])):\n",
    "        try:\n",
    "            # Split station name\n",
    "            dataset, station = full_station_name.split('_', 1)\n",
    "            center_time = det_time\n",
    "            start = center_time - timedelta(minutes=3)\n",
    "            end = center_time + timedelta(minutes=3)\n",
    "            # Get data manager\n",
    "            managers = DatFilesManager(f\"{data_root}/{dataset}/{station}\")\n",
    "            # Load data\n",
    "            data = managers.get_segment(start, end)\n",
    "            if data is None:\n",
    "                print(f\"No data for {station}\")\n",
    "                continue\n",
    "\n",
    "            # Plot signal\n",
    "            # In the signal loading/plotting section:\n",
    "            SAMPLING_RATE = 240  # Hz\n",
    "            SAMPLE_INTERVAL = 1/SAMPLING_RATE  # seconds\n",
    "\n",
    "            # Create accurate time axis\n",
    "            times = pd.to_datetime(start) + pd.to_timedelta(np.arange(len(data)) * SAMPLE_INTERVAL, unit='s')\n",
    "                        # Compute signal energy over a sliding window\n",
    "            if not raw : #Energy plot\n",
    "                SAMPLING_RATE = 240  # Hz\n",
    "                SAMPLE_INTERVAL = 1 / SAMPLING_RATE  # seconds\n",
    "\n",
    "                # Create time axis\n",
    "                times = pd.to_datetime(start) + pd.to_timedelta(np.arange(len(data)) * SAMPLE_INTERVAL, unit='s')\n",
    "                relative_times = (times - det_time).total_seconds()\n",
    "                # Compute energy envelope\n",
    "                window_sec = 5.0\n",
    "                window_samples = int(window_sec * SAMPLING_RATE)\n",
    "                energy = np.convolve(data**2, np.ones(window_samples)/window_samples, mode='same')\n",
    "\n",
    "                # Normalize energy\n",
    "                if np.max(energy) > 0:\n",
    "                    energy /= np.max(energy)\n",
    "\n",
    "                # Vertically shift for visual separation\n",
    "                offset = i * 1.5  # You can tune this value if traces are too close/far\n",
    "                energy_shifted = energy + offset\n",
    "\n",
    "                # Plot energy\n",
    "                ax_signals.plot(\n",
    "                    relative_times, energy_shifted,\n",
    "                    label=f\"{station} (ΔT={tt:.1f}s)\",\n",
    "                    color=colors[i],\n",
    "                    alpha=0.8\n",
    "                )\n",
    "\n",
    "                # Mark detection time\n",
    "                # ax_signals.axvline(relative_times, color=colors[i], linestyle='--', alpha=0.5)\n",
    "\n",
    "            if raw : #raw signal\n",
    "\n",
    "                # Normalize signal\n",
    "                data /= np.max(abs(data))\n",
    "\n",
    "                # Vertically shift for visual separation\n",
    "                offset = i * 1.5  # You can tune this value if traces are too close/far\n",
    "                data_shifted = data + offset\n",
    "                relative_times = (times - det_time).total_seconds()\n",
    "                ax_signals.plot(\n",
    "                    relative_times, data_shifted,\n",
    "                    label=f\"{station} (ΔT={tt:.1f}s)\",\n",
    "                    color=colors[i],\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                ax_signals.axvline(0, color=colors[i], linestyle='--', alpha=0.5)\n",
    "\n",
    "            # Plot detection station on map\n",
    "            dataset, station = full_station_name.split('_', 1)\n",
    "            s = next((s for s in stations_list if s.name == station), None)\n",
    "            if s:\n",
    "                p = s.get_pos()\n",
    "                ax_map.plot(\n",
    "                        p[1], p[0],\n",
    "                        marker='^',\n",
    "                        markersize=8,\n",
    "                        color=colors[i],\n",
    "                        markeredgewidth=1,\n",
    "                        zorder=9)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_station_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Format plots after all data is plotted\n",
    "    ax_signals.set_title(f\"Event {event['event_time']}\\n{len(event['stations'])} stations detected\")\n",
    "    # if raw :\n",
    "    #     ax_signals.xaxis.set_major_formatter(DateFormatter(\"%H:%M:%S\"))\n",
    "    #     ax_signals.set_xlabel(\"UTC Time\")\n",
    "    ax_signals.set_ylabel(\"Amplitude\")\n",
    "    ax_signals.legend(loc='upper right')\n",
    "\n",
    "    ax_map.set_title(\"Event Location (red star) and Detection Stations (colored triangles)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # df = pd.read_csv(\"associations_20250414.csv.csv\", parse_dates=['event_time'])  # Add your converters\n",
    "    df = df[df.num_stations > 7]\n",
    "    DATA_ROOT = \"/media/rsafran/CORSAIR/OHASISBIO\"\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            max_idx = len(df) - 1\n",
    "            prompt = f\"\\nEnter event index (0-{max_idx}) or -1 to quit: \"\n",
    "            event_idx = int(input(prompt))\n",
    "\n",
    "            if event_idx == -1:\n",
    "                break\n",
    "            plot_single_event(event_idx, df, DATA_ROOT, STATIONS, LAT_BOUNDS, LON_BOUNDS)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number\")"
   ],
   "id": "671f101b845c7613",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Least square",
   "id": "bc961ba2f0988617"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading",
   "id": "bf78fbf143dc072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved associations file.\n",
    "path_asso = \"/home/rsafran/PycharmProjects/toolbox/data/detection/association/grids/2018/s_-60-5,35-120,350,0.8,0.6.npy\"\n",
    "\n",
    "# Load associations (allow_pickle=True because the file was saved with pickled Python objects).\n",
    "associations = np.load(path_asso, allow_pickle=True).item()\n",
    "\n",
    "# Flatten associations:\n",
    "# For each association key and each candidate association (tuple) in its list,\n",
    "# we build a row with the association key, candidate number, detection info, and grid info.\n",
    "flattened_data = []\n",
    "\n",
    "for assoc_key, candidates in associations.items():\n",
    "    # Convert the key (datetime) to a string, if needed.\n",
    "    assoc_key_str = str(assoc_key)\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        # candidate is expected to be a tuple: (detections_array, grids_array)\n",
    "        detections_array, grids_array = candidate\n",
    "\n",
    "        # Convert detections array into a list of tuples\n",
    "        # or a string representation if you want a summary.\n",
    "        # For instance, each row of 'detections_array' is [station, datetime]\n",
    "        detection_list = []\n",
    "        for row in detections_array:\n",
    "            # row[0] is typically the station identifier and row[1] is the detection datetime.\n",
    "            detection_list.append((row[0], row[1]))\n",
    "\n",
    "        # Similarly, convert the grid indices array to a list of lists (or string)\n",
    "        grid_list = grids_array.tolist()\n",
    "\n",
    "        flattened_data.append({\n",
    "            \"association_key\": assoc_key_str,\n",
    "            \"candidate_index\": i,\n",
    "            \"detections\": detection_list,\n",
    "            \"grids\": grid_list\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the flattened data.\n",
    "df_associations = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame.\n",
    "df_associations"
   ],
   "id": "6d6077bb43a651f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### eval single value",
   "id": "371d8f5841fbd931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_associations = df_associations[df_associations['detections'].apply(len) > 7]\n",
    "df_associations.reset_index(inplace=True)"
   ],
   "id": "469e96795ea86e66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "row = df_associations.iloc[555]\n",
    "detections = row[\"detections\"]\n",
    "grids = row[\"grids\"]\n",
    "month = pd.to_datetime(row['association_key']).month\n",
    "# print(month)\n",
    "def grid_index_to_coord(indices):\n",
    "    \"\"\"Convert grid indices to (lat, lon) coordinates\"\"\"\n",
    "    i, j = indices\n",
    "    return [PTS_LAT[i], PTS_LON[j]]\n",
    "\n",
    "\n",
    "for [station, d_time] in detections:\n",
    "    print((station, d_time))\n",
    "    print(d_time.timestamp())  #d_time is a datetime object\n",
    "    print(station.get_pos())\n",
    "    print(station.name) #station has a method get_pos() and .name\n",
    "\n",
    "print(grid_index_to_coord(grids[0])) #initial guess event lat and long\n",
    "\n"
   ],
   "id": "b0d1e8824044c694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.physics.sound_model import ISAS_grid as isg\n",
    "lat_bounds = LAT_BOUNDS\n",
    "lon_bounds = LON_BOUNDS\n",
    "PATH = \"/media/rsafran/CORSAIR/ISAS/86442/field/2018\"\n",
    "\n",
    "\n",
    "ds = isg.load_ISAS_TS(PATH, month,lat_bounds,lon_bounds, fast=False)\n",
    "# Convert datetime to seconds since some origin (e.g., min time)\n",
    "arrival_times = [d_time for _, d_time in detections]\n",
    "origin_time = min(arrival_times)\n",
    "arrival_times_seconds = np.array([(t - origin_time).total_seconds() for t in arrival_times])\n",
    "station_positions = [station.get_pos() for station, _ in detections]\n",
    "grid_init_lat = [grid_index_to_coord(grid)[0] for grid in grids]\n",
    "grid_init_lon = [grid_index_to_coord(grid)[1] for grid in grids]\n",
    "def residuals(params, station_positions, arrival_times, ds):\n",
    "    \"\"\"\n",
    "    Improved residuals function that accounts for travel time uncertainty.\n",
    "\n",
    "    params: (lat, lon, t0) where t0 is in seconds\n",
    "    station_positions: list/array of (lat, lon) station coordinates\n",
    "    arrival_times: array of arrival times (in seconds)\n",
    "    ds: dataset with sound velocity data\n",
    "\n",
    "    Returns:\n",
    "    - weighted residuals: difference between modeled and observed arrival times,\n",
    "      weighted by uncertainty when available\n",
    "    \"\"\"\n",
    "    lat, lon, t0 = params\n",
    "    modeled_times = []\n",
    "    uncertainties = []\n",
    "\n",
    "    max_depth = 1000  # Or appropriate depth for your dataset\n",
    "\n",
    "    for i, (slat, slon) in enumerate(station_positions):\n",
    "        try:\n",
    "            travel_time, sig_v, _ = isg.compute_travel_time(\n",
    "                slat, slon, lat, lon, depth=max_depth,\n",
    "                ds=ds, resolution=30, verbose=False,\n",
    "                interpolate_missing=True\n",
    "            )\n",
    "            modeled_times.append(t0 + travel_time)\n",
    "            uncertainties.append(sig_v if sig_v > 0 else 1.0)  # Default to 1.0 if uncertainty is zero\n",
    "        except Exception as e:\n",
    "            print(f\"Error at station {i}: {slat},{slon} to {lat},{lon}: {e}\")\n",
    "            # Add a placeholder with high uncertainty\n",
    "            modeled_times.append(arrival_times[i])  # Use observed time as placeholder\n",
    "            uncertainties.append(1000.0)  # Very high uncertainty\n",
    "\n",
    "    modeled_times = np.array(modeled_times)\n",
    "    uncertainties = np.array(uncertainties)\n",
    "\n",
    "    # Calculate weighted residuals - divide by uncertainty to weight more certain measurements higher\n",
    "    weighted_residuals = (modeled_times - arrival_times) / uncertainties\n",
    "\n",
    "    return weighted_residuals"
   ],
   "id": "418631251c0990d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Better initialization with bounds and handling\n",
    "from scipy.optimize import least_squares\n",
    "import numpy as np\n",
    "from pyproj import Geod\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Define reasonable geographic bounds based on your area of interest\n",
    "lat_min, lat_max = min(LAT_BOUNDS), max(LAT_BOUNDS)\n",
    "lon_min, lon_max = min(LON_BOUNDS), max(LON_BOUNDS)\n",
    "\n",
    "# More conservative initial guess (center of first few stations)\n",
    "lat0 = np.mean(grid_init_lat)\n",
    "lon0 = np.mean(grid_init_lon)\n",
    "\n",
    "\n",
    "# Vectorized distance calculation\n",
    "def calc_distances(lon0, lat0, positions):\n",
    "    return np.array([geod.inv(lon0, lat0, pos[1], pos[0])[2] for pos in positions])\n",
    "\n",
    "\n",
    "# Estimate travel time to nearest station for t0 guess\n",
    "distances = calc_distances(lon0, lat0, station_positions)\n",
    "nearest_idx = np.argmin(distances)\n",
    "nearest_lat, nearest_lon = station_positions[nearest_idx]\n",
    "_, _, distance_m = geod.inv(lon0, lat0, nearest_lon, nearest_lat)\n",
    "\n",
    "# Use a more conservative sound speed estimate \n",
    "sound_speed = 1450  # m/s\n",
    "t0_guess = -distance_m / sound_speed\n",
    "\n",
    "# Initial guess\n",
    "x0 = [lat0, lon0, t0_guess]\n",
    "\n",
    "# Add bounds to keep solution in reasonable area\n",
    "bounds = ([lat_min, lon_min, -np.inf], [lat_max, lon_max, np.inf])\n",
    "\n",
    "# Use jac='3-point' for faster Jacobian approximation\n",
    "result = least_squares(\n",
    "    residuals, x0,\n",
    "    args=(station_positions, arrival_times_seconds, ds),\n",
    "    bounds=bounds,\n",
    "    loss='huber',      # or 'huber'      # tuning parameter\n",
    "    method='trf',\n",
    "    ftol=1e-12, xtol=1e-12, gtol=1e-12,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ],
   "id": "43475591f012dbab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Results\n",
    "estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "print(f\"Initial guess: {lat0, lon0, t0_guess}\")\n",
    "print(f\"Estimated source location: ({estimated_lat:.4f}, {estimated_lon:.4f})\")\n",
    "print(f\"Estimated origin time offset: {estimated_t0:.2f} seconds\")"
   ],
   "id": "a1387549f68ef49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats\n",
    "# Your results\n",
    "estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "initial_lat, initial_lon = lat0, lon0\n",
    "\n",
    "# Optional: List of receiver stations (if available)\n",
    "# receivers = [(lat1, lon1), (lat2, lon2), ...]\n",
    "\n",
    "# Create figure and map projection\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# Plot estimated source location\n",
    "ax.plot(estimated_lon, estimated_lat, marker='*', color='red', markersize=15, label='Estimated Source')\n",
    "\n",
    "# Plot initial guess (optional)\n",
    "ax.plot(initial_lon, initial_lat, marker='x', color='blue', markersize=10, label='Initial Guess')\n",
    "\n",
    "if hasattr(result, 'jac'):\n",
    "    # Get the Jacobian for the lat-lon parameters (exclude time parameter)\n",
    "    J = result.jac[:, :2]\n",
    "\n",
    "    # Calculate the covariance matrix (approximation)\n",
    "    residual_variance = np.sum(result.fun**2) / (len(result.fun) - len(result.x))\n",
    "    covariance_matrix = residual_variance * np.linalg.inv(J.T @ J)\n",
    "\n",
    "    # Eigenvalue decomposition to get ellipse parameters\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Calculate ellipse parameters\n",
    "    # For 95% confidence, use 2.4477 (chi-square with 2 DOF)\n",
    "    confidence = 0.95\n",
    "    chi2_val = stats.chi2.ppf(confidence, 2)\n",
    "\n",
    "    # Semi-major and semi-minor axes\n",
    "    a = np.sqrt(eigenvalues[1] * chi2_val)\n",
    "    b = np.sqrt(eigenvalues[0] * chi2_val)\n",
    "\n",
    "    # Angle in degrees\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n",
    "\n",
    "    # Create and add the ellipse\n",
    "    fac = 1\n",
    "    ellipse = Ellipse(xy=(estimated_lon, estimated_lat),\n",
    "                      width=fac*2 * a, height=fac*2 * b,\n",
    "                      angle=angle,\n",
    "                      edgecolor='red', facecolor='red', alpha=0.8,\n",
    "                      transform=ccrs.PlateCarree(),\n",
    "                      label=f'{int(confidence*100)}% Confidence')\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "# Plot receiver stations (optional)\n",
    "# for lat, lon in receivers:\n",
    "#     ax.plot(lon, lat, marker='^', color='green', markersize=8)\n",
    "\n",
    "# Set extent if needed (min_lon, max_lon, min_lat, max_lat)\n",
    "offset = 5\n",
    "ax.set_extent([estimated_lon - offset, estimated_lon + offset, estimated_lat - offset, estimated_lat + offset])\n",
    "\n",
    "# Add legend and title\n",
    "plt.legend()\n",
    "plt.title('Estimated Source Location on Map')\n",
    "plt.show()\n"
   ],
   "id": "ad4fde7b04091398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy import stats\n",
    "\n",
    "def visualize_localization_result(result, station_positions, ds, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Visualize source localization result with error ellipse.\n",
    "\n",
    "    Parameters:\n",
    "    - result: The optimization result object from least_squares\n",
    "    - station_positions: List of (lat, lon) tuples for station positions\n",
    "    - ds: The ISAS dataset (for context)\n",
    "    - lat_bounds, lon_bounds: The geographic boundaries for the map\n",
    "    \"\"\"\n",
    "    # Extract the best fit parameters and stations\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "    station_lats = [pos[0] for pos in station_positions]\n",
    "    station_lons = [pos[1] for pos in station_positions]\n",
    "\n",
    "    # Set up the map projection\n",
    "    projection = ccrs.PlateCarree()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': projection})\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines(resolution='50m')\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "    # Set map boundaries with some padding\n",
    "    pad = 2  # degrees\n",
    "    ax.set_extent([lon_bounds[0]-pad, lon_bounds[1]+pad,\n",
    "                  lat_bounds[0]-pad, lat_bounds[1]+pad])\n",
    "\n",
    "    # Plot stations\n",
    "    ax.scatter(station_lons, station_lats, color='blue', s=100, marker='^',\n",
    "               transform=ccrs.PlateCarree(), label='Stations', zorder=3)\n",
    "\n",
    "    # Plot the estimated source location\n",
    "    ax.scatter([estimated_lon], [estimated_lat], color='red', s=150, marker='+',\n",
    "               transform=ccrs.PlateCarree(), label='Estimated Source', zorder=4)\n",
    "\n",
    "    # Calculate the error ellipse using the covariance matrix\n",
    "    # This requires the Jacobian from the optimization result\n",
    "    if hasattr(result, 'jac'):\n",
    "        # Get the Jacobian for the lat-lon parameters (exclude time parameter)\n",
    "        J = result.jac[:, :2]\n",
    "\n",
    "        # Calculate the covariance matrix (approximation)\n",
    "        residual_variance = np.sum(result.fun**2) / (len(result.fun) - len(result.x))\n",
    "        covariance_matrix = residual_variance * np.linalg.inv(J.T @ J)\n",
    "\n",
    "        # Eigenvalue decomposition to get ellipse parameters\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "        # Calculate ellipse parameters\n",
    "        # For 95% confidence, use 2.4477 (chi-square with 2 DOF)\n",
    "        confidence = 0.95\n",
    "        chi2_val = stats.chi2.ppf(confidence, 2)\n",
    "\n",
    "        # Semi-major and semi-minor axes\n",
    "        a = np.sqrt(eigenvalues[1] * chi2_val)\n",
    "        b = np.sqrt(eigenvalues[0] * chi2_val)\n",
    "\n",
    "        # Angle in degrees\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n",
    "\n",
    "        # Create and add the ellipse\n",
    "        fac = 10\n",
    "        ellipse = Ellipse(xy=(estimated_lon, estimated_lat),\n",
    "                          width=fac*2 * a, height=fac*2 * b,\n",
    "                          angle=angle,\n",
    "                          edgecolor='red', facecolor='red', alpha=0.8,\n",
    "                          transform=ccrs.PlateCarree(),\n",
    "                          label=f'{int(confidence*100)}% Confidence')\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "        # Add error statistics to title\n",
    "        plt.title(f'Source Localization Result\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)\\n'\n",
    "                 f'Time Offset: {estimated_t0:.2f} s, RMS Error: {np.sqrt(np.mean(result.fun**2)):.2f} s')\n",
    "    else:\n",
    "        # If no Jacobian, just add basic title\n",
    "        plt.title(f'Source Localization Result\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)')\n",
    "\n",
    "    # Add labels and legend\n",
    "    for i, (lon, lat) in enumerate(zip(station_lons, station_lats)):\n",
    "        ax.text(lon + 0.1, lat + 0.1, f'Station {i+1}', transform=ccrs.PlateCarree(),\n",
    "                fontsize=9, horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    # Add a scale bar if desired\n",
    "    # ax.gridlines(draw_labels=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Call this function with your result\n",
    "fig, ax = visualize_localization_result(result, station_positions, ds, LAT_BOUNDS, LON_BOUNDS)\n",
    "\n",
    "plt.savefig('source_localization_result.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "b8fd7d14aef0d4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_residuals(result, station_positions, arrival_times, ds):\n",
    "    \"\"\"\n",
    "    Analyze residuals to identify potential outliers in the data\n",
    "\n",
    "    Parameters:\n",
    "    - result: optimization result from least_squares\n",
    "    - station_positions: list of (lat, lon) tuples for stations\n",
    "    - arrival_times: observed arrival times in seconds\n",
    "    - ds: ISAS dataset\n",
    "\n",
    "    Returns:\n",
    "    - residuals: array of time residuals for each station\n",
    "    - is_outlier: boolean array indicating potential outliers\n",
    "    \"\"\"\n",
    "    # Extract estimated parameters\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "\n",
    "    # Calculate modeled arrival times for each station\n",
    "    modeled_times = []\n",
    "    station_ids = []\n",
    "\n",
    "    for i, (slat, slon) in enumerate(station_positions):\n",
    "        try:\n",
    "            travel_time, _, _ = isg.compute_travel_time(\n",
    "                slat, slon, estimated_lat, estimated_lon,\n",
    "                depth=400, ds=ds, resolution=20,\n",
    "                verbose=False, interpolate_missing=True\n",
    "            )\n",
    "            modeled_times.append(estimated_t0 + travel_time)\n",
    "            station_ids.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating travel time for station {i}: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    modeled_times = np.array(modeled_times)\n",
    "    observed_times = np.array([arrival_times[i] for i in station_ids])\n",
    "\n",
    "    # Calculate residuals (modeled - observed)\n",
    "    time_residuals = modeled_times - observed_times\n",
    "\n",
    "    # Identify outliers using Modified Z-score method\n",
    "    # This is more robust than standard Z-score for small sample sizes\n",
    "    median_residual = np.median(time_residuals)\n",
    "    mad = np.median(np.abs(time_residuals - median_residual))  # Median Absolute Deviation\n",
    "\n",
    "    # MAD needs to be scaled to be comparable to standard deviation\n",
    "    mad_constant = 1.4826  # for normal distribution\n",
    "    modified_z_scores = mad_constant * (time_residuals - median_residual) / (mad + 1e-8)\n",
    "\n",
    "    # Define outlier threshold (typically 3.5 or higher)\n",
    "    outlier_threshold = 3.5\n",
    "    is_outlier = np.abs(modified_z_scores) > outlier_threshold\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nResidual Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Station':8} {'Residual (s)':12} {'Modified Z':10} {'Outlier':8}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, station_id in enumerate(station_ids):\n",
    "        outlier_mark = \"YES\" if is_outlier[i] else \"NO\"\n",
    "        print(f\"{station_id:8d} {time_residuals[i]:12.2f} {modified_z_scores[i]:10.2f} {outlier_mark:8}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"RMS Error: {np.sqrt(np.mean(time_residuals**2)):.2f} seconds\")\n",
    "    print(f\"Median Absolute Deviation: {mad:.2f} seconds\")\n",
    "\n",
    "    return time_residuals, is_outlier, station_ids\n",
    "\n",
    "def visualize_residuals(result, station_positions, arrival_times, ds, lat_bounds, lon_bounds):\n",
    "    \"\"\"\n",
    "    Visualize residuals and identify outliers on a map\n",
    "    \"\"\"\n",
    "    # Calculate residuals and identify outliers\n",
    "    residuals, is_outlier, included_station_ids = analyze_residuals(\n",
    "        result, station_positions, arrival_times, ds\n",
    "    )\n",
    "\n",
    "    # Extract estimated source location\n",
    "    estimated_lat, estimated_lon, estimated_t0 = result.x\n",
    "\n",
    "    # Set up the map\n",
    "    projection = ccrs.PlateCarree()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8),\n",
    "                                subplot_kw={'projection': projection})\n",
    "\n",
    "    # --- First subplot: Map with stations and source ---\n",
    "    ax1.coastlines(resolution='50m')\n",
    "    ax1.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax1.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax1.gridlines(draw_labels=True)\n",
    "\n",
    "    # Set map boundaries with padding\n",
    "    pad = 2  # degrees\n",
    "    ax1.set_extent([lon_bounds[0]-pad, lon_bounds[1]+pad,\n",
    "                   lat_bounds[0]-pad, lat_bounds[1]+pad])\n",
    "\n",
    "    # Plot stations with color based on residual\n",
    "    # Convert residuals to colormap values\n",
    "    norm = plt.Normalize(vmin=-np.max(np.abs(residuals)), vmax=np.max(np.abs(residuals)))\n",
    "\n",
    "    # Plot stations\n",
    "    for i, station_id in enumerate(included_station_ids):\n",
    "        slat, slon = station_positions[station_id]\n",
    "\n",
    "        # Marker properties based on whether it's an outlier\n",
    "        marker_size = 150 if is_outlier[i] else 100\n",
    "        marker_edge = 'black' if is_outlier[i] else None\n",
    "        marker_width = 2 if is_outlier[i] else 0\n",
    "\n",
    "        # Color based on residual value\n",
    "        color = plt.cm.RdBu_r(norm(residuals[i]))\n",
    "\n",
    "        # Add station marker\n",
    "        ax1.scatter([slon], [slat], color=color, s=marker_size, marker='^',\n",
    "                   edgecolor=marker_edge, linewidth=marker_width,\n",
    "                   transform=ccrs.PlateCarree())\n",
    "\n",
    "        # Add station label\n",
    "        ax1.text(slon + 0.1, slat + 0.1, f'Station {station_id}',\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                fontsize=8, horizontalalignment='left')\n",
    "\n",
    "    # Plot the estimated source\n",
    "    ax1.scatter([estimated_lon], [estimated_lat], color='red', s=150, marker='*',\n",
    "               transform=ccrs.PlateCarree(), label='Estimated Source')\n",
    "\n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap='RdBu_r', norm=norm)\n",
    "    cbar = plt.colorbar(sm, ax=ax1, pad=0.05, orientation='horizontal')\n",
    "    cbar.set_label('Residual (seconds)')\n",
    "\n",
    "    ax1.set_title('Source Localization with Residuals\\n'\n",
    "                 f'Estimated Position: ({estimated_lat:.4f}°, {estimated_lon:.4f}°)')\n",
    "\n",
    "    # --- Second subplot: Residual plot ---\n",
    "    # Create a simple plot to show residuals by station\n",
    "    ax2.set_aspect('auto')  # Reset aspect ratio for this plot\n",
    "\n",
    "    # Create bar plot of residuals\n",
    "    bar_positions = np.arange(len(included_station_ids))\n",
    "    colors = [plt.cm.RdBu_r(norm(res)) for res in residuals]\n",
    "\n",
    "    # Plot bars\n",
    "    bars = ax2.bar(bar_positions, residuals, color=colors, edgecolor='black')\n",
    "\n",
    "    # Highlight outliers with pattern\n",
    "    for i, is_out in enumerate(is_outlier):\n",
    "        if is_out:\n",
    "            bars[i].set_hatch('///')\n",
    "\n",
    "    # Add zero line\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # Add threshold lines\n",
    "    mad = np.median(np.abs(residuals - np.median(residuals)))\n",
    "    threshold = 3.5 * 1.4826 * mad\n",
    "    ax2.axhline(y=threshold, color='red', linestyle='--', alpha=0.5, label=f'Outlier Threshold (±{threshold:.2f}s)')\n",
    "    ax2.axhline(y=-threshold, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Set labels\n",
    "    ax2.set_xlabel('Station ID')\n",
    "    ax2.set_ylabel('Residual (seconds)')\n",
    "    ax2.set_title('Arrival Time Residuals by Station')\n",
    "    ax2.set_xticks(bar_positions)\n",
    "    ax2.set_xticklabels([f'Station {sid}' for sid in included_station_ids], rotation=45)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return residuals, is_outlier\n",
    "\n",
    "# Call the function with your results\n",
    "residuals, outliers = visualize_residuals(result, station_positions, arrival_times_seconds, ds, LAT_BOUNDS, LON_BOUNDS)"
   ],
   "id": "2235be57beb5c43d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `result` from least_squares and `station_positions`, `arrival_times_seconds`, and `ds` are in scope\n",
    "\n",
    "# Extract residuals and Jacobian\n",
    "residuals = result.fun\n",
    "J = result.jac\n",
    "\n",
    "# Number of observations and parameters\n",
    "n_obs = residuals.size\n",
    "n_params = result.x.size\n",
    "\n",
    "# Compute reduced chi-squared\n",
    "SSR = np.sum(residuals**2)\n",
    "reduced_chi2 = SSR / (n_obs - n_params)\n",
    "\n",
    "# Parameter covariance and standard errors\n",
    "cov_params = np.linalg.inv(J.T @ J) * reduced_chi2\n",
    "param_std = np.sqrt(np.diag(cov_params))\n",
    "\n",
    "# Parameter names\n",
    "param_names = ['latitude', 'longitude', 't0']\n",
    "\n",
    "# Print summary table\n",
    "print(f\"Reduced χ²: {reduced_chi2:.3f}\\n\")\n",
    "print(\"Parameter estimates with 1σ uncertainties:\")\n",
    "for name, val, err in zip(param_names, result.x, param_std):\n",
    "    print(f\"  {name:<10} = {val:.6f} ± {err:.6f}\")\n",
    "\n",
    "# Plot histogram of weighted residuals\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=10, edgecolor='black')\n",
    "plt.xlabel('Weighted residuals')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Weighted Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d65c4ef243b6935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Geod\n",
    "\n",
    "# compute distances & azimuths from your fitted epicenter\n",
    "lat_fit, lon_fit, _ = result.x\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "dists, azs, res = [], [], result.fun\n",
    "\n",
    "for (slat, slon), r in zip(station_positions, res):\n",
    "    az12, az21, dist = geod.inv(lon_fit, lat_fit, slon, slat)\n",
    "    dists.append(dist/1000)   # in km\n",
    "    azs.append(az12)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dists, res, edgecolor='k')\n",
    "plt.xlabel('Source–station distance (km)')\n",
    "plt.ylabel('Weighted residual (s)')\n",
    "plt.title('Residual vs. Distance')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(azs, res, edgecolor='k')\n",
    "plt.xlabel('Azimuth from event (°)')\n",
    "plt.ylabel('Weighted residual (s)')\n",
    "plt.title('Residual vs. Azimuth')\n",
    "plt.show()\n"
   ],
   "id": "a93cf795ca16a5cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "STATIONS[0].get_pos(include_depth=True)",
   "id": "e85dc4c7a00b2d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "21df11abfe3f60a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
