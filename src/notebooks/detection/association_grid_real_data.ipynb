{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "\n",
    "from utils.detection.association import update_candidates, association_is_new, update_valid_grid, update_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "det_dir = \"../../../data/detection/TiSSNet/OHASISBIO-2018\"\n",
    "catalog_path = \"/home/plerolland/Bureau/dataset.yaml\"\n",
    "out_dir = \"../../../data/detection/association/OHASISBIO_grid.csv\"\n",
    "\n",
    "YEAR = 2018\n",
    "STATIONS = StationsCatalog(catalog_path).filter_out_undated().filter_out_unlocated().ends_after(datetime.datetime(YEAR,1,1)).starts_before(datetime.datetime(YEAR,12,31))\n",
    "SOUND_MODEL = HomogeneousSoundModel(sound_speed=1485.5)\n",
    "\n",
    "MIN_P_TISSNET_PRIMARY = 0.5  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.1  # min probability of detections that can be associated with the browsed one"
   ],
   "id": "7fc99cf131527d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LAT_BOUNDS = [-47, -27]\n",
    "LAT_BOUNDS = [-60, -15]\n",
    "LON_BOUNDS = [68, 88]\n",
    "LON_BOUNDS = [40, 90]\n",
    "PTS_LAT = np.linspace(LAT_BOUNDS[0], LAT_BOUNDS[1], 300)\n",
    "PTS_LON = np.linspace(LON_BOUNDS[0], LON_BOUNDS[1], 300)\n",
    "\n",
    "SOUND_SPEED_UNCERTAINTY = 2\n",
    "GRID_MAX_RES_TIME = (np.sqrt(2) * (PTS_LAT[1]-PTS_LAT[0]) * 111_000) / (SOUND_MODEL.sound_speed - SOUND_SPEED_UNCERTAINTY)\n",
    "PICK_UNCERTAINTY = 5\n",
    "GENERIC_TOLERANCE = 10\n",
    "GRID_TOLERANCE = GRID_MAX_RES_TIME + PICK_UNCERTAINTY\n",
    "print(f\"Grid tolerance of {GRID_TOLERANCE:.2f}s\")\n",
    "\n",
    "GRID_STATION_TRAVEL_TIME = {s : np.zeros((len(PTS_LAT), len(PTS_LON))) for s in STATIONS}\n",
    "for s in tqdm(STATIONS, desc=\"computing travel time grid\"):\n",
    "    for ilat, lat in enumerate(PTS_LAT):\n",
    "        for ilon, lon in enumerate(PTS_LON):\n",
    "            GRID_STATION_TRAVEL_TIME[s][ilat, ilon] = SOUND_MODEL.get_sound_travel_time([lat, lon], s.get_pos())\n",
    "\n",
    "GRID_STATION_COUPLE_TRAVEL_TIME = {s : {s2 : np.zeros((len(PTS_LAT), len(PTS_LON))) for s2 in STATIONS} for s in STATIONS}\n",
    "for s in STATIONS:\n",
    "    for s2 in STATIONS:\n",
    "        GRID_STATION_COUPLE_TRAVEL_TIME[s][s2] = GRID_STATION_TRAVEL_TIME[s2] - GRID_STATION_TRAVEL_TIME[s]\n",
    "\n",
    "STATION_MAX_TRAVEL_TIME = {s : {s2 : SOUND_MODEL.get_sound_travel_time(s.get_pos(), s2.get_pos()) for s2 in STATIONS} for s in STATIONS}"
   ],
   "id": "88c06f3d41cb1d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MERGE_DELTA = datetime.timedelta(seconds=5)  # threshold below which we consider two events should be merged\n",
    "DETECTIONS = {}\n",
    "\n",
    "for det_file in tqdm(glob2.glob(det_dir + \"/*\")):\n",
    "    d = []\n",
    "    with open(det_file, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                d.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    d = np.array(d)\n",
    "    d = d[:,:2]\n",
    "    d = d[d[:,1] > MIN_P_TISSNET_SECONDARY]\n",
    "    d = d[np.argsort(d[:,0])]\n",
    "\n",
    "    # remove duplicates and regularly spaced signals\n",
    "    new_d = [d[0]]\n",
    "    for i in range(1, len(d)):\n",
    "        # check this event is far enough from the previous one\n",
    "        if d[i,0] - d[i-1,0] > MERGE_DELTA:\n",
    "            # check this event is not part of a series of regularly spaced events (which probably means we encounter seismic airgun shots)\n",
    "            if i < 3 or abs((d[i,0]-d[i-1,0]) - (d[i-1,0]-d[i-2,0])) > MERGE_DELTA and abs((d[i,0]-d[i-2,0]) - (d[i-1,0]-d[i-3,0])) > MERGE_DELTA:\n",
    "                new_d.append(d[i])\n",
    "    d = np.array(new_d)\n",
    "\n",
    "    s_name = det_file[:-2].split(\"/\")[-1].split(\"_\")[-1]\n",
    "    s_name, y_start = ID, sname = \"-\".join(s_name.split(\"-\")[:-1]), s_name.split(\"-\")[-1]\n",
    "\n",
    "    station = STATIONS.by_name(s_name).by_starting_year(int(y_start))[0]\n",
    "    DETECTIONS[station] = d\n",
    "\n",
    "    print(f\"Found {len(d)} detections for station {s_name}\")\n",
    "\n",
    "# we keep all detections in a single list, sorted by date, to then browse detections\n",
    "DETECTIONS_MERGED = np.concatenate([[(det[0], det[1], s) for det in DETECTIONS[s]] for s in STATIONS])\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[DETECTIONS_MERGED[:,1] > MIN_P_TISSNET_PRIMARY]\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[np.argsort(DETECTIONS_MERGED[:,0])]"
   ],
   "id": "ce83619997e86174",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "associations = {}\n",
    "association_hashlist = set()\n",
    "\n",
    "print(\"starting association\")\n",
    "\n",
    "REQ_CLOSEST_STATIONS = 3  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "\n",
    "SAVE_PATH_ROOT = \"../../../data/detection/association/grids\"\n",
    "SAVE_PATH_ROOT = None\n",
    "\n",
    "# dets_merged : (n,3) = n_detections x (det_time, det_probability, station)\n",
    "for date1, p1, s1 in tqdm(DETECTIONS_MERGED):\n",
    "    save_path = SAVE_PATH_ROOT\n",
    "    if save_path is not None:\n",
    "        save_path = f'{save_path}/{s1.name}-{date1.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1])\n",
    "    other_stations = other_stations[np.argsort([STATION_MAX_TRAVEL_TIME[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "\n",
    "    current_association = {s1:date1}\n",
    "    anchors = [[si, datei] for si, datei in current_association.items()]\n",
    "    candidates =  update_candidates(None, other_stations, anchors, DETECTIONS, STATION_MAX_TRAVEL_TIME, GENERIC_TOLERANCE)\n",
    "\n",
    "\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "    candidates = {s : candidates[s] for s in other_stations}\n",
    "\n",
    "    def backtrack(station_index, current_association, valid_grid, associations, candidates, save_path):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        for idx in candidates[station]:\n",
    "            date, p = DETECTIONS[station][idx]\n",
    "            if not association_is_new(current_association, date, association_hashlist):\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, GRID_STATION_COUPLE_TRAVEL_TIME, GRID_TOLERANCE, save_path, LON_BOUNDS, LAT_BOUNDS)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date)\n",
    "\n",
    "                if len(current_association) > 3 and not update_results(date1, current_association, valid_points_new, association_hashlist, associations, GRID_STATION_COUPLE_TRAVEL_TIME):\n",
    "                    continue  # no solution\n",
    "\n",
    "                candidates_new = update_candidates(candidates, other_stations[station_index+1:], [[station, date]], DETECTIONS, STATION_MAX_TRAVEL_TIME, GENERIC_TOLERANCE,deep_copy=True)\n",
    "\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations, candidates_new, save_path)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations, candidates, save_path)\n",
    "        return\n",
    "    backtrack(0, current_association, None, associations, candidates, save_path=save_path)"
   ],
   "id": "f8e0926b02ece093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2a7ecf6026b7377",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
