{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import glob2\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Resize\n",
    "from scipy.signal import find_peaks\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import HomogeneousSphericalSoundModel as SoundModel\n",
    "from utils.physics.signal.make_spectrogram import make_spectrogram"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "det_dir = \"../../../data/detection/TiSSNet/GEODAMS_res/\"\n",
    "catalog_path = \"/home/plerolland/Bureau/dataset.yaml\"\n",
    "out_dir = \"../../../data/detection/association/GEODAMS.csv\"\n",
    "\n",
    "stations = StationsCatalog(catalog_path)\n",
    "sound_model = SoundModel(sound_speed=1485.5)\n",
    "\n",
    "MIN_P = 0.1\n",
    "\n",
    "ALLOWED_DELTA = datetime.timedelta(seconds=5)\n",
    "SMALL_DELTA = datetime.timedelta(seconds=5)\n",
    "\n",
    "dets = {}\n",
    "pos = {}\n",
    "for det_file in tqdm(glob2.glob(det_dir + \"*\")):\n",
    "    d = []\n",
    "    with open(det_file, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                d.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    d = np.array(d)\n",
    "    d = d[d[:,1] > MIN_P]\n",
    "    d = d[np.argsort(d[:,0])]\n",
    "\n",
    "    # remove doublons and regularly spaced signals\n",
    "    new_d = [d[0]]\n",
    "    for i in range(1, len(d)):\n",
    "        if d[i,0] - d[i-1,0] > SMALL_DELTA:\n",
    "            if i < 3 or abs((d[i,0]-d[i-1,0]) - (d[i-1,0]-d[i-2,0])) > SMALL_DELTA and abs((d[i,0]-d[i-2,0]) - (d[i-1,0]-d[i-3,0])) > SMALL_DELTA:\n",
    "                new_d.append(d[i])\n",
    "    d = np.array(new_d)\n",
    "\n",
    "    s_name = det_file.split(\"/\")[-1]\n",
    "    dets[s_name] = d\n",
    "\n",
    "    station = stations.by_name(s_name).by_starting_year(np.min(d[:,0]).year)[0]\n",
    "    pos[s_name] = station.get_pos()\n",
    "\n",
    "    print(f\"Found {len(d)} detections for station {s_name}\")\n",
    "\n",
    "s_names = list(dets.keys())"
   ],
   "id": "7fc99cf131527d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the maximal possible time difference of arrivals between each pair of stations\n",
    "allowed_delta_mat = {s_name:{s_name_2:None for s_name_2 in s_names} for s_name in s_names}\n",
    "for i in s_names:\n",
    "    for j in s_names:\n",
    "        d = sound_model.get_sound_travel_time(pos[i], pos[j])\n",
    "        allowed_delta_mat[i][j] = datetime.timedelta(seconds=d) + ALLOWED_DELTA\n",
    "\n",
    "# we keep all detections in a single list, sorted by date\n",
    "dets_merged = np.concatenate([[(s_name, det[0], det[1]) for det in dets[s_name]] for s_name in s_names])\n",
    "dets_merged = dets_merged[np.argsort(dets_merged[:,1])]"
   ],
   "id": "544fc4c8b1cf1d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "possible_associations = []\n",
    "done = set() # keep the record of already used detections (done is a set of datetime)\n",
    "\n",
    "with open(out_dir, \"w\") as f:\n",
    "    f.write(f\"lat,lon,date,p_mean,loc_cost\\n\")\n",
    "\n",
    "for s_name, det_date, det_p in tqdm(dets_merged):\n",
    "    if det_date in done:\n",
    "        continue\n",
    "\n",
    "    # get possibly matching detections for each other station (given expected sound travel time)\n",
    "    candidates = {}\n",
    "    for s_name_2 in s_names:\n",
    "        if s_name_2 != s_name:\n",
    "            candidates[s_name_2] = []\n",
    "            idx = np.searchsorted(dets[s_name_2][:,0], det_date - allowed_delta_mat[s_name][s_name_2], side=\"left\") - 1\n",
    "            if idx < len(dets[s_name_2]):\n",
    "                while dets[s_name_2][idx][0] < det_date + allowed_delta_mat[s_name][s_name_2]:\n",
    "                    if dets[s_name_2][idx][0] in done:\n",
    "                        idx += 1\n",
    "                        if idx >= len(dets[s_name_2]):\n",
    "                            break\n",
    "                        continue\n",
    "                    if dets[s_name_2][idx][0] > (det_date - allowed_delta_mat[s_name][s_name_2]):\n",
    "                        candidates[s_name_2].append((s_name_2, dets[s_name_2][idx][0], dets[s_name_2][idx][1]))\n",
    "                    idx += 1\n",
    "                    if idx >= len(dets[s_name_2]):\n",
    "                        break\n",
    "\n",
    "    # we have our candidates and make our associations\n",
    "    candidates_list = [dets for dets in candidates.values()]\n",
    "    associations = list(itertools.product(*candidates_list))\n",
    "\n",
    "    # check all associations are consistent (i.e. we know they are so with det_date but not if they are so together)\n",
    "    new_associations = []\n",
    "    for association in associations:\n",
    "        consistent = True\n",
    "        for i, (s_name_2, det_date_2, det_p_2) in enumerate(association):\n",
    "            for (s_name_3, det_date_3, det_p_3) in association[i+1:]:\n",
    "                if abs(det_date_3 - det_date_2) > allowed_delta_mat[s_name_2][s_name_3]:\n",
    "                    consistent = False\n",
    "        if consistent:\n",
    "            new_associations.append(association)\n",
    "    associations = new_associations\n",
    "\n",
    "    if len(associations) == 0:\n",
    "        continue\n",
    "\n",
    "    best_a, best_loc = None, None\n",
    "    # at this point we know we have consistent association(s). We check if the location inversion works\n",
    "    for association in associations:\n",
    "        association = [(s_name, det_date, det_p)] + list(association)  # we add the main detection to the association\n",
    "        det_pos = [pos[s_name_2] for (s_name_2, _, _) in association]\n",
    "        det_dates = [det_date_2 for (_, det_date_2, _) in association]\n",
    "\n",
    "        try:\n",
    "            loc = sound_model.localize_common_source(det_pos, det_dates)\n",
    "        except LinAlgError:\n",
    "            continue\n",
    "        if best_loc is None or loc.cost < best_loc.cost:\n",
    "            best_a = association\n",
    "            best_loc = loc\n",
    "\n",
    "    if best_a and best_loc.cost < 1:\n",
    "        det_dates = [det_date_2 for (_, det_date_2, _) in best_a]\n",
    "        date = np.min(det_dates) + datetime.timedelta(seconds=best_loc.x[0])\n",
    "\n",
    "        for (s, d, p) in best_a:\n",
    "            done.add(d)\n",
    "\n",
    "        with open(out_dir, \"a\") as f:\n",
    "            f.write(f\"{best_loc.x[1]},{best_loc.x[2]},{date.timestamp()},{np.mean(np.array(best_a)[:,2])},{best_loc.cost}\\n\")"
   ],
   "id": "84b36d886bba43b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(out_dir, \"r\") as f:\n",
    "    pts = f.readlines()\n",
    "\n",
    "header, pts = pts[0], pts[1:]\n",
    "\n",
    "ref_lat, ref_lon = -12.85, 45.67\n",
    "\n",
    "new_txt = header\n",
    "for l in pts:\n",
    "    pt = l.split(\",\")\n",
    "    lat, lon = float(pt[0]), float(pt[1])\n",
    "\n",
    "    if np.sqrt((lat-ref_lat)**2 + (lon-ref_lon)**2) < 0.15:\n",
    "        new_txt += l\n",
    "\n",
    "with open(out_dir[:-4]+\"_selected.csv\", \"w\") as f:\n",
    "    f.write(new_txt)\n"
   ],
   "id": "23d8c3dba6cb6a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../local/i\", \"r\") as f:\n",
    "    data = [l.split() for l in f.readlines()]\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "r = []\n",
    "for d in data:\n",
    "    r.append([datetime.datetime.strptime(d[0][:-1],\"%Y%j%H%M%S\"), float(d[3]), float(d[4])])\n",
    "data = np.array(r)\n",
    "data = data[np.argsort(data[:,0])]\n",
    "\n",
    "with open(out_dir[:-4]+\"_selected.csv\", \"r\") as f:\n",
    "    pts = f.readlines()[1:]\n",
    "\n",
    "c = 0\n",
    "\n",
    "for l in pts:\n",
    "    pt = l.split(\",\")\n",
    "    lat, lon = float(pt[0]), float(pt[1])\n",
    "    date = datetime.datetime.fromtimestamp(float(pt[2]))\n",
    "\n",
    "    idx = np.searchsorted(data[:,0] - SMALL_DELTA, date)-1\n",
    "    while idx < len(data) and data[idx, 0] - date < SMALL_DELTA:\n",
    "        if date - data[idx, 0] < SMALL_DELTA and abs(lat-data[idx,1])<5 and abs(lon-data[idx,2])<5:\n",
    "            c += 1\n",
    "            break\n",
    "        idx += 1\n",
    "print(c)"
   ],
   "id": "80dccd83facbde3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(data))",
   "id": "1591267f3556cffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5b403567ae8631bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
